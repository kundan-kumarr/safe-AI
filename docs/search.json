[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Safe AI for Cyber-Physical & Intelligent Systems",
    "section": "",
    "text": "Preface\nThis book is a comprehensive guide on Safe AI, bringing together the foundations of machine learning security, adversarial robustness, AI alignment, and trustworthy deployment practices for modern cyber-physical systems (CPS) and LLM-based autonomous agents.\nIt is written as a hybrid between a research handbook, a course textbook, and a practical engineering guide for building and securing intelligent systems.\nThe material integrates insights from academic literature, real-world deployment case studies, and hands-on adversarial evaluations—spanning from classical adversarial machine learning to emerging red/blue-team techniques for large-scale language models.\nSpecial emphasis is placed on CPS, where failures in AI behavior directly influence physical infrastructure such as power grids, transportation systems, and industrial control environments.\nEach chapter begins with explicit learning objectives, conceptual explanations, and diagrams, followed by practical examples, research notes, and additional curated references.\nThe aim is to equip the reader with the full intellectual toolkit required to design, analyze, attack, defend, and align AI systems operating in safety-critical settings.\nThe intended audience should be comfortable with programming and have working familiarity with the following concepts and methods:\nThis book may serve students, researchers, engineers, and practitioners seeking a deep and structured understanding of modern AI security—from the mathematical fundamentals to frontier challenges in alignment, LLM safety, and real-world CPS integration.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Safe AI for Cyber-Physical & Intelligent Systems",
    "section": "",
    "text": "Probability and statistics for uncertainty quantification\n\n\nAlgorithms and data structures for secure and efficient computation\n\n\nReinforcement learning for sequential decision-making in CPS\n\n\nOptimization under constraints\n\nconvex and non-convex optimization\n\ngradient-based and gradient-free methods\n\nrobust and stochastic optimization\n\n\n\n\nAdversarial machine learning\n\nevasion, poisoning, and backdoor attacks\n\nmodel extraction and inversion\n\nadversarial training and certified defenses\n\n\n\n\nOptimization & Control in CPS\n\nmodel predictive control (MPC)\n\ndynamic system stability and safety constraints\n\nuncertainty modeling and robust control strategies\n\n\n\n\nRobustness techniques\n\nadversarial training\n\ndefensive distillation\n\nrandomized smoothing and certified guarantees\n\n\n\n\nTesting and evaluation\n\nblack-box, grey-box, and white-box testing\n\nstress-testing and red-team adversarial probes\n\nsafety benchmarks for LLMs and RL agents\n\n\n\n\nTrustworthiness and safe deployment\n\nexplainability and transparency\n\ndata governance and privacy preservation\n\nmonitoring, auditing, and incident response in AI systems\n\n\n\n\nAI security and alignment foundations\n\nprompt injection and jailbreak mechanisms\n\nsafety guardrails and defensive prompting\n\nAI alignment principles, goal specification, and misgeneralization\n\nsafe agent architectures and oversight mechanisms\n\n\n\n\nResearch methodology\n\nexperimental design\n\nreproducibility and benchmarking\n\nresponsible evaluation of high-stakes AI systems",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#author",
    "href": "index.html#author",
    "title": "Safe AI for Cyber-Physical & Intelligent Systems",
    "section": "Author",
    "text": "Author\nKundan Kumarhttps://kundan-kumarr.github.io/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Safe AI for Cyber-Physical & Intelligent Systems",
    "section": "Citation",
    "text": "Citation\nKumar, K. (2025). Safe AI for Cyber-Physical & Intelligent Systems:\nModel Security, Adversarial Robustness, Agent Safety, and Trustworthy Methods.\nEdition 2025-11.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Safe AI for Cyber-Physical & Intelligent Systems",
    "section": "License",
    "text": "License\nThis work is licensed under the MIT License.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "advml-fundamentals.html",
    "href": "advml-fundamentals.html",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "",
    "text": "4.1 Machine Learning Basics\nThe goal of this lecture is to understand the core principles and architectures of deep learning models. We will learn how deep learning extends traditional machine learning through multi-layer neural networks, explore the mathematical foundation behind neural computations, and understand how training and optimization methods such as gradient descent enable models to learn complex patterns from data.\nObjectives\nReading materials\nMachine learning enables computers to learn patterns from data without explicit programming. The goal of an ML model is to find a mapping function:\nMachine learning aims to learn a function \\(f_\\theta\\) that maps inputs \\(x\\) to outputs \\(y\\):\n\\[\nf_\\theta : X \\rightarrow Y,\n\\]\nwhere \\(\\theta\\) represents model parameters (weights and biases).\nThe learning objective is to minimize the expected loss:\n\\[\n\\theta^* = \\arg\\min_\\theta \\, \\mathbb{E}_{(x, y) \\sim D}[L(y, f_\\theta(x))].\n\\]",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#types-of-machine-learning",
    "href": "advml-fundamentals.html#types-of-machine-learning",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.2 Types of Machine Learning",
    "text": "4.2 Types of Machine Learning\n\n4.2.1 Supervised Learning\nLearns from labeled pairs \\((x_i, y_i)\\) to predict \\(\\hat{y_i}\\).\n\n\nRegression: \\(y_i = f_\\theta(x_i) + \\varepsilon_i\\)\n\n\nClassification: \\(P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}}\\)\n\n\n4.2.2 Unsupervised Learning\nFinds structure in unlabeled data \\(\\{x_i\\}\\).\n\n\nClustering: \\[\n\\min_{\\{C_k\\}} \\sum_{k=1}^K \\sum_{x_i \\in C_k} \\|x_i - \\mu_k\\|^2\n\\]\n\n\nPCA: \\[\nW^* = \\arg\\max_W \\det(W^\\top S W)\n\\]\n\n\n4.2.3 Reinforcement Learning\nLearns a policy \\(\\pi(a|s)\\) that maximizes reward:\n\\[\nJ(\\pi) = \\mathbb{E}\\left[\\sum_{t=0}^{T}\\gamma^t r_t\\right].\n\\]",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#linear-vs-nonlinear-models",
    "href": "advml-fundamentals.html#linear-vs-nonlinear-models",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.3 3. Linear vs Nonlinear Models",
    "text": "4.3 3. Linear vs Nonlinear Models\nA linear classifier defines a boundary:\n\\[\n\\mathbf{w}^\\top \\mathbf{x} + b = 0.\n\\]\nA nonlinear classifier maps data to a transformed space \\(\\phi(x)\\):\n\\[\nf(x) = \\mathbf{w}^\\top \\phi(x) + b.\n\\]\nNeural networks learn these nonlinear transformations automatically through layers of activations. \n\nCodeimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate synthetic data\nnp.random.seed(0)\nX1 = np.random.randn(40, 2) + np.array([-2, 0])\nX2 = np.random.randn(40, 2) + np.array([2, 0])\n\n# Plot points\nplt.scatter(X1[:, 0], X1[:, 1], color=\"dodgerblue\", label=\"Class A\")\nplt.scatter(X2[:, 0], X2[:, 1], color=\"orange\", label=\"Class B\")\n\n# Linear boundary\nx_vals = np.linspace(-4, 4, 100)\nplt.plot(x_vals, -0.5 * x_vals, 'b--', lw=2, label=\"Linear Boundary\")\n\n# Nonlinear boundary (quadratic)\nplt.plot(x_vals, 0.2 * x_vals**2 - 2, 'r-', lw=2, label=\"Nonlinear Boundary\")\n\nplt.title(\"Linear vs Nonlinear Classification\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#structure-of-neural-networks",
    "href": "advml-fundamentals.html#structure-of-neural-networks",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.4 Structure of Neural Networks",
    "text": "4.4 Structure of Neural Networks\nEach neuron performs a linear combination followed by a nonlinear transformation:\n\\[\nz = \\mathbf{w}^\\top \\mathbf{x} + b, \\quad a = \\sigma(z)\n\\]\nFor a deep network with \\(L\\) layers:\n\\[\n\\begin{aligned}\na^{(1)} &= \\sigma(W^{(1)}x + b^{(1)}) \\\\\na^{(2)} &= \\sigma(W^{(2)}a^{(1)} + b^{(2)}) \\\\\n&\\vdots \\\\\n\\hat{y} &= \\sigma(W^{(L)}a^{(L-1)} + b^{(L)})\n\\end{aligned}\n\\]",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#activation-functions",
    "href": "advml-fundamentals.html#activation-functions",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.5 Activation Functions",
    "text": "4.5 Activation Functions\n\n\n\n\n\n\n\n\nFunction\nFormula\nRange\nNotes\n\n\n\nSigmoid\n\\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\n(0, 1)\nSaturates, causes vanishing gradients\n\n\nTanh\n\\(\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n(-1, 1)\nZero-centered\n\n\nReLU\n\\(\\text{ReLU}(x) = \\max(0, x)\\)\n[0, ∞)\nFast and sparse\n\n\nLeaky ReLU\n\\(\\text{LeakyReLU}(x) = \\max(\\alpha x, x)\\)\n(-∞, ∞)\nFixes “dying ReLU”\n\n\nLinear\n\\(f(x) = cx\\)\n(-∞, ∞)\nUsed in regression",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#loss-functions",
    "href": "advml-fundamentals.html#loss-functions",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.6 Loss Functions",
    "text": "4.6 Loss Functions\nTo train a network, we minimize a loss function \\(L(y, \\hat{y})\\):\n\n\nMean Squared Error (MSE): \\[\nL = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2\n\\]\n\n\nCross-Entropy Loss: \\[\nL = -\\sum_i y_i \\log(\\hat{y}_i)\n\\]",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#gradient-descent",
    "href": "advml-fundamentals.html#gradient-descent",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.7 Gradient Descent",
    "text": "4.7 Gradient Descent\nModel parameters are updated iteratively:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t),\n\\]\nwhere \\(\\eta\\) is the learning rate.\n\n4.7.1 Mini-batch Gradient Descent\n\\[\n\\theta_{t+1} = \\theta_t - \\frac{\\eta}{m} \\sum_{i=1}^{m}\\nabla_\\theta L_i(\\theta_t)\n\\]\nBalances speed and stability in updates.",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#momentum-and-learning-rate-schedules",
    "href": "advml-fundamentals.html#momentum-and-learning-rate-schedules",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.8 Momentum and Learning Rate Schedules",
    "text": "4.8 Momentum and Learning Rate Schedules\nAdd momentum to accelerate convergence:\n\\[\nv_t = \\beta v_{t-1} + (1 - \\beta)\\nabla_\\theta L(\\theta_t),\n\\quad \\theta_{t+1} = \\theta_t - \\eta v_t\n\\]\nLearning rate schedules:\n\nStep decay: \\(\\eta_t = \\eta_0 \\gamma^{\\lfloor t/s \\rfloor}\\)\n\nExponential decay: \\(\\eta_t = \\eta_0 e^{-\\lambda t}\\)\n\nCosine annealing: \\(\\eta_t = \\eta_0 \\frac{1}{2}(1 + \\cos(\\pi t/T))\\)",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#vanishing-and-exploding-gradients",
    "href": "advml-fundamentals.html#vanishing-and-exploding-gradients",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.9 Vanishing and Exploding Gradients",
    "text": "4.9 Vanishing and Exploding Gradients\nGradients may vanish or explode when propagated through many layers:\n\\[\n\\frac{\\partial L}{\\partial W^{(l)}} \\propto \\prod_{i&gt;l} \\frac{\\partial a^{(i)}}{\\partial z^{(i)}}\n\\]\nSolutions: - ReLU activations - Batch normalization - Gradient clipping - LSTM gates in RNNs",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#regularization-for-generalization",
    "href": "advml-fundamentals.html#regularization-for-generalization",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.10 Regularization for Generalization",
    "text": "4.10 Regularization for Generalization\n\n4.10.1 Weight Decay\n\\[\nL' = L + \\lambda \\|W\\|_2^2\n\\]\n\n4.10.2 Dropout\n\\[\na_i' = a_i \\cdot r_i, \\quad r_i \\sim \\text{Bernoulli}(1 - p)\n\\]\n\n4.10.3 Early Stopping\nStop when validation loss no longer improves.\n\n4.10.4 Batch Normalization\n\\[\n\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}, \\quad y = \\gamma \\hat{x} + \\beta\n\\]",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#convolutional-neural-networks-cnns",
    "href": "advml-fundamentals.html#convolutional-neural-networks-cnns",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.11 Convolutional Neural Networks (CNNs)",
    "text": "4.11 Convolutional Neural Networks (CNNs)\nConvolution extracts local spatial features:\n\\[\nY(i, j) = \\sum_m \\sum_n X(i + m, j + n) K(m, n)\n\\]\nPooling reduces spatial dimension:\n\nMax Pooling: \\(y = \\max(x_1, \\dots, x_n)\\)\n\nAverage Pooling: \\(y = \\frac{1}{n}\\sum_i x_i\\)",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#residual-networks-resnets",
    "href": "advml-fundamentals.html#residual-networks-resnets",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.12 Residual Networks (ResNets)",
    "text": "4.12 Residual Networks (ResNets)\nResidual connections help train very deep networks:\n\\[\ny = F(x) + x\n\\]\nThe network learns the residual mapping \\(F(x) = H(x) - x\\), stabilizing gradients across hundreds of layers.",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#recurrent-neural-networks-rnns",
    "href": "advml-fundamentals.html#recurrent-neural-networks-rnns",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.13 Recurrent Neural Networks (RNNs)",
    "text": "4.13 Recurrent Neural Networks (RNNs)\nSequential models update a hidden state over time:\n\\[\nh_t = f(W_h h_{t-1} + W_x x_t + b)\n\\]\n\n4.13.1 Long Short-Term Memory (LSTM)\n\\[\n\\begin{aligned}\ni_t &= \\sigma(W_i x_t + U_i h_{t-1} + b_i) \\\\\nf_t &= \\sigma(W_f x_t + U_f h_{t-1} + b_f) \\\\\no_t &= \\sigma(W_o x_t + U_o h_{t-1} + b_o) \\\\\nc_t &= f_t \\odot c_{t-1} + i_t \\odot \\tanh(W_c x_t + U_c h_{t-1} + b_c) \\\\\nh_t &= o_t \\odot \\tanh(c_t)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#transformer-networks",
    "href": "advml-fundamentals.html#transformer-networks",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.14 Transformer Networks",
    "text": "4.14 Transformer Networks\nSelf-attention replaces recurrence, allowing parallel computation.\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n\\]\nMultiple heads attend to different relationships:\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, \\ldots, head_h) W^O\n\\]\nTransformers power modern large language models such as GPT and BERT.",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "advml-fundamentals.html#summary",
    "href": "advml-fundamentals.html#summary",
    "title": "\n4  Deep Learning Models Overview\n",
    "section": "\n4.15 Summary",
    "text": "4.15 Summary\nDeep learning combines principles of linear algebra, calculus, and probability to learn from massive datasets.\n\\[\n\\text{Forward Pass: } y = f_\\theta(x), \\quad\n\\text{Backward Pass: } \\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L\n\\]\nFrom perceptrons to transformers, every model refines this foundation — learning richer representations through mathematics and computation.",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Deep Learning Models Overview</span>"
    ]
  },
  {
    "objectID": "defenses-advml.html",
    "href": "defenses-advml.html",
    "title": "\n8  Poisoning Attacks\n",
    "section": "",
    "text": "8.1 Introduction\nThe goal of this lecture is to bring together knowledge about regression, time series trends, and autocorrelation with the goal of obtaining correct inference in regression problems involving time series. You should become capable of suggesting a model that would meet the goal of data analysis (testing certain relationships) while satisfying assumptions (e.g., uncorrelatedness of residuals) and minimizing the risk of spurious results.\nObjectives\nReading materials\nHere we explore a regression model with autocorrelated residuals. We already know how to\nHere we bring these two skills together.\nIf the residuals do not satisfy the independence assumption, we can sometimes describe them in terms of a specific correlation model involving one or more new parameters and some ‘new residuals.’ These new residuals essentially satisfy the OLS assumptions and can replace the original correlated residuals (which we will sometimes call the ‘old’ residuals) in the model.\nWhen modeling time series, we often want to find so-called leading indicators, i.e., exogenous variables whose lagged values can be used for predicting the response, so we do not need to predict the \\(X\\)-variables. Hence, our models may include lagged versions of both the response and predictor variables (a general form of such a model was given in ?eq-fmult).",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisoning Attacks</span>"
    ]
  },
  {
    "objectID": "defenses-advml.html#introduction",
    "href": "defenses-advml.html#introduction",
    "title": "\n8  Poisoning Attacks\n",
    "section": "",
    "text": "forecast, construct prediction intervals, and test hypotheses about regression coefficients when the residuals satisfy the ordinary least squares (OLS) assumptions, i.e., the residuals are white noise and have a joint normal distribution \\(N(0, \\sigma^{2})\\);\nmodel serial dependence (i.e., autocorrelation) in stationary univariate time series, such as using ARMA models.",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisoning Attacks</span>"
    ]
  },
  {
    "objectID": "defenses-advml.html#sec-ccf",
    "href": "defenses-advml.html#sec-ccf",
    "title": "\n8  Poisoning Attacks\n",
    "section": "\n8.2 Cross-correlation function",
    "text": "8.2 Cross-correlation function\nCross-correlation is a correlation between different time series, typically with a time offset \\(k\\) (a.k.a. lag): \\[\n\\mathrm{cor}(X_{t+k},Y_t).\n\\tag{8.1}\\] (Recall that autocorrelation is also a correlation with a lag present, but for the same time series.) By calculating cross-correlations at each lag \\(k = 0, \\pm 1, \\pm 2, \\dots\\), we obtain the cross-correlation function (CCF). CCF shows how the correlation coefficient varies with the lag \\(k\\) and helps us to identify important lags and find leading indicators for predictive models. For example, the relationship between nutrient input and algal blooms in a marine environment is not immediate since the algae need time to reproduce. The correlation between a time series of births and demand for baby formula would be distributed over smaller lags \\(l\\) \\[\n\\mathrm{cor}(\\mathrm{Births}_{t - l}, \\mathrm{FormulaDemand}_t)\n\\] than the correlation lags \\(L\\) between births and school enrollments \\[\n\\mathrm{cor}(\\mathrm{Births}_{t - L}, \\mathrm{SchoolEnrollment}_t),\n\\] where \\(l,L &gt; 0\\) and \\(l &lt; L\\).\nThe functions ccf() and forecast::ggCcf() estimate the cross-correlation function and plot it as a base-R or a ggplot2 graph, respectively. The functions use the notations like in Equation 8.1, hence when using ccf(x, y) we are typically interested in identifying negative \\(k\\)s that correspond to correlation between past values of the predictor \\(X_t\\) and current values of the response \\(Y_t\\).\nFor example, Figure 8.1 shows the CCF for two time series of sales that have been detrended by taking differences. From this plot, lagged values of the first series, \\(\\Delta \\mathrm{BJsales{.}lead}_{t - 2}\\) and \\(\\Delta \\mathrm{BJsales{.}lead}_{t - 3}\\) are significantly correlated with current values of the second series, \\(\\Delta \\mathrm{BJsales}_{t}\\). Hence, we can call BJsales{.}lead a leading indicator for BJsales.\n\nCodeforecast::ggCcf(diff(BJsales.lead), diff(BJsales))\n\n\n\n\n\n\nFigure 8.1: Cross-correlation function of the detrended time series.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function ccf() assumes that the two time series are evenly spaced and have no missing data. We can use the argument na.action = na.pass to ignore any missing values.\n\n\nNote that the 95% confidence band shown in Figure 8.1 is based on how i.i.d. series of length \\(n\\) would correlate (the same way the band is produced for an autocorrelation function with acf()). However, when \\(n\\) is small, there are not so many pairs \\((X_{t+k}, Y_t)\\) to calculate the correlations at large lags, hence the confidence bands should be adjusted by using the smaller \\(n'(k) = n - |k|\\) to account for this lack of confidence.\nAnother concern is the autocorrelation of individual time series that may lead to spurious cross-correlation (Dean and Dunsmuir 2016). Intuitively, say if \\(X_t\\) and \\(Y_t\\) are independent from each other but both \\(X_t\\) and \\(Y_t\\) are positively autocorrelated – high (or low) values in \\(X_t\\) follow each other, and high (or low) values in \\(Y_t\\) follow each other – there could be a time alignment among our considered lags \\(k = 0, \\pm 1, \\pm 2, \\dots\\) when the ups (or downs) in \\(X_{t+k}\\) match those in \\(Y_t\\). This may result in spurious cross-correlation. To address the concern of autocorrelation, we can apply bootstrapping to approximate the distribution of cross-correlation coefficients corresponding not to the i.i.d. series but to stationary time series with the same autocorrelation structure as the input data. The function funtimes::ccf_boot() implements the sieve bootstrap and returns results for both Pearson and Spearman correlations (Figure 8.2).\n\nCodeset.seed(123)\nres &lt;- funtimes::ccf_boot(diff(BJsales.lead), diff(BJsales), \n                          B = 10000, plot = \"none\")\np1 &lt;- res %&gt;% \n    ggplot(aes(x = Lag, y = r_P, xend = Lag, yend = 0)) + \n    geom_ribbon(aes(ymin = lower_P, ymax = upper_P), fill = 4) +\n    geom_point() + \n    geom_segment() + \n    geom_hline(yintercept = 0) + \n    ylab(\"Pearson correlation\")\np2 &lt;- res %&gt;% \n    ggplot(aes(x = Lag, y = r_S, xend = Lag, yend = 0)) + \n    geom_ribbon(aes(ymin = lower_S, ymax = upper_S), fill = 4) +\n    geom_point() + \n    geom_segment() + \n    geom_hline(yintercept = 0) + \n    ylab(\"Spearman correlation\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.2: Pearson and Spearman cross-correlation functions of the detrended time series with sieve bootstrap confidence intervals. Note that the function funtimes::ccf_boot() automatically produces a base-R plot that was suppressed here to save the results as the object res and use the package ggplot2 for the plots.\n\n\n\n\nFinally, remember that the reported Pearson correlations in Figure 8.1 and Figure 8.2 A measure the direction and strength of linear relationships, while Spearman correlations in Figure 8.2 B correspond to monotonic relationships. To further investigate whether the lagged relationships are linear, monotonic, or of some other form, we need to plot the lagged scatterplots (Figure 8.3). Also see other functions in the package astsa, such as the function astsa::lag1.plot() for exploring nonlinear autocorrelations.\n\nCodeastsa::lag2.plot(diff(BJsales.lead), diff(BJsales), max.lag = 5)\n\n\n\n\n\n\nFigure 8.3: Lagged scatterplots of the detrended time series with lowess smooths and (linear) Pearson correlations reported in the right corners.",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisoning Attacks</span>"
    ]
  },
  {
    "objectID": "defenses-advml.html#linear-regression-with-arma-errors",
    "href": "defenses-advml.html#linear-regression-with-arma-errors",
    "title": "\n8  Poisoning Attacks\n",
    "section": "\n8.3 Linear regression with ARMA errors",
    "text": "8.3 Linear regression with ARMA errors\nWe begin with the simplest case, a constant mean model, where the residuals are serially correlated and follow an AR(1) model, that is \\[\nY_{t} = \\beta_{0} + \\epsilon_{t},\n\\tag{8.2}\\] where \\(\\epsilon_{t} \\sim\\) AR(1), i.e., \\[\n\\epsilon_{t} = \\phi \\epsilon_{t - 1} + a_{t}.\n\\tag{8.3}\\]\nHere \\(\\phi\\) is a real number satisfying \\(0 &lt; |\\phi| &lt; 1\\); \\(a_{t}\\) is white noise with zero mean and the variance \\(\\nu^{2}\\), i.e., \\(a_{t} \\sim \\mathrm{WN}(0,\\nu^2)\\). We also assume that \\(\\mathrm{cov}(a_{t}, \\epsilon_{s}) = 0\\) for all \\(s &lt; t\\) (i.e., that the residuals \\(\\epsilon_s\\) are not correlated with the future white noise \\(a_t\\)).\nEquation 8.3 allows us to express Equation 8.2 as \\[\nY_{t} = \\beta_{0} + \\phi \\epsilon_{t - 1} + a_{t}.\n\\tag{8.4}\\]\nThe advantage of this representation is that the new residuals \\(a_{t}\\) satisfy the OLS assumptions. In particular, since \\(a_{t}\\) is white noise, \\(a_{t}\\) is homoskedastic and uncorrelated.\nWe shall also assume that \\(a_{t}\\) are normally distributed (for justification of the construction of confidence intervals and prediction intervals). However, even if \\(a_{t}\\) are not normal, \\(a_{t}\\) are uncorrelated, which is a big improvement over the serially correlated \\(\\epsilon_{t}\\).\nOur goal is to remove the \\(\\epsilon_{t}\\) entirely from the constant mean model and replace them with \\(a_{t}\\) acting as new residuals. This can be done as follows. First, write Equation 8.2 for \\(t-1\\) and multiply both sides by \\(\\phi\\), \\[\n\\phi Y_{t -1} = \\phi \\beta_{0} + \\phi \\epsilon_{t - 1}.\n\\tag{8.5}\\]\nTaking the difference (Equation 8.4 minus Equation 8.5) eliminates \\(\\epsilon_{t}\\) from the model: \\[\n\\begin{split}\nY_{t} - \\phi Y_{t - 1} &= \\left( \\beta_{0} + \\phi \\epsilon_{t - 1} + a_{t} \\right) - \\left( \\phi \\beta_{0} + \\phi \\epsilon_{t - 1}  \\right) \\\\\n&= (1 - \\phi) \\beta_{0} + a_{t}.\n\\end{split}\n\\]\nTherefore we can rewrite the constant mean model in Equation 8.2 as \\[\nY_{t} = (1 - \\phi) \\beta_{0} + \\phi Y_{t - 1} + a_{t}.\n\\tag{8.6}\\]\nIn general, for any multiple linear regression \\[\nY_{t} = \\beta_{0} + \\sum^{k}_{j =1} \\beta_{j} X_{t,j} + \\epsilon_{t}, ~~  \\text{where} ~~ \\epsilon_{t} \\sim \\mbox{AR(1)},\n\\tag{8.7}\\] we can perform a similar procedure of eliminating \\(\\epsilon_{t}\\).\nThis elimination procedure leads to the alternate expression \\[\nY_{t} = (1 - \\phi) \\beta_{0} + \\phi Y_{t -1} + \\sum^{k}_{j = 1} \\beta_{j} (X_{t,j} - \\phi X_{t-1, j}) + a_{t},\n\\tag{8.8}\\] where \\(a_{t}\\) is white noise, i.e., homoskedastic with constant (zero) mean and uncorrelated. See Appendix B describing the method of generalized least squares and an example of \\(k=1\\).\nNote that rewriting the model in this way pulls the autocorrelation parameter for the old residuals, \\(\\phi\\), into the regression part of the model. Thus there are now \\(k + 2\\) unknown regression parameters (\\(\\beta_{0}, \\beta_{1}, \\dots, \\beta_{k}\\), and \\(\\phi\\)). The introduction of an additional parameter into the regression part of the model can be regarded as the price to be paid for extracting new residuals \\(a_{t}\\) that satisfy the OLS assumptions.\nNote that the new residuals \\(a_{t}\\) have smaller variance than the \\(\\epsilon_{t}\\). In fact, \\[\n\\begin{split}\n\\sigma^{2} & = \\mbox{var} (\\epsilon_{t} ) = \\mbox{var} (\\phi \\epsilon_{t - 1} + a_{t}) \\\\\n\\\\\n& =  \\phi^{2} \\mbox{var}(\\epsilon_{t - 1}) + \\mbox{var} (a_{t} ) ~~~ \\mbox{since} ~~ \\mbox{cov}(a_{t}, \\epsilon_{t - 1}) = 0\\\\\n\\\\\n& = \\phi^{2}\\sigma^{2}  + \\nu^{2},\n\\end{split}\n\\] leading to the relation \\[\n\\nu^{2} = \\sigma^{2} (1 - \\phi^{2} ).\n\\tag{8.9}\\]\nHowever, comparing Equation 8.6 with Equation 8.6, and Equation 8.8 with Equation 8.7, we see that the rewritten form of the model is not linear in terms of the parameters \\(\\beta_{0}, \\beta_{1}, \\dots, \\beta_{k}\\) and \\(\\phi\\). For example, the intercept term \\((1 - \\phi) \\beta_{0}\\) involves a product of two of the parameters. This nonlinearity makes the OLS, implemented in the R functions lm() and lsfit(), a poor method for obtaining parameter estimates. Instead, we will use the method of maximum likelihood (ML) carried out through such R functions as arima().\nThe function arima() allows us to input the model in its original form, as in Equation 8.7. It then internally rewrites the model to put it in the form of Equation 8.8. (So we do not have to rewrite the model!) It then makes the assumption that the \\(a_t\\) are normal and constructs the multivariate normal likelihood function \\[\nL (Y_{1} , \\dots , Y_{n}; Q ),\n\\] where \\(n\\) is the sample size and \\(Q\\) is the vector of all unknown parameters. In general, for an AR(1) model with \\(k\\) original predictors, we have \\(Q = (\\phi, \\beta_{0}, \\beta_{1}, \\dots, \\beta_{k}, \\nu^{2})\\). Recall that \\(\\nu^{2} = \\mathrm{var}(a_{t}) = \\mathrm{cov}(a_{t} , a_{t})\\).\nThe function arima() then uses the historical data \\(Y_{1}, \\dots, Y_{n}\\) to find the parameter estimates \\[\n\\hat{Q} = \\left( \\hat{\\phi}, \\hat{\\beta}_{0} , \\hat{\\beta}_{1} , \\dots, \\hat{\\beta}_{k} , \\hat{\\nu}^2 \\right),\n\\] which maximize the likelihood \\(L\\). These estimates (and other things, such as the standard errors of the estimates) can be saved to an output object in R. We will use an example to illustrate the use and interpretation of the function arima().\nMoreover, we can extend the regression model in Equation 8.7 with AR(1) errors to a model with a more general form of errors, ARMA, by assuming \\(\\epsilon_t \\sim \\text{ARMA}(p,q)\\) (see Chapter 6.6 in Brockwell and Davis 2002 and https://robjhyndman.com/hyndsight/arimax/): \\[\n\\begin{split}\nY_t &= \\sum^{k}_{j =1} \\beta_{j} X_{t,j} + \\epsilon_{t},\\\\\n\\epsilon_{t} &= \\phi_1 \\epsilon_{t-1} + \\dots + \\phi_p \\epsilon_{t-p} + \\theta_1 a_{t-1} + \\dots + \\theta_q a_{t-q} + a_t.\n\\end{split}\n\\tag{8.10}\\] This model in Equation 8.10 can be specified in R functions arima(), fable::ARIMA(), forecast::Arima(), and forecast::auto.arima().\n\n\n\n\n\n\nNote\n\n\n\nThe R package forecast has been superseded by the new package fable that uses an alternate parameterization of constants, see ?fable::ARIMA.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember that the variables \\(Y_t\\) and \\(X_{t,j}\\) (\\(j = 1,\\dots,k\\)) should be detrended prior to the analysis (to avoid spurious regression results, see the previous lecture on dealing with trends in regression). If differencing is chosen as the method of detrending, the orders of differences \\(d\\) and \\(D\\) can be specified directly within the mentioned ARIMA functions (so R will do the differencing for us).",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisoning Attacks</span>"
    ]
  },
  {
    "objectID": "defenses-advml.html#arimax",
    "href": "defenses-advml.html#arimax",
    "title": "\n8  Poisoning Attacks\n",
    "section": "\n8.4 ARIMAX",
    "text": "8.4 ARIMAX\nARIMAX (‘X’ stands for ‘external regressor’) models are closely related to Equation 8.10, but there is an important difference. For simplicity of notation, we can present an ARMAX(\\(p,q\\)) model that is a regular ARMA(\\(p,q\\)) model for \\(Y_t\\) plus the external regressors: \\[\n\\begin{split}\nY_t &= \\phi_1 Y_{t-1} + \\dots + \\phi_p Y_{t-p} + \\theta_1 a_{t-1} + \\dots + \\theta_q a_{t-q} + a_t\\\\\n&+\\sum^{k}_{j =1} \\beta_{j} X_{t,j},\n\\end{split}\n\\tag{8.11}\\] where \\(a_t\\) is still a zero-mean white noise process. Interestingly, at this time there is no convenient way to estimate this model in R. One could manually write lagged values of \\(Y_t\\) as external regressors (i.e., create new variables in R for \\(Y_{t-1},\\dots, Y_{t-p}\\)), use these variables in the R functions mentioned above, but force \\(p=0\\) (e.g., Soliman et al. 2019 used the functions from the R package forecast).\nThe difference between Equation 8.10 and Equation 8.11 is the presence of lagged values of the response variable, \\(Y_t\\), in Equation 8.11. As Hyndman points out, regression coefficients \\(\\beta_j\\) in Equation 8.11 lose their interpretability compared with usual regression and do not show the effect on \\(Y_t\\) when \\(X_t\\) increased by one. Instead, \\(\\beta\\)’s in ARMAX Equation 8.11 are interpreted conditional on the value of previous values of the response variable (https://robjhyndman.com/hyndsight/arimax/). Therefore, model formulation as in Equation 8.10 may be preferred.\n\n\n\n\n\n\nNote\n\n\n\nOther applicable models include models with mixed effects such as for repeated measures ANOVA (e.g., implemented in R using nlme::gls() and nlme::lme(), see different correlation structures; without a grouping factor, the results of nlme::lme(..., correlation = corARMA(...)) should be similar to estimating model in Equation 8.10). Other regression functions often borrow the functionality (and syntax) of the package nlme for estimating random effects, so autocorrelated residuals can be incorporated into a generalized additive model, GAM, mgcv::gamm(); generalized additive model for location scale and shape, GAMLSS, gamlss::gamlss(), which also can be used just as GAM, if scale and shape parameters are not modeled. A slightly different solution is possible using a generalized autoregressive moving average model (GARMA) demonstrated in Section 8.5, see gamlss.util::garmaFit().\n\n\n\n\n\n\n\n\nExample: Golden tilefish and AMO\n\n\n\nHere we have time series of 1918–2017 annual landings of golden tilefish (tonne) in the U.S. North Atlantic region and the Atlantic Multi-decadal Oscillation (AMO) index characterizing climatic conditions (Figure 8.4). These time series were described in Nesslage et al. (2021) with R code by Lyubchich and Nesslage (2020). The goal is to develop a regression model to explore the relationship between the landings and AMO.\n\nCodeD &lt;- read.csv(\"data/tilefish.csv\")\nsummary(D)\n\n#&gt;       Year         Landings         AMO          \n#&gt;  Min.   :1918   Min.   :   5   Min.   :-0.43383  \n#&gt;  1st Qu.:1943   1st Qu.: 454   1st Qu.:-0.14821  \n#&gt;  Median :1968   Median : 749   Median : 0.01392  \n#&gt;  Mean   :1968   Mean   : 952   Mean   : 0.00147  \n#&gt;  3rd Qu.:1992   3rd Qu.:1204   3rd Qu.: 0.14944  \n#&gt;  Max.   :2017   Max.   :3968   Max.   : 0.35817  \n#&gt;                 NA's   :3\n\n\n\nCodep1 &lt;- ggplot(D, aes(x = Year, y = Landings)) +\n    geom_line() +\n    xlab(\"Year\") +\n    ylab(\"Landings (tonne)\")\np2 &lt;- ggplot(D, aes(x = Year, y = AMO)) +\n    geom_line() +\n    xlab(\"Year\") +\n    ylab(\"AMO\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.4: Time series plots of the golden tilefish landings and AMO.\n\n\n\n\nInterpolation of missing data (a.k.a. imputation) is a separate and very expansive topic. For a univariate time series, linear interpolation can be implemented using forecast::na.interp(). Also, the landings time series required a power transformation, so the square root transformation was applied (Figure 8.5).\n\nCodeD &lt;- D %&gt;% \n    mutate(Landings_noNA = as.numeric(forecast::na.interp(D$Landings))) %&gt;% \n    mutate(Landings_noNA_sqrt = sqrt(Landings_noNA))\n\nD %&gt;% \n    dplyr::select(AMO, Landings_noNA_sqrt) %&gt;% \n    GGally::ggpairs()\n\n\n\n\n\n\nFigure 8.5: Scatterplot matrix of landings and AMO.\n\n\n\n\n\nCodeforecast::ggCcf(D$AMO, D$Landings_noNA_sqrt) + \n    ggtitle(\"\")\n\n\n\n\n\n\nFigure 8.6: Estimated cross-correlation function (CCF) of the AMO and landings time series.\n\n\n\n\nBased on the strongest lagged correlations (Figure 8.6), implement a model \\[\n\\sqrt{Landings_t} = \\beta_0 + \\beta_{1} AMO_{t-7} + \\epsilon_{t},\n\\] where \\(\\epsilon_{t} \\sim\\) ARMA(\\(p,q\\)), and the orders \\(p\\) and \\(q\\) are selected automatically based on Akaike information criterion.\n\nCodelibrary(fable)\nm1 &lt;- D %&gt;% \n    select(Year, Landings_noNA_sqrt, AMO) %&gt;% \n    as_tsibble(index = Year) %&gt;% \n    model(ARIMA(Landings_noNA_sqrt ~ dplyr::lag(AMO, 7)))\n\nreport(m1)\n\n#&gt; Series: Landings_noNA_sqrt \n#&gt; Model: LM w/ ARIMA(2,0,0) errors \n#&gt; \n#&gt; Coefficients:\n#&gt;         ar1     ar2  dplyr::lag(AMO, 7)  intercept\n#&gt;       0.991  -0.184               -6.30       28.3\n#&gt; s.e.  0.104   0.102                5.02        3.4\n#&gt; \n#&gt; sigma^2 estimated as 41.61:  log likelihood=-307\n#&gt; AIC=625   AICc=626   BIC=638\n\n\nWe forgot to check the stationarity of the time series! It seems that the landings can be considered as a unit-root process, so differencing is needed, hence a modified model is\n\\[\n\\Delta \\sqrt{Landings_t} = \\beta_0 + \\beta_{1} AMO_{t-7} + \\epsilon_{t}.\n\\]\n\nCodem2 &lt;- forecast::auto.arima(diff(D$Landings_noNA_sqrt),\n                 xreg = dplyr::lag(D$AMO, 7)[-1],\n                 allowmean = TRUE)\nm2\n\n#&gt; Series: diff(D$Landings_noNA_sqrt) \n#&gt; Regression with ARIMA(0,0,0) errors \n#&gt; \n#&gt; Coefficients:\n#&gt;        xreg\n#&gt;       -2.58\n#&gt; s.e.   3.82\n#&gt; \n#&gt; sigma^2 = 46.3:  log likelihood = -313\n#&gt; AIC=629   AICc=630   BIC=634\n\n\nNote it is different from the implementation with d = 1, which differences all the series.\n\\[\n\\Delta \\sqrt{Landings_t} = \\beta_0 + \\beta_{1} \\Delta AMO_{t-7} + \\epsilon_{t}\n\\]\n\nCodeforecast::auto.arima(D$Landings_noNA_sqrt,\n                     xreg = dplyr::lag(D$AMO, 7), \n                     d = 1)\n\n#&gt; Series: D$Landings_noNA_sqrt \n#&gt; Regression with ARIMA(0,1,0) errors \n#&gt; \n#&gt; Coefficients:\n#&gt;        xreg\n#&gt;       -5.37\n#&gt; s.e.   5.14\n#&gt; \n#&gt; sigma^2 = 45.7:  log likelihood = -309\n#&gt; AIC=623   AICc=623   BIC=628\n\n\nSimilarly, using the newer package fable.\n\nCodem3 &lt;- D %&gt;% \n    select(Year, Landings_noNA_sqrt, AMO) %&gt;% \n    as_tsibble(index = Year) %&gt;% \n    model(ARIMA(Landings_noNA_sqrt ~ pdq(d = 1) + dplyr::lag(AMO, 7)))\n\nreport(m3)\n\n#&gt; Series: Landings_noNA_sqrt \n#&gt; Model: LM w/ ARIMA(0,1,0) errors \n#&gt; \n#&gt; Coefficients:\n#&gt;       dplyr::lag(AMO, 7)\n#&gt;                    -5.37\n#&gt; s.e.                5.14\n#&gt; \n#&gt; sigma^2 estimated as 45.73:  log likelihood=-309\n#&gt; AIC=623   AICc=623   BIC=628",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisoning Attacks</span>"
    ]
  },
  {
    "objectID": "defenses-advml.html#sec-GARMA",
    "href": "defenses-advml.html#sec-GARMA",
    "title": "\n8  Poisoning Attacks\n",
    "section": "\n8.5 GARMA",
    "text": "8.5 GARMA\nLet \\(Y_t\\) be the observed time series and \\(\\boldsymbol{X_t}\\) exogenous regressors. Then, we can model the conditional distribution of \\(Y_t\\), given \\(Y_1,\\dots,Y_{t-1}\\), \\(\\boldsymbol{X_1,\\dots,X_{t}}\\) as \\[\ng(\\mu_t)=\\boldsymbol{X}'_t\\beta+\\sum_{j=1}^p\\phi_j\\{g(Y_{t-j})- \\boldsymbol{X}'_{t-j}\\beta\\}\n+\\sum_{j=1}^q\\theta_j \\{g(Y_{t-j})-g(\\mu_{t-j})\\},\n\\tag{8.12}\\] where \\(g(\\cdot)\\) is an appropriate link function; \\(\\mu_t\\) is a conditional mean of the dependent variable; \\(\\boldsymbol{\\beta}\\) is a vector of regression coefficients; \\(\\phi_j\\), \\(j=1,\\dots, p\\), are the autoregressive coefficients; \\(\\theta_j\\), \\(j=1,\\dots,q\\), are the moving average coefficients, while \\(p\\) and \\(q\\) are the autoregressive and moving average orders, respectively.\nIn certain cases, the function \\(g(\\cdot)\\) requires some transformation of the original series \\(Y_{t-j}\\) to avoid the non-existence of \\(g(Y_{t-j})\\) (Benjamin et al. 2003).\nThe generalized autoregressive moving average model (Equation 8.12), GARMA(\\(p,q\\)), represents a flexible observation-driven modification of the classical Box–Jenkins methodology and GLMs for integer-valued time series. GARMA further advances the classical Gaussian ARMA model to a case where the distribution of the dependent variable is not only non-Gaussian but can be discrete. The dependent variable is assumed to belong to a conditional exponential family distribution given the past information of the process and thus the GARMA can be used to model a variety of discrete distributions (Benjamin et al. 2003). The GARMA model also extends the work of Zeger and Qaqish (1988), who proposed an autoregressive exponential family model, and Li (1994), who introduced its moving average counterpart.\n\n\n\n\n\n\nExample: Insurance claims GARMA model\n\n\n\nConsider the weekly number of house insurance claims related to water and weather damage in one Canadian city. The number is standardized by the daily number of insured properties in that city. Explore the relationship between the number of claims and weekly total precipitation (mm).\n\nCodeInsurance &lt;- read.csv(\"data/insurance_weekly.csv\") %&gt;%\n    dplyr::select(Claims, Precipitation)\nsummary(Insurance)\n\n#&gt;      Claims       Precipitation   \n#&gt;  Min.   :  0.00   Min.   : 0.000  \n#&gt;  1st Qu.:  1.00   1st Qu.: 0.775  \n#&gt;  Median :  3.00   Median : 3.800  \n#&gt;  Mean   :  3.61   Mean   : 7.713  \n#&gt;  3rd Qu.:  4.00   3rd Qu.:10.000  \n#&gt;  Max.   :170.00   Max.   :77.300\n\n\n\nCodeInsurance %&gt;%\n    GGally::ggpairs()\n\n\n\n\n\n\nFigure 8.7: Scatterplot matrix of weekly weather-related home insurance outcomes and precipitation.\n\n\n\n\nBased on the distribution plots in Figure 8.7, the data are highly right-skewed (have heavy right tails). The number of claims is also a discrete variable. Therefore, we deal with non-normal distributions and need to use generalized-type models, like the generalized linear or additive models (GLMs or GAMs). Since there are many zeros in the counts of claims, we can start with the zero-adjusted Poisson distribution to model the number of claims (Gupta et al. 1996; Stasinopoulos and Rigby 2007).\nIn our case, there is just a slight chance that past-week precipitation affects the current-week insurance claims. Hence, we will still keep the current precipitation and additionally explore the lagged effects, using the cross-correlation function (Figure 8.8).\n\nCodelogconstant &lt;- 1\nforecast::ggCcf(Insurance$Precipitation, \n                log(Insurance$Claims + logconstant),\n                lag.max = 3) + \n    ggtitle(\"\")\n\n\n\n\n\n\nFigure 8.8: Estimated cross-correlation function (CCF) of precipitation and number of home insurance claims.\n\n\n\n\nBased on the estimated CCFs (Figure 8.8), past-week precipitation is significantly correlated with the current-week number of claims, so we can add the lagged predictor into our models.\n\nCodeInsurance &lt;- Insurance %&gt;%\n    mutate(Precipitation_lag1 = dplyr::lag(Precipitation, 1),\n           Week = 1:nrow(Insurance),\n           Year = rep(2002:2011, each = 52),\n           Claims_ln = log(Claims + logconstant))\n\n\nBased on Figure 8.9, there might be an increasing trend in the number of claims that we might be able to approximate with a linear function.\n\nCodep1 &lt;- ggplot2::autoplot(as.ts(log(Insurance$Claims + logconstant))) +\n    xlab(\"Week\") +\n    ylab(\"ln(Number of claims)\")\np2 &lt;- forecast::ggAcf(as.ts(log(Insurance$Claims + logconstant)),\n                      lag.max = 110) +\n    ggtitle(\"\") +\n    xlab(\"Lag (weeks)\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.9: Time series plot and sample ACF of the weekly number of claims in a city, after the ln transformation.\n\n\n\n\nPlot the data once again after the transformations (Figure 8.10).\n\nCodeInsurance %&gt;%\n    dplyr::select(-Claims, -Year) %&gt;%\n    GGally::ggpairs()\n\n\n\n\n\n\nFigure 8.10: Scatterplot matrix of the weekly number of weather-related home insurance claims and precipitation.\n\n\n\n\nFit a GARMA model relating the weekly number of insurance claims to the total precipitation during that and previous weeks.\n\n# The model function doesn't accept NAs, so remove them\nInsurance_noNA &lt;- na.omit(Insurance)\n\nlibrary(gamlss.util)\nm00zip &lt;- garmaFit(Claims ~ Precipitation + Week + Precipitation_lag1\n                   ,family = ZIP\n                   ,data = Insurance_noNA)\n\n#&gt; deviance of linear model=  3014\n\n\nObtain ACF and PACF plots of the model residuals to select ARMA order (Figure 8.11).\n\nCodep1 &lt;- forecast::ggAcf(m00zip$residuals) +\n    ggtitle(\"\") +\n    xlab(\"Lag (weeks)\")\np2 &lt;- forecast::ggAcf(m00zip$residuals, type = \"partial\") +\n    ggtitle(\"\") +\n    xlab(\"Lag (weeks)\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.11: ACF and PACF plots of residuals of the base model GARMA(0,0) based on ZIP distribution.\n\n\n\n\nBased on the observed ACF and PACF patterns in Figure 8.11, an appropriate model for the temporal dependence could be ARMA(3,0). Refit the GARMA model specifying these orders. Then verify that the temporal dependence in residuals was removed (Figure 8.12), and assess other assumptions (Figure 8.13), including homogeneity and normality of the quantile residuals.\n\nset.seed(12345)\nm30zip &lt;- garmaFit(Claims ~ Precipitation + Week + Precipitation_lag1\n                ,order = c(3, 0)\n                ,family = ZIP\n                ,data = Insurance_noNA)\n\n#&gt; deviance of linear model=  3014 \n#&gt; deviance of  garma model=  2810\n\nsummary(m30zip)\n\n#&gt; \n#&gt; Family:  c(\"ZIP\", \"Poisson Zero Inflated\") \n#&gt; Fitting method: \"nlminb\" \n#&gt; \n#&gt; Call:  garmaFit(formula = Claims ~ Precipitation + Week + Precipitation_lag1,  \n#&gt;     order = c(3, 0), data = Insurance_noNA, family = ZIP) \n#&gt; \n#&gt; \n#&gt; Coefficient(s):\n#&gt;                             Estimate   Std. Error  t value   Pr(&gt;|t|)    \n#&gt; beta.(Intercept)         1.291077866  0.129683556  9.95560 &lt; 2.22e-16 ***\n#&gt; beta.Precipitation       0.030148082  0.001386908 21.73762 &lt; 2.22e-16 ***\n#&gt; beta.Week               -0.000480057  0.000335308 -1.43169    0.15223    \n#&gt; beta.Precipitation_lag1  0.015741322  0.001742323  9.03468 &lt; 2.22e-16 ***\n#&gt; phi1                     0.293691946  0.025304217 11.60644 &lt; 2.22e-16 ***\n#&gt; phi2                     0.022192255  0.023077722  0.96163    0.33623    \n#&gt; phi3                     0.114732889  0.025263610  4.54143 5.5874e-06 ***\n#&gt; sigma                    0.058125044  0.014483105  4.01330 5.9876e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;  Degrees of Freedom for the fit: 8 Residual Deg. of Freedom   511 \n#&gt; Global Deviance:     2810 \n#&gt;             AIC:     2826 \n#&gt;             SBC:     2860\n\n\n\nCodep1 &lt;- forecast::ggAcf(m30zip$residuals) +\n    ggtitle(\"\") +\n    xlab(\"Lag (weeks)\")\np2 &lt;- forecast::ggAcf(m30zip$residuals, type = \"partial\") +\n    ggtitle(\"\") +\n    xlab(\"Lag (weeks)\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.12: ACF and PACF plots of the GARMA(0,3) model residuals based on ZIP distribution.\n\n\n\n\n\nCodeplot(m30zip)\n\n#&gt; ******************************************************************\n#&gt;   Summary of the Randomised Quantile Residuals\n#&gt;                            mean   =  0.00873 \n#&gt;                        variance   =  1.74 \n#&gt;                coef. of skewness  =  1.4 \n#&gt;                coef. of kurtosis  =  8.44 \n#&gt; Filliben correlation coefficient  =  0.962 \n#&gt; ******************************************************************\n\n\n\n\n\n\n\nFigure 8.13: Default diagnostics of the GARMA(0,3) model residuals based on ZIP distribution.\n\n\n\n\nSee Dunn and Smyth (1996) for the details on randomized quantile residuals. Overall, their poor correspondence with the standard normal distribution shows a lack of fit of the model. See ?gamlss.family for other distribution families, continuous and discrete; somewhat out-of-date tables with many of these distributions listed are available from Stasinopoulos and Rigby (2007).\nHere we try one other distribution appropriate for modeling overdispersed count data – negative binomial distribution. See the model summary below and the verification of residuals in Figure 8.14 and Figure 8.15.\n\nset.seed(12345)\nm03nbi &lt;- garmaFit(Claims ~ Precipitation + Week + Precipitation_lag1\n                ,order = c(0, 3)\n                ,family = NBI\n                ,data = Insurance_noNA)\n\n#&gt; deviance of linear model=  2324 \n#&gt; deviance of  garma model=  2276\n\nsummary(m03nbi)\n\n#&gt; \n#&gt; Family:  c(\"NBI\", \"Negative Binomial type I\") \n#&gt; Fitting method: \"nlminb\" \n#&gt; \n#&gt; Call:  garmaFit(formula = Claims ~ Precipitation + Week + Precipitation_lag1,  \n#&gt;     order = c(0, 3), data = Insurance_noNA, family = NBI) \n#&gt; \n#&gt; \n#&gt; Coefficient(s):\n#&gt;                            Estimate  Std. Error  t value   Pr(&gt;|t|)    \n#&gt; beta.(Intercept)        0.632190103 0.114305972  5.53068 3.1899e-08 ***\n#&gt; beta.Precipitation      0.024795914 0.002980593  8.31912 &lt; 2.22e-16 ***\n#&gt; beta.Week               0.001254920 0.000319393  3.92908 8.5273e-05 ***\n#&gt; beta.Precipitation_lag1 0.013227586 0.003520075  3.75776 0.00017144 ***\n#&gt; theta1                  0.155521292 0.032452567  4.79226 1.6491e-06 ***\n#&gt; theta2                  0.024441132 0.032148849  0.76025 0.44710572    \n#&gt; theta3                  0.073719033 0.030828465  2.39127 0.01679042 *  \n#&gt; sigma                   0.360726970 0.039810003  9.06121 &lt; 2.22e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;  Degrees of Freedom for the fit: 8 Residual Deg. of Freedom   511 \n#&gt; Global Deviance:     2276 \n#&gt;             AIC:     2292 \n#&gt;             SBC:     2326\n\n\n\nCodep1 &lt;- forecast::ggAcf(m03nbi$residuals) +\n    ggtitle(\"\") +\n    xlab(\"Lag (weeks)\")\np2 &lt;- forecast::ggAcf(m03nbi$residuals, type = \"partial\") +\n    ggtitle(\"\") +\n    xlab(\"Lag (weeks)\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.14: ACF and PACF plots of the GARMA(0,3) model residuals based on NBI distribution.\n\n\n\n\n\nCodeplot(m03nbi)\n\n#&gt; ******************************************************************\n#&gt;   Summary of the Randomised Quantile Residuals\n#&gt;                            mean   =  0.0159 \n#&gt;                        variance   =  0.958 \n#&gt;                coef. of skewness  =  0.561 \n#&gt;                coef. of kurtosis  =  6.43 \n#&gt; Filliben correlation coefficient  =  0.983 \n#&gt; ******************************************************************\n\n\n\n\n\n\n\nFigure 8.15: Default diagnostics of the GARMA(0,3) model residuals based on NBI distribution.\n\n\n\n\nThe residual diagnostics look better for the latter model. One could also consider modeling the strictly periodical component such as using the Fourier series, however, Figure 8.9 did not show a prominent seasonality.",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisoning Attacks</span>"
    ]
  },
  {
    "objectID": "defenses-advml.html#sec-GAMLSS",
    "href": "defenses-advml.html#sec-GAMLSS",
    "title": "\n8  Poisoning Attacks\n",
    "section": "\n8.6 GAMLSS",
    "text": "8.6 GAMLSS\nStasinopoulos and Rigby (2007) provide an extension of a generalized additive model to \\(k=1,2,3,4\\) parameters \\(\\boldsymbol{\\theta}_k\\) of a distribution in a so-called generalized additive model for location scale and shape (GAMLSS). The \\(k\\) parameters represent the location parameter \\(\\mu\\) (it is what we typically model with regression models), scale \\(\\sigma\\), and two shape parameters: skewness and kurtosis. The model can be further generalized for \\(k&gt;4\\) if needed. It allows us to fit \\(k\\) individual models to study relationships between regressors \\(\\boldsymbol{x}\\) and parameters of the response distribution: \\[\ng_k(\\boldsymbol{\\theta}_k) = h_k\\left(\\boldsymbol{X}_k,\\boldsymbol{\\beta}_k\\right) + \\sum_{j=1}^{J_k}h_{jk}(\\boldsymbol{x}_{jk}),\n\\tag{8.13}\\] where \\(k=1\\) produces model for the mean; \\(h_k(\\cdot)\\) and \\(h_{jk}(\\cdot)\\) are nonlinear functions; \\(\\boldsymbol{\\beta}_k\\) is a parameter vector of length \\(J_k\\); \\(\\boldsymbol{X}_k\\) is an \\(n\\times J_k\\) design matrix; \\(\\boldsymbol{x}_{jk}\\) are vectors of length \\(n\\). The terms in the GAMLSS provide a flexible framework to specify nonlinearities, random effects, and correlation structure as in the mixed effects models (Zuur et al. 2009); see Table 3 by Stasinopoulos and Rigby (2007) for the possible specifications of the additive terms. Hence, a model as in Equation 8.13 may accommodate non-normal distributions, possibly nonlinear relationships, and spatiotemporal dependencies in the data.\n\n\n\n\n\n\nExample: Insurance claims GAMLSS\n\n\n\nHere, we use insights from the latest GARMA model and specify the same regressors and distribution family of the response variable in a GAMLSS.\nNegative binomial distribution has two parameters (see ?NBI) – location and scale – so the GAMLSS using this distribution family can include up to two equations (\\(k = 1, 2\\) in Equation 8.13).\nWe will keep a linear relationship between claims and the week number (parametric linear trend; note that the variable Week here is the time index, not the week of the year) but will use nonparametric additive smooths for the relationships between number of claims and precipitation amounts.\nWe will model the scale (variability) of the number of claims using a smooth term of the week number.\nWe start by fitting a model as follows: \\[\n\\begin{split}\nClaims_t &= NegBin(\\mu_t, \\sigma_t) \\\\\n\\ln(\\mu_t) &= a_0 + a_1 Week_t + f_1(Precipitation_t) + f_2(Precipitation_{t-1}) + \\epsilon_t \\\\\n\\ln(\\sigma_t) &= b_0 + f_3(Week) \\\\\n\\end{split}\n\\tag{8.14}\\] where \\(\\epsilon_t \\sim \\mathrm{WN}(0,\\sigma^2)\\); \\(a_0\\), \\(a_1\\), and \\(b_0\\) are parametric coefficients; \\(f_1\\), \\(f_2\\), and \\(f_3\\) are nonparametric smooths.\nIn its expandable collection of smoothers, the R package gamlss features penalized B-splines. See the help files ?gamlss and ?pb for more details and the review of spline functions in R by Perperoglou et al. (2019). However, we used the original P-splines ps() because they provided more visually smooth results than pb() under the default settings. Note that the model summary below has a table for each parameter of the distribution. The obtained term plots are in Figure 8.16 and Figure 8.17.\n\nlibrary(gamlss)\nset.seed(12345)\nm00_gamlss &lt;- gamlss(Claims ~ ps(Precipitation) + Week + ps(Precipitation_lag1)\n                     ,sigma.formula = ~ps(Week)\n                     ,family = NBI\n                     ,control = gamlss.control(c.crit = 0.01, trace = FALSE)\n                     ,data = Insurance_noNA)\nsummary(m00_gamlss)\n\n#&gt; ******************************************************************\n#&gt; Family:  c(\"NBI\", \"Negative Binomial type I\") \n#&gt; \n#&gt; Call:  gamlss(formula = Claims ~ ps(Precipitation) + Week +  \n#&gt;     ps(Precipitation_lag1), sigma.formula = ~ps(Week),  \n#&gt;     family = NBI, data = Insurance_noNA, control = gamlss.control(c.crit = 0.01,  \n#&gt;         trace = FALSE)) \n#&gt; \n#&gt; Fitting method: RS() \n#&gt; \n#&gt; ------------------------------------------------------------------\n#&gt; Mu link function:  log\n#&gt; Mu Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            0.572497   0.092566    6.18  1.3e-09 ***\n#&gt; ps(Precipitation)      0.014442   0.002691    5.37  1.2e-07 ***\n#&gt; Week                   0.001520   0.000269    5.64  2.8e-08 ***\n#&gt; ps(Precipitation_lag1) 0.011326   0.002908    3.90  0.00011 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; ------------------------------------------------------------------\n#&gt; Sigma link function:  log\n#&gt; Sigma Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -0.371642   0.218617   -1.70  0.08975 .  \n#&gt; ps(Week)    -0.002379   0.000717   -3.32  0.00098 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; ------------------------------------------------------------------\n#&gt; NOTE: Additive smoothing terms exist in the formulas: \n#&gt;  i) Std. Error for smoothers are for the linear effect only. \n#&gt; ii) Std. Error for the linear terms maybe are not accurate. \n#&gt; ------------------------------------------------------------------\n#&gt; No. of observations in the fit:  519 \n#&gt; Degrees of Freedom for the fit:  15\n#&gt;       Residual Deg. of Freedom:  504 \n#&gt;                       at cycle:  6 \n#&gt;  \n#&gt; Global Deviance:     2214 \n#&gt;             AIC:     2244 \n#&gt;             SBC:     2308 \n#&gt; ******************************************************************\n\n\n\nCodegamlss.ggplots::fitted_terms(m00_gamlss, rug = TRUE, nrow = 1, what = \"mu\")\n\n\n\n\n\n\nFigure 8.16: GAMLSS terms showing the estimated relationships between the regressors and the location parameter for the distribution of the number of home insurance claims.\n\n\n\n\n\nCodegamlss.ggplots::fitted_terms(m00_gamlss, rug = TRUE, nrow = 1, what = \"sigma\")\n\n\n\n\n\n\nFigure 8.17: GAMLSS terms showing the estimated relationships between the regressors and the scale parameter for the distribution of the number of home insurance claims.\n\n\n\n\nFor the correlation estimates to work, we need to specify a grouping factor for the random effects. To create a grouping factor, we added a variable with the city ID; here it has only one level \"City A\" and hence does not affect the estimates substantially. If we had information from multiple cities, the specified form of autoregressive structure form = ~Week|City could readily accommodate it.\nThe code below estimates the model from Equation 8.14 again but assumes the new autocorrelation structure in the residuals: \\(\\epsilon_t \\sim\\) ARMA(0,3). The control arguments change the default settings to speed up the algorithm convergence. See the model summary below and the verification of residuals in Figure 8.18.\n\nset.seed(12345)\nInsurance_noNA$City &lt;- as.factor(\"City A\")\nm03_gamlss &lt;- gamlss(Claims ~ re(fixed = ~ps(Precipitation) + Week + ps(Precipitation_lag1)\n                                 ,random = ~1|City\n                                 ,correlation = corARMA(form = ~Week|City, q = 3)\n                                 ,method = \"REML\")\n                     ,sigma.formula = ~ps(Week)\n                     ,family = NBI\n                     ,i.control = glim.control(cc = 0.01, bf.cyc = 100, bf.tol = 0.01)\n                     ,control = gamlss.control(c.crit = 0.01, trace = FALSE)\n                     ,data = Insurance_noNA)\n\nModel summary for the location parameter:\n\nsummary(getSmo(m03_gamlss))\n\n#&gt; Linear mixed-effects model fit by REML\n#&gt;   Data: Data \n#&gt;    AIC  BIC logLik\n#&gt;   1521 1559   -751\n#&gt; \n#&gt; Random effects:\n#&gt;  Formula: ~1 | City\n#&gt;         (Intercept) Residual\n#&gt; StdDev:       0.147     1.26\n#&gt; \n#&gt; Correlation Structure: ARMA(0,3)\n#&gt;  Formula: ~Week | City \n#&gt;  Parameter estimate(s):\n#&gt; Theta1 Theta2 Theta3 \n#&gt; 0.1946 0.0437 0.0332 \n#&gt; Variance function:\n#&gt;  Structure: fixed weights\n#&gt;  Formula: ~W.var \n#&gt; Fixed effects:  list(fix.formula) \n#&gt;                         Value Std.Error  DF t-value p-value\n#&gt; (Intercept)            -0.568    0.2051 515   -2.77  0.0058\n#&gt; ps(Precipitation)       0.018    0.0033 515    5.49  0.0000\n#&gt; Week                    0.001    0.0004 515    3.36  0.0008\n#&gt; ps(Precipitation_lag1)  0.009    0.0036 515    2.64  0.0086\n#&gt;  Correlation: \n#&gt;                        (Intr) ps(Pr) Week  \n#&gt; ps(Precipitation)      -0.145              \n#&gt; Week                   -0.616 -0.010       \n#&gt; ps(Precipitation_lag1) -0.135  0.017 -0.022\n#&gt; \n#&gt; Standardized Within-Group Residuals:\n#&gt;    Min     Q1    Med     Q3    Max \n#&gt; -1.384 -0.549 -0.155  0.327 14.056 \n#&gt; \n#&gt; Number of Observations: 519\n#&gt; Number of Groups: 1\n\n\nModel summary for all distribution parameters:\n\nsummary(m03_gamlss)\n\n#&gt; ******************************************************************\n#&gt; Family:  c(\"NBI\", \"Negative Binomial type I\") \n#&gt; \n#&gt; Call:  gamlss(formula = Claims ~ re(fixed = ~ps(Precipitation) +  \n#&gt;     Week + ps(Precipitation_lag1), random = ~1 | City,  \n#&gt;     correlation = corARMA(form = ~Week | City, q = 3),  \n#&gt;     method = \"REML\"), sigma.formula = ~ps(Week), family = NBI,  \n#&gt;     data = Insurance_noNA, control = gamlss.control(c.crit = 0.01,  \n#&gt;         trace = FALSE), i.control = glim.control(cc = 0.01,  \n#&gt;         bf.cyc = 100, bf.tol = 0.01)) \n#&gt; \n#&gt; Fitting method: RS() \n#&gt; \n#&gt; ------------------------------------------------------------------\n#&gt; Mu link function:  log\n#&gt; Mu Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.1723     0.0329    35.6   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; ------------------------------------------------------------------\n#&gt; Sigma link function:  log\n#&gt; Sigma Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.115810   0.201071    0.58     0.56    \n#&gt; ps(Week)    -0.003421   0.000684   -5.00  7.8e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; ------------------------------------------------------------------\n#&gt; NOTE: Additive smoothing terms exist in the formulas: \n#&gt;  i) Std. Error for smoothers are for the linear effect only. \n#&gt; ii) Std. Error for the linear terms maybe are not accurate. \n#&gt; ------------------------------------------------------------------\n#&gt; No. of observations in the fit:  519 \n#&gt; Degrees of Freedom for the fit:  9\n#&gt;       Residual Deg. of Freedom:  510 \n#&gt;                       at cycle:  5 \n#&gt;  \n#&gt; Global Deviance:     2240 \n#&gt;             AIC:     2258 \n#&gt;             SBC:     2296 \n#&gt; ******************************************************************\n\n\n\nCodeplot(m03_gamlss, ts = TRUE)\n\n#&gt; ******************************************************************\n#&gt;   Summary of the Randomised Quantile Residuals\n#&gt;                            mean   =  0.0141 \n#&gt;                        variance   =  0.924 \n#&gt;                coef. of skewness  =  0.446 \n#&gt;                coef. of kurtosis  =  5.68 \n#&gt; Filliben correlation coefficient  =  0.987 \n#&gt; ******************************************************************\n\n\n\n\n\n\n\nFigure 8.18: Residual diagnostics of the GAMLSS for the number of home insurance claims.\n\n\n\n\nHence, compared with the GARMA model from the previous section, the GAMLSS Equation 8.14 uses the same predictors and distribution family and specifies ARMA(0,3) dependence in the residuals. What differs is that the GAMLSS allows nonlinear relationships between the number of claims and precipitation and models the variability changing nonlinearly across weeks.",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisoning Attacks</span>"
    ]
  },
  {
    "objectID": "defenses-advml.html#sec-Granger",
    "href": "defenses-advml.html#sec-Granger",
    "title": "\n8  Poisoning Attacks\n",
    "section": "\n8.7 Granger causality",
    "text": "8.7 Granger causality\nThe Granger causality (Granger 1969; Kirchgässner and Wolters 2007) concept is based on predictability, which is why we should consider it in the time series course. Pearl causality is based on the analysis of interventions (Rebane and Pearl 1987; Pearl 2009).\nLet \\(I_t\\) be the total information set available at the time \\(t\\). This information set includes the two time series \\(X\\) and \\(Y\\). Let \\(\\bar{X}_t\\) be the set of all current and past values of \\(X\\), i.e., \\(\\bar{X}_t = \\{X_{t}, X_{t-1}, \\dots, X_{t-k}, \\dots \\}\\) and analogously of \\(Y\\). Let \\(\\sigma^2(\\cdot)\\) be the variance of the corresponding forecast error.\n\n\n\n\n\n\nNote\n\n\n\nDifference of two sets, \\(A\\) and \\(B\\), is denoted by \\(A \\setminus B\\); but sometimes the minus sign is used, \\(A - B\\).\n\n\nGranger causality\n\\(X\\) is (simply) Granger causal to \\(Y\\) if future values of \\(Y\\) can be predicted better, i.e., with a smaller forecast error variance, if current and past values of \\(X\\) are used: \\[\n\\sigma^2(Y_{t+1}|I_t) &lt; \\sigma^2(Y_{t+1}|I_t \\setminus \\bar{X}_t).\n\\tag{8.15}\\]\nInstantaneous Granger causality\n\\(X\\) is instantaneously Granger causal to \\(Y\\) if the future value of \\(Y\\), \\(Y_{t+1}\\), can be predicted better, i.e., with a smaller forecast error variance, if the future value of \\(X\\), \\(X_{t+1}\\), is used in addition to the current and past values of \\(X\\): \\[\n\\sigma^2(Y_{t+1}|\\{I_t, X_{t+1}\\}) &lt; \\sigma^2(Y_{t+1}|I_t ).\n\\tag{8.16}\\]\nFeedback\nThere is feedback between \\(X\\) and \\(Y\\) if \\(X\\) is causal to \\(Y\\) and \\(Y\\) is causal to \\(X\\). Feedback is only defined for the case of simple causal relations.\nThe test for Equation 8.15 and Equation 8.16 is, essentially, an \\(F\\)-test comparing two nested models: with additional predictors \\(X\\) and without. In other words, consider the model: \\[\nY_t = \\beta_0 + \\sum_{k=1}^{k_1}\\beta_k Y_{t-k} + \\sum_{k=k_0}^{k_2}\\alpha_k X_{t-k} + U_t\n\\tag{8.17}\\] with \\(k_0 = 1\\). An \\(F\\)-test is applied to test the null hypothesis, H\\(_0\\): \\(\\alpha_1 = \\alpha_2 = \\dots = \\alpha_{k_2} = 0\\). By switching \\(X\\) and \\(Y\\) in Equation 8.17, it can be tested whether a simple causal relation from \\(Y\\) to \\(X\\) exists. There is a feedback relation if the null hypothesis is rejected in both directions (\\(X\\rightarrow Y\\) and \\(Y\\rightarrow X\\)). To test whether there is an instantaneous causality, we finally set \\(k_0 = 0\\) and perform a \\(t\\) or \\(F\\)-test for the null hypothesis H\\(_0\\): \\(\\alpha_0 = 0\\).\nThe problem with this test is that the results are strongly dependent on the number of lags of the explanatory variable, \\(k_2\\). There is a trade-off: the more lagged values we include, the better the influence of this variable can be captured. This argues for a high maximal lag. On the other hand, the power of this test is lower the more lagged values are included (Chapter 3 of Kirchgässner and Wolters 2007). Two general procedures can be used to select the lags: inspecting the sensitivity of results to different \\(k_2\\) (sensitivity analysis) or one of the different information criteria guiding model selection.\n\n\n\n\n\n\nExample: Insurance claims and precipitation Granger causality test\n\n\n\nFor example, use the insurance data to test the relationships between the log-transformed home insurance number of claims and precipitation. A quick test has been performed to check that the time series are stationary and do not have a strong seasonality, so the chance of detecting spurious relationships is minimized. We can use the series in Equation 8.17.\n\nlmtest::grangertest(Claims_ln ~ Precipitation \n                    ,order = 1\n                    ,data = Insurance_noNA)\n\n#&gt; Granger causality test\n#&gt; \n#&gt; Model 1: Claims_ln ~ Lags(Claims_ln, 1:1) + Lags(Precipitation, 1:1)\n#&gt; Model 2: Claims_ln ~ Lags(Claims_ln, 1:1)\n#&gt;   Res.Df Df    F Pr(&gt;F)   \n#&gt; 1    515                  \n#&gt; 2    516 -1 10.5 0.0013 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe low \\(p\\)-value shows that precipitation is a Granger cause of the number of home insurance claims. Now test the reverse relationship.\n\nlmtest::grangertest(Precipitation ~ Claims_ln  \n                    ,order = 1\n                    ,data = Insurance_noNA)\n\n#&gt; Granger causality test\n#&gt; \n#&gt; Model 1: Precipitation ~ Lags(Precipitation, 1:1) + Lags(Claims_ln, 1:1)\n#&gt; Model 2: Precipitation ~ Lags(Precipitation, 1:1)\n#&gt;   Res.Df Df   F Pr(&gt;F)\n#&gt; 1    515              \n#&gt; 2    516 -1 0.2   0.65\n\n\nReverse testing does not confirm a statistically significant Granger causality. Hence, we do not have enough evidence to claim that insurance claims are a Granger cause of precipitation (we could expect to come to this conclusion based on our knowledge of how things work), and hence there is no evidence of feedback between losses and precipitation.\nThe function lmtest::grangertest() does not allow us to set order = 0 (to test instantaneous Granger causality), but we can do it manually. First, to show that ‘manual’ results match the lmtest::grangertest() output, repeat the test above using two nested models with lag 1.\n\nM1 &lt;- lm(Claims_ln ~ dplyr::lag(Claims_ln, 1) + dplyr::lag(Precipitation, 1)\n         ,data = Insurance_noNA)\nM2 &lt;- lm(Claims_ln ~ dplyr::lag(Claims_ln, 1)\n         ,data = Insurance_noNA)\nanova(M1, M2)\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: Claims_ln ~ dplyr::lag(Claims_ln, 1) + dplyr::lag(Precipitation, \n#&gt;     1)\n#&gt; Model 2: Claims_ln ~ dplyr::lag(Claims_ln, 1)\n#&gt;   Res.Df RSS Df Sum of Sq    F Pr(&gt;F)   \n#&gt; 1    515 211                            \n#&gt; 2    516 215 -1     -4.32 10.5 0.0013 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results match.\nSecond, test the instantaneous Granger causality in the same manner.\n\nM3 &lt;- lm(Claims_ln ~ dplyr::lag(Claims_ln, 1) + dplyr::lag(Precipitation, 1) + \n             Precipitation, data = Insurance_noNA)\nM4 &lt;- lm(Claims_ln ~ dplyr::lag(Claims_ln, 1) + dplyr::lag(Precipitation, 1)\n         ,data = Insurance_noNA)\nanova(M3, M4)\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: Claims_ln ~ dplyr::lag(Claims_ln, 1) + dplyr::lag(Precipitation, \n#&gt;     1) + Precipitation\n#&gt; Model 2: Claims_ln ~ dplyr::lag(Claims_ln, 1) + dplyr::lag(Precipitation, \n#&gt;     1)\n#&gt;   Res.Df RSS Df Sum of Sq  F  Pr(&gt;F)    \n#&gt; 1    514 203                            \n#&gt; 2    515 211 -1     -7.91 20 9.5e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results show the presence of such causality.\nAs an extension to the demonstrated techniques for testing Granger causality, we may consider more complex generalized and additive models, to relax the assumptions of normality and linearity in the modeling process.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe lmtest::grangertest() options set one value to both \\(k_1\\) and \\(k_2\\) in Equation 8.17. In our example, it was \\(k_1 = k_2 = 1\\). The ‘manual’ test using the function anova() can be used for models with \\(k_1 = k_2\\) or \\(k_1 \\neq k_2\\).",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisoning Attacks</span>"
    ]
  },
  {
    "objectID": "defenses-advml.html#conclusion",
    "href": "defenses-advml.html#conclusion",
    "title": "\n8  Poisoning Attacks\n",
    "section": "\n8.8 Conclusion",
    "text": "8.8 Conclusion\nMultivariate models are still much more difficult to fit than univariate ones. Multiple regression remains a treacherous procedure when applied to time series data. Many observed time series exhibit nonlinear characteristics, but nonlinear models often fail to give better out-of-sample forecasts than linear models, perhaps because the latter are more robust to departures from model assumptions. It is always a good idea to end with the so-called eyeball test. Plot the forecasts on a time plot of the data and check that they look intuitively reasonable (Chatfield 2000).\n\n\n\n\nBenjamin MA, Rigby RA, Stasinopoulos DM (2003) Generalized autoregressive moving average models. Journal of the American Statistical Association 98:214–223. https://doi.org/10.1198/016214503388619238\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York, NY, USA\n\n\nChatfield C (2000) Time-series forecasting. CRC Press, Boca Raton, FL, USA\n\n\nDean RT, Dunsmuir WTM (2016) Dangers and uses of cross-correlation in analyzing time series in perception, performance, movement, and neuroscience: The importance of constructing transfer function autoregressive models. Behavior Research Methods 48:783–802. https://doi.org/10.3758/s13428-015-0611-2\n\n\nDunn PK, Smyth GK (1996) Randomized quantile residuals. Journal of Computational and Graphical Statistics 5:236–244. https://doi.org/10.2307/1390802\n\n\nGranger CWJ (1969) Investigating causal relations by econometric models and cross-spectral methods. Econometrica 37:424–438. https://doi.org/10.2307/1912791\n\n\nGupta PL, Gupta RC, Tripathi RC (1996) Analysis of zero-adjusted count data. Computational Statistics & Data Analysis 23:207–218. https://doi.org/10.1016/S0167-9473(96)00032-1\n\n\nKirchgässner G, Wolters J (2007) Introduction to modern time series analysis. Springer-Verlag, Berlin, Germany\n\n\nLi WK (1994) Time series models based on generalized linear models: Some further results. Biometrics 50:506–511. https://doi.org/10.2307/2533393\n\n\nLyubchich V, Nesslage G (2020) Environmental drivers of golden tilefish fisheries v1.0. Version v1.0. Zenodo\n\n\nNesslage G, Lyubchich V, Nitschke P, et al (2021) Environmental drivers of golden tilefish (Lopholatilus chamaeleonticeps) commercial landings and catch-per-unit-effort. Fisheries Oceanography 30:608–622. https://doi.org/10.1111/fog.12540\n\n\nPearl J (2009) Causality: Models, reasoning, and inference, 2nd edn. Cambridge University Press, Cambridge, UK\n\n\nPerperoglou A, Sauerbrei W, Abrahamowicz M, Schmid M (2019) A review of spline function procedures in R. BMC Medical Research Methodology 19: https://doi.org/10.1186/s12874-019-0666-3\n\n\nRebane G, Pearl J (1987) The recovery of causal poly-trees from statistical data. In: Proceedings of the third annual conference on uncertainty in artificial intelligence. pp 222–228\n\n\nShumway RH, Stoffer DS (2017) Time series analysis and its applications with R examples, 4th edn. Springer, New York, NY, USA\n\n\nSoliman M, Lyubchich V, Gel YR (2019) Complementing the power of deep learning with statistical model fusion: Probabilistic forecasting of influenza in Dallas County, Texas, USA. Epidemics 28:100345. https://doi.org/10.1016/j.epidem.2019.05.004\n\n\nStasinopoulos DM, Rigby RA (2007) Generalized additive models for location scale and shape (GAMLSS) in R. Journal of Statistical Software 23:1–46. https://doi.org/10.18637/jss.v023.i07\n\n\nZeger SL, Qaqish B (1988) Markov regression models for time series: A quasi-likelihood approach. Biometrics 44:1019–1031. https://doi.org/10.2307/2531732\n\n\nZuur A, Ieno EN, Walker NJ, et al (2009) Mixed effects models and extensions in ecology with R. Springer, New York",
    "crumbs": [
      "Part II — Adversarial Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisoning Attacks</span>"
    ]
  },
  {
    "objectID": "tool-attacks.html",
    "href": "tool-attacks.html",
    "title": "11  Adversarial LLMs",
    "section": "",
    "text": "11.1 Time series forecasts\nThis lecture …\nObjectives\nReading materials",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Adversarial LLMs</span>"
    ]
  },
  {
    "objectID": "tool-attacks.html#time-series-forecasts",
    "href": "tool-attacks.html#time-series-forecasts",
    "title": "11  Adversarial LLMs",
    "section": "",
    "text": "11.1.1 Assumptions\n\n11.1.2 How to obtain forecasts from different types of models (white noise-like; recursive like ARIMA and Exponential smoothing; with x-variables and ARMA structure; Examples in R)\nUnivariate models - extrapolation of trends; ARIMA are 1-step-ahead forecasts (AIC)\nTypes of forecasts (point prediction or intervals; can be different intervals for the same point prediction)\n\n11.1.3 Types of forecasts, especially when multiple regression\nTypes (ex-post from ex-ante forecasts)\nLet \\(\\hat{Y}_T(h)\\) be a forecast \\(h\\) steps ahead made at the time \\(T\\). If \\(\\hat{Y}_T(h)\\) only uses information up to time \\(T\\), the resulting forecasts are called out-of-sample forecasts. Economists call them ex-ante forecasts. We have discussed several ways to select the optimal method or model for forecasting, e.g., using PMAE, PMSE, or coverage – all calculated on a testing set. Chatfield (2000) lists several ways to unfairly ‘improve’ forecasts:\n\nFitting the model to all the data including the test set.\nFitting several models to the training set and choosing the model which gives the best ‘forecasts’ of the test set. The selected model is then used (again) to produce forecasts of the test set, even though the latter has already been used in the modeling process.\nUsing the known test-set values of ‘future’ observations on the explanatory variables in multivariate forecasting. This will improve forecasts of the dependent variable in the test set, but these future values will not of course be known at the time the forecast is supposedly made (though in practice the ‘forecast’ is made at a later date). Economists call such forecasts ex-post forecasts to distinguish them from ex-ante forecasts. The latter, being genuinely out-of-sample, use forecasts of future values of explanatory variables, where necessary, to compute forecasts of the response variable. Ex-post forecasts can be useful for assessing the effects of explanatory variables, provided the analyst does not pretend that they are genuine out-of-sample forecasts.\n\nSo what to do if we put lots of effort to build a regression model using time series and need to forecast the response, \\(Y_t\\), which is modeled using different independent variables \\(X_{t,k}\\) (\\(k=1,\\dots,K\\))? Two options are possible.\nLeading indicators\nIf \\(X_{t,k}\\)’s are leading indicators with lags starting at \\(l\\), we, generally, would not need their future values to obtain the forecasts \\(\\hat{Y}_T(h)\\), where \\(h\\leqslant l\\). For example, the model for losses tested in Section 8.7 shows that precipitation with lag 1 is a good predictor for current losses, i.e., precipitation is a leading indicator. The 1-week ahead forecast of \\(Y_{t+1}\\) can be obtained using the current precipitation \\(X_t\\) (all data are available). If \\(h&gt;l\\), we will be forced to forecast the independent variables, \\(X_{t,k}\\)’s – see the next option.\nForecast of predictors\nIf we opt for forecasting \\(X_{t,k}\\)’s, the errors (uncertainty) of such forecasts will be larger, because future \\(X_{t,k}\\)’s themselves will be the estimates. Nevertheless, it might be the only choice when leading indicators are not available. Building a full and comprehensive model with all diagnostics for each regressor is usually unfeasible and even problematic if we plan to consider multivariate models for regressors (the complexity of models will quickly escalate). As an alternative, it is common to use automatic or semi-automatic univariate procedures that can help to forecast each of the \\(X_{t,k}\\)’s. For example, consider exponential smoothing, Holt–Winters smoothing, and auto-selected SARIMA/ARIMA/ARMA/AR/MA models – all those can be automated for a large number of forecasts to make.",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Adversarial LLMs</span>"
    ]
  },
  {
    "objectID": "tool-attacks.html#cross-validation-schemes",
    "href": "tool-attacks.html#cross-validation-schemes",
    "title": "11  Adversarial LLMs",
    "section": "\n11.2 Cross-validation schemes",
    "text": "11.2 Cross-validation schemes\nGoals and intended implementation of the model hence the selection of the scheme.\nTraining - Testing (split %% and n)\nTraining - Testing - Evaluation\nWindow approach caret:: forecast::",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Adversarial LLMs</span>"
    ]
  },
  {
    "objectID": "tool-attacks.html#metrics-for-model-comparison",
    "href": "tool-attacks.html#metrics-for-model-comparison",
    "title": "11  Adversarial LLMs",
    "section": "\n11.3 Metrics for model comparison",
    "text": "11.3 Metrics for model comparison\nHow do we compare forecasting models to decide which one is better? We will look at various ways of choosing between models as the course progresses, but the most obvious answer is to see which one is better at predicting.\nSuppose we have used the data \\(Y_1, \\dots, Y_n\\) to build \\(M\\) forecasting models \\(\\hat{Y}^{(m)}_t\\) (\\(m = 1,\\dots,M\\)) and we now obtain future observations \\(Y_{n+1}, \\dots, Y_{n+k}\\) that were not used to fit the models (also called out-of-sample data, after-sample, or the testing set; \\(k\\) is the size of this set). The difference \\(Y_t - \\hat{Y}^{(m)}_t\\) is the forecast (or prediction) error at the time \\(t\\) for the \\(m\\)th model. For each model, compute the prediction mean square error (PMSE) \\[\nPMSE_m = k^{-1}\\sum_{t=n+1}^{n+k}\\left(Y_t - \\hat{Y}^{(m)}_t\\right)^2\n\\tag{11.1}\\] and prediction mean absolute error (PMAE) \\[\nPMAE_m = k^{-1}\\sum_{t=n+1}^{n+k}\\left|Y_t - \\hat{Y}^{(m)}_t\\right|\n\\tag{11.2}\\] and, similarly, prediction root mean square error (PRMSE; \\(PRMSE = \\sqrt{PMSE}\\)), prediction mean absolute percentage error (PMAPE, if \\(Y_t \\neq 0\\) in the testing period), etc. We choose the model with the smallest error.\nOne obvious drawback to the above method is that it requires us to wait for future observations to compare models. A way around this is to take the historical dataset \\(Y_1, \\dots, Y_n\\) and split it into a training set \\(Y_1, \\dots, Y_k\\) and a testing set \\(Y_{k+1}, \\dots, Y_n\\), where \\((n - k)\\ll k\\), i.e., most of the data goes into the training set.\n\n\n\n\n\n\nNote\n\n\n\nThis scheme of splitting time series into the testing and training sets is a simple form of cross-validation. Not all forms of cross-validation apply to time series due to the usual temporal dependence in time series data. We need to select cross-validation techniques that can accommodate such dependence. Usually, it implies selecting data for validation not at random but in consecutive chunks (periods) and, ideally, with testing or validation periods being after the training period.\n\n\nForecasting models are then built using only the training set and used to ‘forecast’ values from the testing set. Sometimes it is called an out-of-sample forecast because we predict values for the times we have not used for the model specification and estimation. The testing set is used as a set of future observations to compute the PMSE. The PMSE and other errors are computed for each model over the testing set and then compared to see errors for which models are smaller.\nIf two models produce approximately the same errors, we choose the model that is simpler (involves fewer variables). This is called the law of parsimony.\nThe above error measures (PMSE, PRMSE, PMAE, etc.) compare observed and forecasted data points, hence, are measures of the accuracy of the point forecasts. Another way of comparing models could be based on the quality of their interval forecasts, i.e., by assessing how good the prediction intervals are. To assess the quality of interval forecasts, one may start by computing the empirical coverage (proportion of observations in the testing set that are within – covered by – corresponding prediction intervals for given confidence \\(C\\), e.g., 95%) and average interval width. Prediction intervals are well-calibrated if empirical coverage is close to \\(C\\) (more important) while intervals are not too wide (less important).\n\n\n\n\n\n\nNote\n\n\n\nTo select the best coverage, one can calculate the absolute differences between the nominal coverage \\(C\\) and each empirical coverage \\(\\hat{C}_m\\): \\[\n\\Delta_m = |C - \\hat{C}_m|.\n\\] Hence, we select the model with the smallest \\(\\Delta_m\\), not the largest coverage \\(\\hat{C}_m\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to obtain different prediction intervals from the same model. For example, we can calculate prediction intervals based on normal and bootstrapped distributions. In this case, point forecasts are the same, but interval forecasts differ.\n\n\nWe may compare a great number of models using the training set, and choose the best one (with the smallest errors), however, it would be unfair to use the out-of-sample errors from the testing set for demonstrating the model performance because this part of the sample was used to select the model. Thus, it is advisable to have one more chunk of the same time series that was not used for model specification, estimation, or selection. Errors of the selected model on this validation set will be closer to the true (genuine) out-of-sample errors and can be used to improve coverage of true out-of-sample forecasts when the model is finally deployed.",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Adversarial LLMs</span>"
    ]
  },
  {
    "objectID": "tool-attacks.html#worked-out-example-of-comparing-several-models",
    "href": "tool-attacks.html#worked-out-example-of-comparing-several-models",
    "title": "11  Adversarial LLMs",
    "section": "\n11.4 Worked out example of comparing several models",
    "text": "11.4 Worked out example of comparing several models\n\nBasic (naive) models: average / climatology / HWinters\nCommonly or previously used model (GLM)\nState-of-the-science or proposed model",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Adversarial LLMs</span>"
    ]
  },
  {
    "objectID": "tool-attacks.html#conclusion",
    "href": "tool-attacks.html#conclusion",
    "title": "11  Adversarial LLMs",
    "section": "\n11.5 Conclusion",
    "text": "11.5 Conclusion\n\n\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York, NY, USA\n\n\nChatfield C (2000) Time-series forecasting. CRC Press, Boca Raton, FL, USA\n\n\nHastie TJ, Tibshirani RJ, Friedman JH (2009) The elements of statistical learning: Data mining, inference, and prediction, 2nd edn. Springer, New York, NY, USA",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Adversarial LLMs</span>"
    ]
  },
  {
    "objectID": "memory-poisoning.html",
    "href": "memory-poisoning.html",
    "title": "\n12  Adversarial LLMs1\n",
    "section": "",
    "text": "12.1 Introduction\nThe goal of this lecture\nA adversarial attacks is typically derived based on an assumed threat model regarding the goal, knowledge, and target strategy of the adversary Adversary’s goal Poisoning attack, evasion attack: cause the ML model to perform incorrectly Privacy attack: acquire knowledge about the training data or the model Availability attack: cause the ML model to be become unavailable Adversary’s knowledge White-box attack: the adversary has full knowledge of the ML model Black-box attack: has no knowledge of the ML model Gray-box attack: has some knowledge of the ML model Adversary’s target strategy Targeted attack: cause the ML model to output a target label for an input Non-targeted attack: cause the ML to output any incorrect label for an input\nA adversarial attacks is typically derived based on an assumed threat model regarding the goal, knowledge, and target strategy of the adversary\nAdversarial Machine Learning is a field that studies the vulnerabilities of machine learning models to malicious manipulation and develops techniques to make them more robust. Core Concept At its heart, adversarial ML involves creating adversarial examples - inputs that are intentionally designed to fool machine learning models. These are often imperceptible or subtle modifications to legitimate inputs that cause models to make incorrect predictions. Classic example: Adding carefully calculated noise to an image of a panda that’s invisible to humans, but causes an image classifier to confidently misidentify it as a gibbon.\nAdversarial Machine Learning is the study of attacks against ML systems and defenses against them. The key insight is that every adversarial attack operates within a threat model defined by three dimensions:\nObjectives\nReading materials\nBy now, we have been working in the time domain, such that our analysis could be seen as a regression of the present on the past (for example, ARIMA models). We have been using many time series plots with time on the \\(x\\)-axis.\nAn alternative approach is to analyze time series in a spectral domain, such that use a regression of present on a linear combination of sine and cosine functions. In this type of analysis, we often use periodogram plots, with frequency or period on the \\(x\\)-axis.",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "memory-poisoning.html#regression-on-sinusoidal-components",
    "href": "memory-poisoning.html#regression-on-sinusoidal-components",
    "title": "\n12  Adversarial LLMs1\n",
    "section": "\n12.2 Regression on sinusoidal components",
    "text": "12.2 Regression on sinusoidal components\nThe simplest form of spectral analysis consists of regression on a periodic component: \\[\nY_t = A\\cos \\omega t + B \\sin \\omega t + C + \\epsilon_t,\n\\tag{12.1}\\] where \\(t =1,\\dots,T\\) and \\(\\epsilon_t \\sim \\mathrm{WN}(0, \\sigma^2)\\). Without loss of generality, we assume \\(0 \\leqslant \\omega \\leqslant \\pi\\). In fact, for discrete data, frequencies outside this range are aliased into this range. For example, suppose that \\(-\\pi &lt; (\\omega = \\pi - \\delta) &lt; 0\\), then \\[\n\\begin{split}\n\\cos(\\omega t) &= \\cos((\\pi - \\delta)t) \\\\\n& = \\cos(\\pi t) \\cos(\\delta t) + \\sin(\\pi t) \\sin(\\delta t) \\\\\n& = \\cos(\\delta t).\n\\end{split}\n\\] Hence, a sampled sinusoid with a frequency smaller than 0 appears to coincide with a sinusoid with frequency in the interval \\([0, \\pi]\\).\n\n\n\n\n\n\nNote\n\n\n\nThe term \\(A\\cos \\omega t + B \\sin \\omega t\\) is a periodic function with the period \\(2\\pi / \\omega\\). The period \\(2\\pi / \\omega\\) represents the number of time units that it takes for the function to take the same value again, i.e., to complete a cycle. The frequency, measured in cycles per time unit, is given by the inverse \\(\\omega / (2 \\pi)\\). The angular frequency, measured in radians per time unit, is given by \\(\\omega\\). Because of its convenience, the angular frequency \\(\\omega\\) will be used to describe the periodicity of the function, and its name is shortened to frequency when there is no danger of confusion.\n\n\nConsider monthly data that exhibit a 12-month seasonality. Hence, the period \\(2\\pi / \\omega = 12\\), which implies the angular frequency \\(\\omega = \\pi / 6\\). The frequency, measured in cycles per time unit, is given by the inverse \\[\n\\frac{\\omega}{2\\pi} = \\frac{1}{12} \\approx 0.08.\n\\]\nThe formulas to estimate parameters of regression in Equation 24.1 take a much simpler form if \\(\\omega\\) is one of the Fourier frequencies, defined by \\[\n\\omega_j=\\frac{2\\pi j}{T}, \\quad  j=0,\\dots, \\frac{T}{2},\n\\] then \\[\n\\begin{split}\n\\hat{A}&=\\frac{2}{T}\\sum_t Y_t\\cos \\omega_jt,\\\\\n\\hat{B}&=\\frac{2}{T}\\sum_t Y_t\\sin \\omega_jt,\\\\\n\\hat{C}&=\\overline{Y}=\\frac{1}{T}\\sum_tY_t.\n\\end{split}\n\\]\nA suitable way of testing the significance of the sinusoidal component with frequency \\(\\omega_j\\) is using its contribution to the sum of squares \\[\nR_T(\\omega_j)=\\frac{T}{2}\\left( \\hat{A}^2+\\hat{B}^2 \\right).\n\\] If the \\(\\epsilon_t \\sim N(0, \\sigma^2)\\), then it follows that \\(\\hat{A}\\) and \\(\\hat{B}\\) are also independent normal, each with the variance \\(2\\sigma^2/T\\), so under the null hypothesis of \\(A = B = 0\\) we find that \\[\\frac{R_T(\\omega_j)}{\\sigma^2}\\sim \\chi_2^2\\] or equivalently that \\(R_T(\\omega_j)/(2\\sigma^2)\\) has an exponential distribution with mean 1. The above theory can be extended to the simultaneous estimation of several periodic components.",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "memory-poisoning.html#periodogram",
    "href": "memory-poisoning.html#periodogram",
    "title": "\n12  Adversarial LLMs1\n",
    "section": "\n12.3 Periodogram",
    "text": "12.3 Periodogram\nThe Fourier transform uses Fourier series, such as the pairs of sines and cosines with different periods, to describe the frequencies present in the original time series.\nThe Fourier transform applied to an equally-spaced time series \\(Y_t\\) (where \\(t = 1,\\dots,T\\)) is also called the discrete Fourier transform (DFT) because the time is discrete (not the values \\(Y_t\\)).\nTo reduce the computational complexity of DFT and speed up the computations, one of the fast Fourier transform (FFT) algorithms is typically used.\nThe results of the Fourier transform are shown in a periodogram that describes the spectral properties of the signal.\nThe periodogram is defined as \\[\nI_T(\\omega) = \\frac{1}{2\\pi T}\\left| \\sum_{t=1}^T Y_te^{i\\omega t} \\right|^2,\n\\] which is an approximately unbiased estimator of the spectral density \\(f\\).\nSome undesirable features of the periodogram:\n\n\n\\(I_T(\\omega)\\) for fixed \\(\\omega\\) is not a consistent estimate of \\(f(\\omega)\\), since \\[\nI_T(\\omega_j) \\sim \\frac{f(\\omega_j)}{2} \\chi^2_2.\n\\] Therefore, the variance of \\(f^2(\\omega)\\) does not tend to 0 as \\(T \\rightarrow \\infty\\).\nThe independence of periodogram ordinates at different Fourier frequencies suggests that the sample periodogram plotted as a function of \\(\\omega\\) will be extremely irregular.\n\nSuppose that \\(\\gamma(h)\\) is the autocovariance function of a stationary process and that \\(f(\\omega)\\) is the spectral density for the same process (\\(h\\) is the time lag and \\(\\omega\\) is the frequency). The autocovariance \\(\\gamma(h)\\) and the spectral density \\(f(\\omega)\\) are related: \\[\n\\gamma(h) = \\int_{-1/2}^{1/2} e^{2\\pi i \\omega h} f(\\omega) d \\omega,\n\\] and \\[\nf(\\omega) = \\sum_{h=-\\infty}^{+\\infty} \\gamma(h) e^{-2\\pi i \\omega h}.\n\\]\nIn the language of advanced calculus, the autocovariance and spectral density are Fourier transform pairs. These Fourier transform equations show that there is a direct link between the time domain representation and the frequency domain representation of a time series.",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "memory-poisoning.html#smoothing",
    "href": "memory-poisoning.html#smoothing",
    "title": "\n12  Adversarial LLMs1\n",
    "section": "\n12.4 Smoothing",
    "text": "12.4 Smoothing\nThe idea behind smoothing is to take weighted averages over neighboring frequencies to reduce the variability associated with individual periodogram values. However, such an operation necessarily introduces some bias into the estimation procedure. Theoretical studies focus on the amount of smoothing that is required to obtain an optimum trade-off between bias and variance. In practice, this usually means that the choice of a kernel and amount of smoothing is somewhat subjective.\nThe main form of a smoothed estimator is given by \\[\n\\hat{f}(\\lambda) = \\int^{\\pi}_{-\\pi}\\frac{1}{h}K\\left( \\frac{\\omega-\\lambda}{h}\\right)I_T(\\omega)d\\omega,\n\\] where \\(I_T(\\cdot)\\) is the periodogram based on \\(T\\) observations, \\(K(\\cdot)\\) is a kernel function, and \\(h\\) is the bandwidth. We usually take \\(K(\\cdot)\\) to be a nonnegative function, symmetric about 0 and integrating to 1. Thus, any symmetric density, such as the normal density, will work. In practice, however, it is more usual to take a kernel of finite range, such as the Epanechnikov kernel \\[\nK(x)=\\frac{3}{4\\sqrt{5}}\\left(1-\\frac{x^2}{5} \\right), \\quad -\\sqrt{5}\\leqslant x \\leqslant \\sqrt{5},\n\\] which is 0 outside the interval \\([-\\sqrt{5}, \\sqrt{5}]\\). This choice of kernel function has some optimality properties. However, in practice, this optimality is less important than the choice of bandwidth \\(h\\), which effectively controls the range over which the periodogram is smoothed.\nThere are some additional difficulties with the performance of the sample periodogram in the presence of a sinusoidal variation whose frequency is not one of the Fourier frequencies. This effect is known as leakage. The reason of leakage is that we always consider a truncated periodogram. Truncation implicitly assumes that the time series is periodic with a period \\(T\\), which, of course, is not always true. So we artificially introduce non-existent periodicities into the estimated spectrum, i.e., cause ‘leakage’ of the spectrum. (If the time series is perfectly periodic over \\(T\\) then there is no leakage.) The leakage can be treated using an operation of tapering on the periodogram, i.e., by choosing appropriate periodogram windows.\n\n\n\n\n\n\nNote\n\n\n\nWhen we work with periodograms, we lose all phase (relative location/time origin) information: the periodogram will be the same if all the data were circularly rotated to a new time origin, i.e., the observed data are treated as perfectly periodic.\n\n\nAnother popular choice to smooth a periodogram is the Daniell kernel (with parameter \\(m\\)). For a time series, it is a centered moving average of values between the times \\(t - m\\) and \\(t  + m\\) (inclusive). For example, the smoothing formula for a Daniell kernel with \\(m = 2\\) is \\[\n\\begin{split}\n\\hat{x}_t &= \\frac{x_{t-2}+x_{t-1}+x_t + x_{t+1}+x_{t+2}}{5} \\\\\n&= 0.2x_{t-2} + 0.2x_{t-1} +0.2x_t + 0.2x_{t+1} +0.2x_{t+2}.\n\\end{split}\n\\]\nThe weighting coefficients for a Daniell kernel with \\(m = 2\\) can be checked using the function kernel(). In the output, the indices in coef[ ] refer to the time difference from the center of the average at the time \\(t\\).\n\nkernel(\"daniell\", m = 2)\n\n#&gt; Daniell(2) \n#&gt; coef[-2] = 0.2\n#&gt; coef[-1] = 0.2\n#&gt; coef[ 0] = 0.2\n#&gt; coef[ 1] = 0.2\n#&gt; coef[ 2] = 0.2\n\n\nThe modified Daniell kernel is such that the two endpoints in the averaging receive half the weight that the interior points do. For a modified Daniell kernel with \\(m = 2\\), the smoothing is \\[\n\\begin{split}\n\\hat{x}_t &= \\frac{x_{t-2}+2x_{t-1}+2x_t + 2x_{t+1} + x_{t+2}}{8} \\\\\n& = 0.125x_{t-2} +0.25x_{t-1}+0.25x_t+0.25x_{t+1}+0.125x_{t+2}\n\\end{split}\n\\]\nList the weighting coefficients:\n\nkernel(\"modified.daniell\", m = 2)\n\n#&gt; mDaniell(2) \n#&gt; coef[-2] = 0.125\n#&gt; coef[-1] = 0.250\n#&gt; coef[ 0] = 0.250\n#&gt; coef[ 1] = 0.250\n#&gt; coef[ 2] = 0.125\n\n\nEither the Daniell kernel or the modified Daniell kernel can be convoluted (repeated) so that the smoothing is applied again to the smoothed values. This produces a more extensive smoothing by averaging over a wider time interval. For instance, to repeat a Daniell kernel with \\(m = 2\\) on the smoothed values that resulted from a Daniell kernel with \\(m = 2\\), the formula would be \\[\n\\hat{\\hat{x}}_t = \\frac{\\hat{x}_{t-2}+\\hat{x}_{t-1}+\\hat{x}_t +\\hat{x}_{t+1}+\\hat{x}_{t+2}}{5}.\n\\]\nThe weights for averaging the original data for convoluted Daniell kernels with \\(m = 2\\) in both smooths will be\n\nkernel(\"daniell\", m = c(2, 2))\n\n#&gt; Daniell(2,2) \n#&gt; coef[-4] = 0.04\n#&gt; coef[-3] = 0.08\n#&gt; coef[-2] = 0.12\n#&gt; coef[-1] = 0.16\n#&gt; coef[ 0] = 0.20\n#&gt; coef[ 1] = 0.16\n#&gt; coef[ 2] = 0.12\n#&gt; coef[ 3] = 0.08\n#&gt; coef[ 4] = 0.04\n\nkernel(\"modified.daniell\", m = c(2, 2))\n\n#&gt; mDaniell(2,2) \n#&gt; coef[-4] = 0.0156\n#&gt; coef[-3] = 0.0625\n#&gt; coef[-2] = 0.1250\n#&gt; coef[-1] = 0.1875\n#&gt; coef[ 0] = 0.2188\n#&gt; coef[ 1] = 0.1875\n#&gt; coef[ 2] = 0.1250\n#&gt; coef[ 3] = 0.0625\n#&gt; coef[ 4] = 0.0156\n\n\nThe center values are weighted slightly more heavily in the modified Daniell kernel than in the unmodified one.\nWhen we smooth a periodogram, we are smoothing across frequencies rather than across times.\n\n\n\n\n\n\nExample: Spectral analysis of the airline passenger data\n\n\n\nRecall the airline passenger time series (?fig-airpassangers) that has an increasing trend and strong multiplicative seasonality.\nThe series should be detrended before a spectral analysis. For demonstration purposes, compute periodogram for the raw data and log-transformed and detrended.\n\n# Fit a quadratic trend to log-transformed data\nt &lt;- as.vector(time(AirPassengers))\nmod &lt;- lm(log10(AirPassengers) ~ poly(t, degree = 2))\nres &lt;- ts(mod$residuals)\nattributes(res) &lt;- attributes(AirPassengers)\n\n# Compute spectra\nspec_raw &lt;- spec.pgram(AirPassengers, detrend = FALSE, plot = FALSE)\nspec_log_detrend &lt;- spec.pgram(res, detrend = FALSE, plot = FALSE)\n\nThe \\(x\\)-axes of the periodograms correspond to \\(\\omega / (2\\pi)\\) or the number of cycles per the time series period specified in the ts object (12 months). If the frequency of 12 was not specified for this time series, the \\(x\\)-axes would be different, and the first spectrum peak would correspond to \\(\\approx 0.08\\).\nFigure 24.1 B shows that the spectrum of the raw data is dominated by low frequencies (trend). After detrending the data (Figure 24.1 C), the spectrum at frequency 1 (meaning one cycle per year) is the dominant one.\n\nCodepAirPassengers &lt;- forecast::autoplot(AirPassengers, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Airline passengers (thousand)\") +\n    ggtitle(\"Raw series\")\np2 &lt;- data.frame(Spectrum = spec_raw$spec, \n                 Frequency = spec_raw$freq) %&gt;% \n    ggplot(aes(x = Frequency, y = Spectrum)) +\n    scale_y_continuous(trans = 'log10') +\n    geom_line() + \n    ggtitle(\"\")\np3 &lt;- forecast::autoplot(res, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Airline passengers residuals (lg[thousand])\") + \n    ggtitle(\"Detrended and log-transformed series\")\np4 &lt;- data.frame(Spectrum = spec_log_detrend$spec, \n                 Frequency = spec_log_detrend$freq) %&gt;% \n    ggplot(aes(x = Frequency, y = Spectrum)) +\n    scale_y_continuous(trans = 'log10') +\n    geom_line() + \n    ggtitle(\"\")\n(pAirPassengers + p2) / (p3 + p4) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 12.1: Monthly AirPassengers time series, log-transformed and detrended, and the corresponding fast Fourier transforms.\n\n\n\n\nNote that the function spec.pgram() has the option of removing a simpler (linear) trend and showing the results as a base-R plot. The confidence band in the upper right corner of this plot helps to identify the statistical significance of the peaks (Figure 24.2).\n\nCodepar(mfrow = c(2, 2))\nspec.pgram(res, spans = c(3))\nspec.pgram(res, spans = c(3, 3))\nspec.pgram(res, spans = c(7, 7))\nspec.pgram(res, spans = c(11, 11))\n\n\n\n\n\n\nFigure 12.2: Smoothed periodograms of monthly log-transformed and detrended AirPassengers time series.\n\n\n\n\nAnother method of testing the reality of a peak is to look at its harmonics. It is extremely unlikely that a true cycle will be shaped perfectly as a sine curve, hence at least a few of the first harmonics will show up as well. For example, in monthly data with annual seasonality (period of 12 months), we often observe peaks at 6, 4, and 3 months. This is exactly the pattern that we see in the figures above.\nWe can also approximate our data with an AR model and then plot the approximating periodogram of the AR model (Figure 24.3).\n\nCodespectrum(res, method = \"ar\")\n\n\n\n\n\n\nFigure 12.3: Periodogram of AR process approximating the monthly log-transformed and detrended AirPassengers time series.",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "memory-poisoning.html#periodogram-for-unequally-spaced-time-series",
    "href": "memory-poisoning.html#periodogram-for-unequally-spaced-time-series",
    "title": "\n12  Adversarial LLMs1\n",
    "section": "\n12.5 Periodogram for unequally-spaced time series",
    "text": "12.5 Periodogram for unequally-spaced time series\nUnequally/unevenly-spaced time series or gaps in observations (missing data) can be handled with the Lomb–Scargle periodogram calculation. This method was originally developed for the analysis of unevenly-spaced astronomical signals (Lomb 1976; Scargle 1982) but found application in many other domains, including environmental science (e.g., Ruf 1999; Campos et al. 2008; Siskey et al. 2016).\n\n\n\n\n\n\nExample: Ammonium concentration in wastewater system\n\n\n\nFor example, consider a time series imputeTS::tsNH4 of ammonium (NH\\(_4\\)) concentration in a wastewater system (Figure 24.4). This time series contains observations from regular 10-minute intervals, but some of the observations are missing.\n\nCodeforecast::autoplot(imputeTS::tsNH4) +\n    xlab(\"Day\") +\n    ylab(bquote(NH[4]~concentration))\n\n\n\n\n\n\nFigure 12.4: Time series of ammonium concentration in a wastewater system.\n\n\n\n\nSave the data in a format acceptable by the Lomb–Scargle function.\n\nD &lt;- data.frame(NH4 = as.vector(imputeTS::tsNH4),\n                Time = as.vector(time(imputeTS::tsNH4)))\n\nIf needed, na.omit(D) can be used to remove the rows with missing values.\nFigure 24.5 and Figure 24.6 show periodograms calculated based on these data. Note that the function lomb::lsp() has arguments adjusting the span of the frequencies and significance level.\n\nCodepar(mfrow = c(2, 2))\nlomb::lsp(D$NH4, times = D$Time, type = \"frequency\", alpha = 0.05, main = \"\")\n\n\n\n\n\n\nFigure 12.5: Lomb–Scargle periodograms computed for the unequally-spaced time series of ammonium concentration.\n\n\n\n\n\nCodelomb::lsp(D$NH4, times = D$Time, type = \"period\", alpha = 0.05, main = \"\")\n\n\n\n\n\n\nFigure 12.6: Lomb–Scargle periodograms computed for the unequally-spaced time series of ammonium concentration.",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "memory-poisoning.html#frequency-estimation-in-time",
    "href": "memory-poisoning.html#frequency-estimation-in-time",
    "title": "\n12  Adversarial LLMs1\n",
    "section": "\n12.6 Frequency estimation in time",
    "text": "12.6 Frequency estimation in time\nThe assumption of a whole time series having a constant spectrum might be very limiting for the analysis of long time series. An estimation of frequencies locally in time allows us to study how the spectrum changes in time, and also detect smaller (short-lived) signals that would be averaged out otherwise. A straightforward solution is to separate the time series into windows (sub-periods) and estimate a spectrum in each such window.\nThe windowed analysis makes sense when in each window we have enough observations to estimate the frequencies. As a guide, we should have at least two observations per period to be able to represent the signal in the discrete Fourier transform. For example, we cannot estimate the seasonality if we sample once per year – this is our sampling rate or sampling frequency \\(F_s\\). To start identifying seasonal periodicity, our sampling rate should be at least 2 samples per year. The highest frequency we can work with is limited by the Nyquist frequency \\[\nF_N = \\frac{F_s}{2},\n\\] which is half of the sampling frequency.\nAfter applying the FFT in each window, the results can be visualized in a spectrogram. The spectrogram combines information from multiple periodograms: it shows time on the \\(x\\)-axis, frequency on the \\(y\\)-axis, and the corresponding spectrum power as the color.\n\n\n\n\n\n\nExample: Spectrograms for the NAO\n\n\n\nConsider a long time series of the North Atlantic Oscillation (NAO) index obtained from an ensemble of 100 model-constrained NAO reconstructions (Figure 24.7).\n\nCodeNAOdf &lt;- readr::read_csv(\"data/NAO.csv\", skip = 12, show_col_types = FALSE) %&gt;% \n    rename(NAOproxy = nao_mc_mean) %&gt;% \n    select(Year, NAOproxy) %&gt;% \n    arrange(Year)\nNAO &lt;- ts(NAOdf$NAOproxy, start = NAOdf$Year[1])\nspec_raw &lt;- spec.pgram(NAO, detrend = FALSE, plot = FALSE, spans = 3)\n\n# Find the frequency with the max power\nfmax &lt;- spec_raw$freq[which.max(spec_raw$spec)]\n\n\n\nCodep1 &lt;- forecast::autoplot(NAO) +\n    xlab(\"Year\") +\n    ylab(\"NAO\")\np2 &lt;- data.frame(Spectrum = spec_raw$spec, \n                 Frequency = spec_raw$freq) %&gt;% \n    ggplot(aes(x = Frequency, y = Spectrum)) +\n    scale_y_continuous(trans = 'log10') +\n    geom_vline(xintercept = fmax, col = \"blue\", lty = 2) + \n    geom_line() + \n    ggtitle(\"\")\np3 &lt;- data.frame(Spectrum = spec_raw$spec, \n                 Period = 1/spec_raw$freq) %&gt;% \n    ggplot(aes(x = Period, y = Spectrum)) +\n    scale_y_continuous(trans = 'log10') +\n    geom_vline(xintercept = 1/fmax, col = \"blue\", lty = 2) +\n    geom_line() + \n    ggtitle(\"\")\np1 / (p2 + p3) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 12.7: North Atlantic Oscillation (NAO) winter index (December, January, and February) calculated as the mean of 100 model-constrained NAO reconstructions, along with its smoothed periodograms. The dashed lines denote the maximal magnitude of the spectrum.\n\n\n\n\nThis is an annual time series, hence the sampling frequency \\(F_s = 1\\). From the global FFT results, the frequency with the maximal power is 0.019 year\\(^{-1}\\), which is equivalent to the period of about 53.3 years. We will set the window for the FFT and calculate the spectrogram. Note that we can use overlapping windows for a smoother picture.\n\n# Set the window size (years), for applying the FFT in each window\nwindow_size = 30\n\n# Compute the spectrogram\nSP &lt;- signal::specgram(x = NAO,\n                       n = window_size,\n                       overlap = window_size/3,\n                       Fs = 1)\n\nSee the spectrograms in Figure 24.8 and compare them with the time series plot in Figure 24.7 A.\n\nCodepar(mfrow = c(1, 2))\n\n# Plot the spectrogram without decorations\nprint(SP, main = \"A) Raw results\")\n\n# Discard phase information\nP &lt;- abs(SP$S)\n\n# Normalize the spectrum to the range [0, 1]\nP &lt;- P - min(P)\nP &lt;- P/max(P)\n\n# Extract time\nYears &lt;- time(NAO)[SP$t]\n\n# Plot the spectrogram after processing\noce::imagep(x = Years,\n            y = SP$f,\n            z = t(P),\n            col = oce::oce.colorsViridis,\n            ylab = expression(Frequency~(y^-1 )),\n            xlab = \"Year\",\n            main = \"B) After normalization and adding colors\",\n            drawPalette = TRUE,\n            decimate = FALSE)\n\n\n\n\n\n\nFigure 12.8: Spectrograms for North Atlantic Oscillation (NAO) winter index.",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "memory-poisoning.html#conclusion",
    "href": "memory-poisoning.html#conclusion",
    "title": "\n12  Adversarial LLMs1\n",
    "section": "\n12.7 Conclusion",
    "text": "12.7 Conclusion\nFor challenging problems, smoothing, multitapering, linear filtering, (repeated) pre-whitening, and Lomb–Scargle can be used together. Beware that aperiodic but autoregressive processes produce peaks in spectral densities. Harmonic analysis is a complicated art rather than a straightforward procedure.\nIt is extremely difficult to derive the significance of a weak periodicity from harmonic analysis. Do not believe analytical estimates (e.g., exponential probability), as they rarely apply to real data. It is essential to make simulations, typically permuting or bootstrapping the data keeping the observing times fixed. Simulations of the final model with the observation times are also advised.",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "memory-poisoning.html#appendix",
    "href": "memory-poisoning.html#appendix",
    "title": "\n12  Adversarial LLMs1\n",
    "section": "\n12.8 Appendix",
    "text": "12.8 Appendix\nWavelet analysis\nAn alternative to the windowed FFT is the wavelet analysis, which also estimates the strength of time series variations for different frequencies and times. Wavelet is a ‘small wave’ or function \\(\\psi(t)\\) approximating the variations. Popular wavelet functions are Meyer, Morlet, and Mexican hat.\nFigure 24.9 shows the results of using the Morlet wavelet to process the NAO time series.\n\nCodelibrary(dplR)\nwv &lt;- morlet(y1 = NAOdf$NAOproxy, x1 = NAOdf$Year)\nlevs &lt;- quantile(wv$Power, probs = c(0, 0.5, 0.75, 0.9, 0.95, 1))\nwavelet.plot(wv, wavelet.levels = levs)\n\n\n\n\n\n\nFigure 12.9: Wavelet analysis of the North Atlantic Oscillation (NAO) winter index.\n\n\n\n\n\n\n\n\nCampos MC, Costa JL, Quintella BR, et al (2008) Activity and movement patterns of the Lusitanian toadfish inferred from pressure-sensitive data-loggers in the Mira estuary (Portugal). Fisheries Management and Ecology 15:449–458. https://doi.org/10.1111/j.1365-2400.2008.00629.x\n\n\nLomb NR (1976) Least-squares frequency analysis of unequally spaced data. Astrophysics and Space Science 39:447–462. https://doi.org/10.1007/BF00648343\n\n\nNason GP (2008) Wavelet methods in statistics with R. Springer, New York, NY, USA\n\n\nRuf T (1999) The Lomb–Scargle periodogram in biological rhythm research: Analysis of incomplete and unequally spaced time-series. Biological Rhythm Research 30:178–201. https://doi.org/10.1076/brhm.30.2.178.1422\n\n\nScargle JD (1982) Studies in astronomical time series analysis. II – statistical aspects of spectral analysis of unevenly spaced data. Astrophysical Journal 263:835–853. https://doi.org/10.1086/160554\n\n\nShumway RH, Stoffer DS (2017) Time series analysis and its applications with R examples, 4th edn. Springer, New York, NY, USA\n\n\nSiskey MR, Lyubchich V, Liang D, et al (2016) Periodicity of strontium:calcium across annuli further validates otolith-ageing for Atlantic bluefin tuna (Thunnus thynnus). Fisheries Research 177:13–17. https://doi.org/10.1016/j.fishres.2016.01.004",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "rag-poisoning.html",
    "href": "rag-poisoning.html",
    "title": "13  Fundamental of AI and Cybersecurity",
    "section": "",
    "text": "Coming Soon!!",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fundamental of AI and Cybersecurity</span>"
    ]
  },
  {
    "objectID": "jailbreak-automation.html",
    "href": "jailbreak-automation.html",
    "title": "14  Fundamental of AI and Cybersecurity",
    "section": "",
    "text": "Coming Soon!!",
    "crumbs": [
      "Part III — Attacks on Large Language Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamental of AI and Cybersecurity</span>"
    ]
  },
  {
    "objectID": "defensive-prompts.html",
    "href": "defensive-prompts.html",
    "title": "15  Fundamental of AI and Cybersecurity",
    "section": "",
    "text": "Coming Soon!!",
    "crumbs": [
      "Part IV — Defenses for Large Language Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fundamental of AI and Cybersecurity</span>"
    ]
  },
  {
    "objectID": "tool-gating.html",
    "href": "tool-gating.html",
    "title": "16  Fundamental of AI and Cybersecurity",
    "section": "",
    "text": "Coming Soon!!",
    "crumbs": [
      "Part IV — Defenses for Large Language Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Fundamental of AI and Cybersecurity</span>"
    ]
  },
  {
    "objectID": "rag-security.html",
    "href": "rag-security.html",
    "title": "17  Fundamental of AI and Cybersecurity",
    "section": "",
    "text": "Coming Soon!!",
    "crumbs": [
      "Part IV — Defenses for Large Language Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fundamental of AI and Cybersecurity</span>"
    ]
  },
  {
    "objectID": "monitoring.html",
    "href": "monitoring.html",
    "title": "18  Fundamental of AI and Cybersecurity",
    "section": "",
    "text": "Coming Soon!!",
    "crumbs": [
      "Part IV — Defenses for Large Language Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fundamental of AI and Cybersecurity</span>"
    ]
  },
  {
    "objectID": "governance.html",
    "href": "governance.html",
    "title": "19  Fundamental of AI and Cybersecurity",
    "section": "",
    "text": "Coming Soon!!",
    "crumbs": [
      "Part IV — Defenses for Large Language Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fundamental of AI and Cybersecurity</span>"
    ]
  },
  {
    "objectID": "safe-rl.html",
    "href": "safe-rl.html",
    "title": "20  Fundamental of AI and Cybersecurity",
    "section": "",
    "text": "Coming Soon!!",
    "crumbs": [
      "Part V — Safe AI for Cyber-Physical Systems",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Fundamental of AI and Cybersecurity</span>"
    ]
  },
  {
    "objectID": "cps-attacks.html",
    "href": "cps-attacks.html",
    "title": "21  Fundamental of AI and Cybersecurity",
    "section": "",
    "text": "Coming Soon!!",
    "crumbs": [
      "Part V — Safe AI for Cyber-Physical Systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fundamental of AI and Cybersecurity</span>"
    ]
  },
  {
    "objectID": "research-methods.html",
    "href": "research-methods.html",
    "title": "23  Fundamental of AI and Cybersecurity",
    "section": "",
    "text": "Coming Soon!!",
    "crumbs": [
      "Part VI — Methods for Research, Evaluation & Reproducibility",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Fundamental of AI and Cybersecurity</span>"
    ]
  },
  {
    "objectID": "research-directions.html",
    "href": "research-directions.html",
    "title": "\n24  Adversarial LLMs1\n",
    "section": "",
    "text": "24.1 Introduction\nThe goal of this lecture\nA adversarial attacks is typically derived based on an assumed threat model regarding the goal, knowledge, and target strategy of the adversary Adversary’s goal Poisoning attack, evasion attack: cause the ML model to perform incorrectly Privacy attack: acquire knowledge about the training data or the model Availability attack: cause the ML model to be become unavailable Adversary’s knowledge White-box attack: the adversary has full knowledge of the ML model Black-box attack: has no knowledge of the ML model Gray-box attack: has some knowledge of the ML model Adversary’s target strategy Targeted attack: cause the ML model to output a target label for an input Non-targeted attack: cause the ML to output any incorrect label for an input\nA adversarial attacks is typically derived based on an assumed threat model regarding the goal, knowledge, and target strategy of the adversary\nAdversarial Machine Learning is a field that studies the vulnerabilities of machine learning models to malicious manipulation and develops techniques to make them more robust. Core Concept At its heart, adversarial ML involves creating adversarial examples - inputs that are intentionally designed to fool machine learning models. These are often imperceptible or subtle modifications to legitimate inputs that cause models to make incorrect predictions. Classic example: Adding carefully calculated noise to an image of a panda that’s invisible to humans, but causes an image classifier to confidently misidentify it as a gibbon.\nAdversarial Machine Learning is the study of attacks against ML systems and defenses against them. The key insight is that every adversarial attack operates within a threat model defined by three dimensions:\nObjectives\nReading materials\nBy now, we have been working in the time domain, such that our analysis could be seen as a regression of the present on the past (for example, ARIMA models). We have been using many time series plots with time on the \\(x\\)-axis.\nAn alternative approach is to analyze time series in a spectral domain, such that use a regression of present on a linear combination of sine and cosine functions. In this type of analysis, we often use periodogram plots, with frequency or period on the \\(x\\)-axis.",
    "crumbs": [
      "Part VI — Methods for Research, Evaluation & Reproducibility",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "research-directions.html#regression-on-sinusoidal-components",
    "href": "research-directions.html#regression-on-sinusoidal-components",
    "title": "\n24  Adversarial LLMs1\n",
    "section": "\n24.2 Regression on sinusoidal components",
    "text": "24.2 Regression on sinusoidal components\nThe simplest form of spectral analysis consists of regression on a periodic component: \\[\nY_t = A\\cos \\omega t + B \\sin \\omega t + C + \\epsilon_t,\n\\tag{24.1}\\] where \\(t =1,\\dots,T\\) and \\(\\epsilon_t \\sim \\mathrm{WN}(0, \\sigma^2)\\). Without loss of generality, we assume \\(0 \\leqslant \\omega \\leqslant \\pi\\). In fact, for discrete data, frequencies outside this range are aliased into this range. For example, suppose that \\(-\\pi &lt; (\\omega = \\pi - \\delta) &lt; 0\\), then \\[\n\\begin{split}\n\\cos(\\omega t) &= \\cos((\\pi - \\delta)t) \\\\\n& = \\cos(\\pi t) \\cos(\\delta t) + \\sin(\\pi t) \\sin(\\delta t) \\\\\n& = \\cos(\\delta t).\n\\end{split}\n\\] Hence, a sampled sinusoid with a frequency smaller than 0 appears to coincide with a sinusoid with frequency in the interval \\([0, \\pi]\\).\n\n\n\n\n\n\nNote\n\n\n\nThe term \\(A\\cos \\omega t + B \\sin \\omega t\\) is a periodic function with the period \\(2\\pi / \\omega\\). The period \\(2\\pi / \\omega\\) represents the number of time units that it takes for the function to take the same value again, i.e., to complete a cycle. The frequency, measured in cycles per time unit, is given by the inverse \\(\\omega / (2 \\pi)\\). The angular frequency, measured in radians per time unit, is given by \\(\\omega\\). Because of its convenience, the angular frequency \\(\\omega\\) will be used to describe the periodicity of the function, and its name is shortened to frequency when there is no danger of confusion.\n\n\nConsider monthly data that exhibit a 12-month seasonality. Hence, the period \\(2\\pi / \\omega = 12\\), which implies the angular frequency \\(\\omega = \\pi / 6\\). The frequency, measured in cycles per time unit, is given by the inverse \\[\n\\frac{\\omega}{2\\pi} = \\frac{1}{12} \\approx 0.08.\n\\]\nThe formulas to estimate parameters of regression in Equation 24.1 take a much simpler form if \\(\\omega\\) is one of the Fourier frequencies, defined by \\[\n\\omega_j=\\frac{2\\pi j}{T}, \\quad  j=0,\\dots, \\frac{T}{2},\n\\] then \\[\n\\begin{split}\n\\hat{A}&=\\frac{2}{T}\\sum_t Y_t\\cos \\omega_jt,\\\\\n\\hat{B}&=\\frac{2}{T}\\sum_t Y_t\\sin \\omega_jt,\\\\\n\\hat{C}&=\\overline{Y}=\\frac{1}{T}\\sum_tY_t.\n\\end{split}\n\\]\nA suitable way of testing the significance of the sinusoidal component with frequency \\(\\omega_j\\) is using its contribution to the sum of squares \\[\nR_T(\\omega_j)=\\frac{T}{2}\\left( \\hat{A}^2+\\hat{B}^2 \\right).\n\\] If the \\(\\epsilon_t \\sim N(0, \\sigma^2)\\), then it follows that \\(\\hat{A}\\) and \\(\\hat{B}\\) are also independent normal, each with the variance \\(2\\sigma^2/T\\), so under the null hypothesis of \\(A = B = 0\\) we find that \\[\\frac{R_T(\\omega_j)}{\\sigma^2}\\sim \\chi_2^2\\] or equivalently that \\(R_T(\\omega_j)/(2\\sigma^2)\\) has an exponential distribution with mean 1. The above theory can be extended to the simultaneous estimation of several periodic components.",
    "crumbs": [
      "Part VI — Methods for Research, Evaluation & Reproducibility",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "research-directions.html#periodogram",
    "href": "research-directions.html#periodogram",
    "title": "\n24  Adversarial LLMs1\n",
    "section": "\n24.3 Periodogram",
    "text": "24.3 Periodogram\nThe Fourier transform uses Fourier series, such as the pairs of sines and cosines with different periods, to describe the frequencies present in the original time series.\nThe Fourier transform applied to an equally-spaced time series \\(Y_t\\) (where \\(t = 1,\\dots,T\\)) is also called the discrete Fourier transform (DFT) because the time is discrete (not the values \\(Y_t\\)).\nTo reduce the computational complexity of DFT and speed up the computations, one of the fast Fourier transform (FFT) algorithms is typically used.\nThe results of the Fourier transform are shown in a periodogram that describes the spectral properties of the signal.\nThe periodogram is defined as \\[\nI_T(\\omega) = \\frac{1}{2\\pi T}\\left| \\sum_{t=1}^T Y_te^{i\\omega t} \\right|^2,\n\\] which is an approximately unbiased estimator of the spectral density \\(f\\).\nSome undesirable features of the periodogram:\n\n\n\\(I_T(\\omega)\\) for fixed \\(\\omega\\) is not a consistent estimate of \\(f(\\omega)\\), since \\[\nI_T(\\omega_j) \\sim \\frac{f(\\omega_j)}{2} \\chi^2_2.\n\\] Therefore, the variance of \\(f^2(\\omega)\\) does not tend to 0 as \\(T \\rightarrow \\infty\\).\nThe independence of periodogram ordinates at different Fourier frequencies suggests that the sample periodogram plotted as a function of \\(\\omega\\) will be extremely irregular.\n\nSuppose that \\(\\gamma(h)\\) is the autocovariance function of a stationary process and that \\(f(\\omega)\\) is the spectral density for the same process (\\(h\\) is the time lag and \\(\\omega\\) is the frequency). The autocovariance \\(\\gamma(h)\\) and the spectral density \\(f(\\omega)\\) are related: \\[\n\\gamma(h) = \\int_{-1/2}^{1/2} e^{2\\pi i \\omega h} f(\\omega) d \\omega,\n\\] and \\[\nf(\\omega) = \\sum_{h=-\\infty}^{+\\infty} \\gamma(h) e^{-2\\pi i \\omega h}.\n\\]\nIn the language of advanced calculus, the autocovariance and spectral density are Fourier transform pairs. These Fourier transform equations show that there is a direct link between the time domain representation and the frequency domain representation of a time series.",
    "crumbs": [
      "Part VI — Methods for Research, Evaluation & Reproducibility",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "research-directions.html#smoothing",
    "href": "research-directions.html#smoothing",
    "title": "\n24  Adversarial LLMs1\n",
    "section": "\n24.4 Smoothing",
    "text": "24.4 Smoothing\nThe idea behind smoothing is to take weighted averages over neighboring frequencies to reduce the variability associated with individual periodogram values. However, such an operation necessarily introduces some bias into the estimation procedure. Theoretical studies focus on the amount of smoothing that is required to obtain an optimum trade-off between bias and variance. In practice, this usually means that the choice of a kernel and amount of smoothing is somewhat subjective.\nThe main form of a smoothed estimator is given by \\[\n\\hat{f}(\\lambda) = \\int^{\\pi}_{-\\pi}\\frac{1}{h}K\\left( \\frac{\\omega-\\lambda}{h}\\right)I_T(\\omega)d\\omega,\n\\] where \\(I_T(\\cdot)\\) is the periodogram based on \\(T\\) observations, \\(K(\\cdot)\\) is a kernel function, and \\(h\\) is the bandwidth. We usually take \\(K(\\cdot)\\) to be a nonnegative function, symmetric about 0 and integrating to 1. Thus, any symmetric density, such as the normal density, will work. In practice, however, it is more usual to take a kernel of finite range, such as the Epanechnikov kernel \\[\nK(x)=\\frac{3}{4\\sqrt{5}}\\left(1-\\frac{x^2}{5} \\right), \\quad -\\sqrt{5}\\leqslant x \\leqslant \\sqrt{5},\n\\] which is 0 outside the interval \\([-\\sqrt{5}, \\sqrt{5}]\\). This choice of kernel function has some optimality properties. However, in practice, this optimality is less important than the choice of bandwidth \\(h\\), which effectively controls the range over which the periodogram is smoothed.\nThere are some additional difficulties with the performance of the sample periodogram in the presence of a sinusoidal variation whose frequency is not one of the Fourier frequencies. This effect is known as leakage. The reason of leakage is that we always consider a truncated periodogram. Truncation implicitly assumes that the time series is periodic with a period \\(T\\), which, of course, is not always true. So we artificially introduce non-existent periodicities into the estimated spectrum, i.e., cause ‘leakage’ of the spectrum. (If the time series is perfectly periodic over \\(T\\) then there is no leakage.) The leakage can be treated using an operation of tapering on the periodogram, i.e., by choosing appropriate periodogram windows.\n\n\n\n\n\n\nNote\n\n\n\nWhen we work with periodograms, we lose all phase (relative location/time origin) information: the periodogram will be the same if all the data were circularly rotated to a new time origin, i.e., the observed data are treated as perfectly periodic.\n\n\nAnother popular choice to smooth a periodogram is the Daniell kernel (with parameter \\(m\\)). For a time series, it is a centered moving average of values between the times \\(t - m\\) and \\(t  + m\\) (inclusive). For example, the smoothing formula for a Daniell kernel with \\(m = 2\\) is \\[\n\\begin{split}\n\\hat{x}_t &= \\frac{x_{t-2}+x_{t-1}+x_t + x_{t+1}+x_{t+2}}{5} \\\\\n&= 0.2x_{t-2} + 0.2x_{t-1} +0.2x_t + 0.2x_{t+1} +0.2x_{t+2}.\n\\end{split}\n\\]\nThe weighting coefficients for a Daniell kernel with \\(m = 2\\) can be checked using the function kernel(). In the output, the indices in coef[ ] refer to the time difference from the center of the average at the time \\(t\\).\n\nkernel(\"daniell\", m = 2)\n\n#&gt; Daniell(2) \n#&gt; coef[-2] = 0.2\n#&gt; coef[-1] = 0.2\n#&gt; coef[ 0] = 0.2\n#&gt; coef[ 1] = 0.2\n#&gt; coef[ 2] = 0.2\n\n\nThe modified Daniell kernel is such that the two endpoints in the averaging receive half the weight that the interior points do. For a modified Daniell kernel with \\(m = 2\\), the smoothing is \\[\n\\begin{split}\n\\hat{x}_t &= \\frac{x_{t-2}+2x_{t-1}+2x_t + 2x_{t+1} + x_{t+2}}{8} \\\\\n& = 0.125x_{t-2} +0.25x_{t-1}+0.25x_t+0.25x_{t+1}+0.125x_{t+2}\n\\end{split}\n\\]\nList the weighting coefficients:\n\nkernel(\"modified.daniell\", m = 2)\n\n#&gt; mDaniell(2) \n#&gt; coef[-2] = 0.125\n#&gt; coef[-1] = 0.250\n#&gt; coef[ 0] = 0.250\n#&gt; coef[ 1] = 0.250\n#&gt; coef[ 2] = 0.125\n\n\nEither the Daniell kernel or the modified Daniell kernel can be convoluted (repeated) so that the smoothing is applied again to the smoothed values. This produces a more extensive smoothing by averaging over a wider time interval. For instance, to repeat a Daniell kernel with \\(m = 2\\) on the smoothed values that resulted from a Daniell kernel with \\(m = 2\\), the formula would be \\[\n\\hat{\\hat{x}}_t = \\frac{\\hat{x}_{t-2}+\\hat{x}_{t-1}+\\hat{x}_t +\\hat{x}_{t+1}+\\hat{x}_{t+2}}{5}.\n\\]\nThe weights for averaging the original data for convoluted Daniell kernels with \\(m = 2\\) in both smooths will be\n\nkernel(\"daniell\", m = c(2, 2))\n\n#&gt; Daniell(2,2) \n#&gt; coef[-4] = 0.04\n#&gt; coef[-3] = 0.08\n#&gt; coef[-2] = 0.12\n#&gt; coef[-1] = 0.16\n#&gt; coef[ 0] = 0.20\n#&gt; coef[ 1] = 0.16\n#&gt; coef[ 2] = 0.12\n#&gt; coef[ 3] = 0.08\n#&gt; coef[ 4] = 0.04\n\nkernel(\"modified.daniell\", m = c(2, 2))\n\n#&gt; mDaniell(2,2) \n#&gt; coef[-4] = 0.0156\n#&gt; coef[-3] = 0.0625\n#&gt; coef[-2] = 0.1250\n#&gt; coef[-1] = 0.1875\n#&gt; coef[ 0] = 0.2188\n#&gt; coef[ 1] = 0.1875\n#&gt; coef[ 2] = 0.1250\n#&gt; coef[ 3] = 0.0625\n#&gt; coef[ 4] = 0.0156\n\n\nThe center values are weighted slightly more heavily in the modified Daniell kernel than in the unmodified one.\nWhen we smooth a periodogram, we are smoothing across frequencies rather than across times.\n\n\n\n\n\n\nExample: Spectral analysis of the airline passenger data\n\n\n\nRecall the airline passenger time series (?fig-airpassangers) that has an increasing trend and strong multiplicative seasonality.\nThe series should be detrended before a spectral analysis. For demonstration purposes, compute periodogram for the raw data and log-transformed and detrended.\n\n# Fit a quadratic trend to log-transformed data\nt &lt;- as.vector(time(AirPassengers))\nmod &lt;- lm(log10(AirPassengers) ~ poly(t, degree = 2))\nres &lt;- ts(mod$residuals)\nattributes(res) &lt;- attributes(AirPassengers)\n\n# Compute spectra\nspec_raw &lt;- spec.pgram(AirPassengers, detrend = FALSE, plot = FALSE)\nspec_log_detrend &lt;- spec.pgram(res, detrend = FALSE, plot = FALSE)\n\nThe \\(x\\)-axes of the periodograms correspond to \\(\\omega / (2\\pi)\\) or the number of cycles per the time series period specified in the ts object (12 months). If the frequency of 12 was not specified for this time series, the \\(x\\)-axes would be different, and the first spectrum peak would correspond to \\(\\approx 0.08\\).\nFigure 24.1 B shows that the spectrum of the raw data is dominated by low frequencies (trend). After detrending the data (Figure 24.1 C), the spectrum at frequency 1 (meaning one cycle per year) is the dominant one.\n\nCodepAirPassengers &lt;- forecast::autoplot(AirPassengers, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Airline passengers (thousand)\") +\n    ggtitle(\"Raw series\")\np2 &lt;- data.frame(Spectrum = spec_raw$spec, \n                 Frequency = spec_raw$freq) %&gt;% \n    ggplot(aes(x = Frequency, y = Spectrum)) +\n    scale_y_continuous(trans = 'log10') +\n    geom_line() + \n    ggtitle(\"\")\np3 &lt;- forecast::autoplot(res, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Airline passengers residuals (lg[thousand])\") + \n    ggtitle(\"Detrended and log-transformed series\")\np4 &lt;- data.frame(Spectrum = spec_log_detrend$spec, \n                 Frequency = spec_log_detrend$freq) %&gt;% \n    ggplot(aes(x = Frequency, y = Spectrum)) +\n    scale_y_continuous(trans = 'log10') +\n    geom_line() + \n    ggtitle(\"\")\n(pAirPassengers + p2) / (p3 + p4) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 24.1: Monthly AirPassengers time series, log-transformed and detrended, and the corresponding fast Fourier transforms.\n\n\n\n\nNote that the function spec.pgram() has the option of removing a simpler (linear) trend and showing the results as a base-R plot. The confidence band in the upper right corner of this plot helps to identify the statistical significance of the peaks (Figure 24.2).\n\nCodepar(mfrow = c(2, 2))\nspec.pgram(res, spans = c(3))\nspec.pgram(res, spans = c(3, 3))\nspec.pgram(res, spans = c(7, 7))\nspec.pgram(res, spans = c(11, 11))\n\n\n\n\n\n\nFigure 24.2: Smoothed periodograms of monthly log-transformed and detrended AirPassengers time series.\n\n\n\n\nAnother method of testing the reality of a peak is to look at its harmonics. It is extremely unlikely that a true cycle will be shaped perfectly as a sine curve, hence at least a few of the first harmonics will show up as well. For example, in monthly data with annual seasonality (period of 12 months), we often observe peaks at 6, 4, and 3 months. This is exactly the pattern that we see in the figures above.\nWe can also approximate our data with an AR model and then plot the approximating periodogram of the AR model (Figure 24.3).\n\nCodespectrum(res, method = \"ar\")\n\n\n\n\n\n\nFigure 24.3: Periodogram of AR process approximating the monthly log-transformed and detrended AirPassengers time series.",
    "crumbs": [
      "Part VI — Methods for Research, Evaluation & Reproducibility",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "research-directions.html#periodogram-for-unequally-spaced-time-series",
    "href": "research-directions.html#periodogram-for-unequally-spaced-time-series",
    "title": "\n24  Adversarial LLMs1\n",
    "section": "\n24.5 Periodogram for unequally-spaced time series",
    "text": "24.5 Periodogram for unequally-spaced time series\nUnequally/unevenly-spaced time series or gaps in observations (missing data) can be handled with the Lomb–Scargle periodogram calculation. This method was originally developed for the analysis of unevenly-spaced astronomical signals (Lomb 1976; Scargle 1982) but found application in many other domains, including environmental science (e.g., Ruf 1999; Campos et al. 2008; Siskey et al. 2016).\n\n\n\n\n\n\nExample: Ammonium concentration in wastewater system\n\n\n\nFor example, consider a time series imputeTS::tsNH4 of ammonium (NH\\(_4\\)) concentration in a wastewater system (Figure 24.4). This time series contains observations from regular 10-minute intervals, but some of the observations are missing.\n\nCodeforecast::autoplot(imputeTS::tsNH4) +\n    xlab(\"Day\") +\n    ylab(bquote(NH[4]~concentration))\n\n\n\n\n\n\nFigure 24.4: Time series of ammonium concentration in a wastewater system.\n\n\n\n\nSave the data in a format acceptable by the Lomb–Scargle function.\n\nD &lt;- data.frame(NH4 = as.vector(imputeTS::tsNH4),\n                Time = as.vector(time(imputeTS::tsNH4)))\n\nIf needed, na.omit(D) can be used to remove the rows with missing values.\nFigure 24.5 and Figure 24.6 show periodograms calculated based on these data. Note that the function lomb::lsp() has arguments adjusting the span of the frequencies and significance level.\n\nCodepar(mfrow = c(2, 2))\nlomb::lsp(D$NH4, times = D$Time, type = \"frequency\", alpha = 0.05, main = \"\")\n\n\n\n\n\n\nFigure 24.5: Lomb–Scargle periodograms computed for the unequally-spaced time series of ammonium concentration.\n\n\n\n\n\nCodelomb::lsp(D$NH4, times = D$Time, type = \"period\", alpha = 0.05, main = \"\")\n\n\n\n\n\n\nFigure 24.6: Lomb–Scargle periodograms computed for the unequally-spaced time series of ammonium concentration.",
    "crumbs": [
      "Part VI — Methods for Research, Evaluation & Reproducibility",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "research-directions.html#frequency-estimation-in-time",
    "href": "research-directions.html#frequency-estimation-in-time",
    "title": "\n24  Adversarial LLMs1\n",
    "section": "\n24.6 Frequency estimation in time",
    "text": "24.6 Frequency estimation in time\nThe assumption of a whole time series having a constant spectrum might be very limiting for the analysis of long time series. An estimation of frequencies locally in time allows us to study how the spectrum changes in time, and also detect smaller (short-lived) signals that would be averaged out otherwise. A straightforward solution is to separate the time series into windows (sub-periods) and estimate a spectrum in each such window.\nThe windowed analysis makes sense when in each window we have enough observations to estimate the frequencies. As a guide, we should have at least two observations per period to be able to represent the signal in the discrete Fourier transform. For example, we cannot estimate the seasonality if we sample once per year – this is our sampling rate or sampling frequency \\(F_s\\). To start identifying seasonal periodicity, our sampling rate should be at least 2 samples per year. The highest frequency we can work with is limited by the Nyquist frequency \\[\nF_N = \\frac{F_s}{2},\n\\] which is half of the sampling frequency.\nAfter applying the FFT in each window, the results can be visualized in a spectrogram. The spectrogram combines information from multiple periodograms: it shows time on the \\(x\\)-axis, frequency on the \\(y\\)-axis, and the corresponding spectrum power as the color.\n\n\n\n\n\n\nExample: Spectrograms for the NAO\n\n\n\nConsider a long time series of the North Atlantic Oscillation (NAO) index obtained from an ensemble of 100 model-constrained NAO reconstructions (Figure 24.7).\n\nCodeNAOdf &lt;- readr::read_csv(\"data/NAO.csv\", skip = 12, show_col_types = FALSE) %&gt;% \n    rename(NAOproxy = nao_mc_mean) %&gt;% \n    select(Year, NAOproxy) %&gt;% \n    arrange(Year)\nNAO &lt;- ts(NAOdf$NAOproxy, start = NAOdf$Year[1])\nspec_raw &lt;- spec.pgram(NAO, detrend = FALSE, plot = FALSE, spans = 3)\n\n# Find the frequency with the max power\nfmax &lt;- spec_raw$freq[which.max(spec_raw$spec)]\n\n\n\nCodep1 &lt;- forecast::autoplot(NAO) +\n    xlab(\"Year\") +\n    ylab(\"NAO\")\np2 &lt;- data.frame(Spectrum = spec_raw$spec, \n                 Frequency = spec_raw$freq) %&gt;% \n    ggplot(aes(x = Frequency, y = Spectrum)) +\n    scale_y_continuous(trans = 'log10') +\n    geom_vline(xintercept = fmax, col = \"blue\", lty = 2) + \n    geom_line() + \n    ggtitle(\"\")\np3 &lt;- data.frame(Spectrum = spec_raw$spec, \n                 Period = 1/spec_raw$freq) %&gt;% \n    ggplot(aes(x = Period, y = Spectrum)) +\n    scale_y_continuous(trans = 'log10') +\n    geom_vline(xintercept = 1/fmax, col = \"blue\", lty = 2) +\n    geom_line() + \n    ggtitle(\"\")\np1 / (p2 + p3) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 24.7: North Atlantic Oscillation (NAO) winter index (December, January, and February) calculated as the mean of 100 model-constrained NAO reconstructions, along with its smoothed periodograms. The dashed lines denote the maximal magnitude of the spectrum.\n\n\n\n\nThis is an annual time series, hence the sampling frequency \\(F_s = 1\\). From the global FFT results, the frequency with the maximal power is 0.019 year\\(^{-1}\\), which is equivalent to the period of about 53.3 years. We will set the window for the FFT and calculate the spectrogram. Note that we can use overlapping windows for a smoother picture.\n\n# Set the window size (years), for applying the FFT in each window\nwindow_size = 30\n\n# Compute the spectrogram\nSP &lt;- signal::specgram(x = NAO,\n                       n = window_size,\n                       overlap = window_size/3,\n                       Fs = 1)\n\nSee the spectrograms in Figure 24.8 and compare them with the time series plot in Figure 24.7 A.\n\nCodepar(mfrow = c(1, 2))\n\n# Plot the spectrogram without decorations\nprint(SP, main = \"A) Raw results\")\n\n# Discard phase information\nP &lt;- abs(SP$S)\n\n# Normalize the spectrum to the range [0, 1]\nP &lt;- P - min(P)\nP &lt;- P/max(P)\n\n# Extract time\nYears &lt;- time(NAO)[SP$t]\n\n# Plot the spectrogram after processing\noce::imagep(x = Years,\n            y = SP$f,\n            z = t(P),\n            col = oce::oce.colorsViridis,\n            ylab = expression(Frequency~(y^-1 )),\n            xlab = \"Year\",\n            main = \"B) After normalization and adding colors\",\n            drawPalette = TRUE,\n            decimate = FALSE)\n\n\n\n\n\n\nFigure 24.8: Spectrograms for North Atlantic Oscillation (NAO) winter index.",
    "crumbs": [
      "Part VI — Methods for Research, Evaluation & Reproducibility",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "research-directions.html#conclusion",
    "href": "research-directions.html#conclusion",
    "title": "\n24  Adversarial LLMs1\n",
    "section": "\n24.7 Conclusion",
    "text": "24.7 Conclusion\nFor challenging problems, smoothing, multitapering, linear filtering, (repeated) pre-whitening, and Lomb–Scargle can be used together. Beware that aperiodic but autoregressive processes produce peaks in spectral densities. Harmonic analysis is a complicated art rather than a straightforward procedure.\nIt is extremely difficult to derive the significance of a weak periodicity from harmonic analysis. Do not believe analytical estimates (e.g., exponential probability), as they rarely apply to real data. It is essential to make simulations, typically permuting or bootstrapping the data keeping the observing times fixed. Simulations of the final model with the observation times are also advised.",
    "crumbs": [
      "Part VI — Methods for Research, Evaluation & Reproducibility",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "research-directions.html#appendix",
    "href": "research-directions.html#appendix",
    "title": "\n24  Adversarial LLMs1\n",
    "section": "\n24.8 Appendix",
    "text": "24.8 Appendix\nWavelet analysis\nAn alternative to the windowed FFT is the wavelet analysis, which also estimates the strength of time series variations for different frequencies and times. Wavelet is a ‘small wave’ or function \\(\\psi(t)\\) approximating the variations. Popular wavelet functions are Meyer, Morlet, and Mexican hat.\nFigure 24.9 shows the results of using the Morlet wavelet to process the NAO time series.\n\nCodelibrary(dplR)\nwv &lt;- morlet(y1 = NAOdf$NAOproxy, x1 = NAOdf$Year)\nlevs &lt;- quantile(wv$Power, probs = c(0, 0.5, 0.75, 0.9, 0.95, 1))\nwavelet.plot(wv, wavelet.levels = levs)\n\n\n\n\n\n\nFigure 24.9: Wavelet analysis of the North Atlantic Oscillation (NAO) winter index.\n\n\n\n\n\n\n\n\nCampos MC, Costa JL, Quintella BR, et al (2008) Activity and movement patterns of the Lusitanian toadfish inferred from pressure-sensitive data-loggers in the Mira estuary (Portugal). Fisheries Management and Ecology 15:449–458. https://doi.org/10.1111/j.1365-2400.2008.00629.x\n\n\nLomb NR (1976) Least-squares frequency analysis of unequally spaced data. Astrophysics and Space Science 39:447–462. https://doi.org/10.1007/BF00648343\n\n\nNason GP (2008) Wavelet methods in statistics with R. Springer, New York, NY, USA\n\n\nRuf T (1999) The Lomb–Scargle periodogram in biological rhythm research: Analysis of incomplete and unequally spaced time-series. Biological Rhythm Research 30:178–201. https://doi.org/10.1076/brhm.30.2.178.1422\n\n\nScargle JD (1982) Studies in astronomical time series analysis. II – statistical aspects of spectral analysis of unevenly spaced data. Astrophysical Journal 263:835–853. https://doi.org/10.1086/160554\n\n\nShumway RH, Stoffer DS (2017) Time series analysis and its applications with R examples, 4th edn. Springer, New York, NY, USA\n\n\nSiskey MR, Lyubchich V, Liang D, et al (2016) Periodicity of strontium:calcium across annuli further validates otolith-ageing for Atlantic bluefin tuna (Thunnus thynnus). Fisheries Research 177:13–17. https://doi.org/10.1016/j.fishres.2016.01.004",
    "crumbs": [
      "Part VI — Methods for Research, Evaluation & Reproducibility",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Adversarial LLMs1</span>"
    ]
  },
  {
    "objectID": "responsible-deployment.html",
    "href": "responsible-deployment.html",
    "title": "Preface",
    "section": "",
    "text": "Author\nThis book is a comprehensive guide on Safe AI, bringing together the foundations of machine learning security, adversarial robustness, AI alignment, and trustworthy deployment practices for modern cyber-physical systems (CPS) and LLM-based autonomous agents.\nIt is written as a hybrid between a research handbook, a course textbook, and a practical engineering guide for building and securing intelligent systems.\nThe material integrates insights from academic literature, real-world deployment case studies, and hands-on adversarial evaluations—spanning from classical adversarial machine learning to emerging red/blue-team techniques for large-scale language models.\nSpecial emphasis is placed on CPS, where failures in AI behavior directly influence physical infrastructure such as power grids, transportation systems, and industrial control environments.\nEach chapter begins with explicit learning objectives, conceptual explanations, and diagrams, followed by practical examples, research notes, and additional curated references.\nThe aim is to equip the reader with the full intellectual toolkit required to design, analyze, attack, defend, and align AI systems operating in safety-critical settings.\nThe intended audience should be comfortable with programming and have working familiarity with the following concepts and methods:\nThis book may serve students, researchers, engineers, and practitioners seeking a deep and structured understanding of modern AI security—from the mathematical fundamentals to frontier challenges in alignment, LLM safety, and real-world CPS integration.\nKundan Kumarhttps://kundan-kumarr.github.io/",
    "crumbs": [
      "Part VII — Alignment & Safe Autonomous Agents",
      "Preface"
    ]
  },
  {
    "objectID": "responsible-deployment.html#citation",
    "href": "responsible-deployment.html#citation",
    "title": "Preface",
    "section": "Citation",
    "text": "Citation\nKumar, K. (2025). Safe AI for Cyber-Physical & Intelligent Systems:\nModel Security, Adversarial Robustness, Agent Safety, and Trustworthy Methods.\nEdition 2025-11.",
    "crumbs": [
      "Part VII — Alignment & Safe Autonomous Agents",
      "Preface"
    ]
  },
  {
    "objectID": "responsible-deployment.html#license",
    "href": "responsible-deployment.html#license",
    "title": "Preface",
    "section": "License",
    "text": "License\nThis work is licensed under the MIT License.",
    "crumbs": [
      "Part VII — Alignment & Safe Autonomous Agents",
      "Preface"
    ]
  },
  {
    "objectID": "open-problems.html",
    "href": "open-problems.html",
    "title": "32  Fundamental of AI and Cybersecurity",
    "section": "",
    "text": "Coming Soon!!",
    "crumbs": [
      "Part VII — Alignment & Safe Autonomous Agents",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Fundamental of AI and Cybersecurity</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Benjamin MA, Rigby RA, Stasinopoulos DM (2003) Generalized\nautoregressive moving average models. Journal of the American\nStatistical Association 98:214–223. https://doi.org/10.1198/016214503388619238\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and\nforecasting, 2nd edn. Springer, New York, NY, USA\n\n\nCampos MC, Costa JL, Quintella BR, et al (2008) Activity and movement\npatterns of the Lusitanian toadfish inferred from\npressure-sensitive data-loggers in the Mira estuary\n(Portugal). Fisheries Management and Ecology 15:449–458. https://doi.org/10.1111/j.1365-2400.2008.00629.x\n\n\nChatfield C (2000) Time-series forecasting. CRC Press, Boca Raton, FL,\nUSA\n\n\nChatterjee S, Hadi AS (2006) Regression analysis by example, 4th edn.\nJohn Wiley & Sons, Hoboken, NJ, USA\n\n\nCochrane D, Orcutt GH (1949) Application of least squares regression to\nrelationships containing auto-correlated error terms. Journal of the\nAmerican Statistical Association 44:32–61. https://doi.org/10.2307/2280349\n\n\nDean RT, Dunsmuir WTM (2016) Dangers and uses of cross-correlation in\nanalyzing time series in perception, performance, movement, and\nneuroscience: The importance of constructing transfer function\nautoregressive models. Behavior Research Methods 48:783–802. https://doi.org/10.3758/s13428-015-0611-2\n\n\nDegras D, Xu Z, Zhang T, Wu WB (2012) Testing for parallelism among\ntrends in multiple time series. IEEE Transactions on Signal\nProcessing 60:1087–1097. https://doi.org/10.1109/TSP.2011.2177831\n\n\nDuguay CR, Brown L, Kang K-K, Kheyrollah Pour H (2013) State of the\nclimate in 2012: Lake ice. Bulletin of the American Meteorological\nSociety 94:S124–S126. https://doi.org/10.1175/2013BAMSStateoftheClimate.1\n\n\nDunn PK, Smyth GK (1996) Randomized quantile residuals. Journal of\nComputational and Graphical Statistics 5:236–244. https://doi.org/10.2307/1390802\n\n\nEngle RF, Granger CWJ (1987) Co-integration and error correction:\nRepresentation, estimation, and testing. Econometrica 55:251–276. https://doi.org/10.2307/1913236\n\n\nEun CS, Lee J (2010) Mean-variance convergence around the world. Journal\nof Banking & Finance 34:856–870. https://doi.org/10.1016/j.jbankfin.2009.09.016\n\n\nGranger CWJ (1969) Investigating causal relations by econometric models\nand cross-spectral methods. Econometrica 37:424–438. https://doi.org/10.2307/1912791\n\n\nGupta PL, Gupta RC, Tripathi RC (1996) Analysis of zero-adjusted count\ndata. Computational Statistics & Data Analysis 23:207–218. https://doi.org/10.1016/S0167-9473(96)00032-1\n\n\nHastie TJ, Tibshirani RJ, Friedman JH (2009) The elements of\nstatistical learning: Data mining, inference, and prediction, 2nd\nedn. Springer, New York, NY, USA\n\n\nKirchgässner G, Wolters J (2007) Introduction to modern\ntime series analysis. Springer-Verlag, Berlin, Germany\n\n\nLatifovic R, Pouliot D (2007) Analysis of climate change impacts on lake\nice phenology in Canada using the historical satellite data\nrecord. Remote Sensing of Environment 106:492–507. https://doi.org/10.1016/j.rse.2006.09.015\n\n\nLi WK (1994) Time series models based on generalized linear models: Some\nfurther results. Biometrics 50:506–511. https://doi.org/10.2307/2533393\n\n\nLomb NR (1976) Least-squares frequency analysis of unequally spaced\ndata. Astrophysics and Space Science 39:447–462. https://doi.org/10.1007/BF00648343\n\n\nLyubchich V (2016) Detecting time series trends and their\nsynchronization in climate data. Intelligence Innovations Investments\n12:132–137. https://www.researchgate.net/publication/318283780_Detecting_time_series_trends_and_their_synchronization_in_climate_data\n\n\nLyubchich V, Gel YR (2016) A local factor nonparametric test for trend\nsynchronism in multiple time series. Journal of Multivariate Analysis\n150:91–104. https://doi.org/10.1016/j.jmva.2016.05.004\n\n\nLyubchich V, Nesslage G (2020) Environmental drivers of golden tilefish fisheries\nv1.0. Version v1.0. Zenodo\n\n\nNason GP (2008) Wavelet methods in\nstatistics with R. Springer, New York, NY, USA\n\n\nNesslage G, Lyubchich V, Nitschke P, et al (2021) Environmental drivers\nof golden tilefish (Lopholatilus\nchamaeleonticeps) commercial landings and catch-per-unit-effort.\nFisheries Oceanography 30:608–622. https://doi.org/10.1111/fog.12540\n\n\nPark C, Hannig J, Kang K-H (2014) Nonparametric comparison of multiple\nregression curves in scale-space. Journal of Computational and Graphical\nStatistics 23:657–677. https://doi.org/10.1080/10618600.2013.822816\n\n\nPark C, Vaughan A, Hannig J, Kang K-H (2009) SiZer analysis\nfor the comparison of time series. Journal of Statistical Planning and\nInference 139:3974–3988. https://doi.org/10.1016/j.jspi.2009.05.003\n\n\nPearl J (2009) Causality: Models, reasoning, and inference, 2nd edn.\nCambridge University Press, Cambridge, UK\n\n\nPerperoglou A, Sauerbrei W, Abrahamowicz M, Schmid M (2019) A review of\nspline function procedures in R. BMC Medical Research\nMethodology 19: https://doi.org/10.1186/s12874-019-0666-3\n\n\nRebane G, Pearl J (1987) The recovery of causal poly-trees from\nstatistical data. In: Proceedings of the third annual conference on\nuncertainty in artificial intelligence. pp 222–228\n\n\nRice J (1984) Bandwidth choice for nonparametric regression. The Annals\nof Statistics 12:1215–1230. https://doi.org/10.1214/aos/1176346788\n\n\nRuf T (1999) The Lomb–Scargle periodogram in biological\nrhythm research: Analysis of incomplete and unequally spaced\ntime-series. Biological Rhythm Research 30:178–201. https://doi.org/10.1076/brhm.30.2.178.1422\n\n\nScargle JD (1982) Studies in astronomical time series analysis.\nII – statistical aspects of spectral analysis of unevenly\nspaced data. Astrophysical Journal 263:835–853. https://doi.org/10.1086/160554\n\n\nShumway RH, Stoffer DS (2017) Time series analysis\nand its applications with R examples, 4th edn.\nSpringer, New York, NY, USA\n\n\nSiskey MR, Lyubchich V, Liang D, et al (2016) Periodicity of strontium:calcium across annuli further validates\notolith-ageing for Atlantic bluefin tuna\n(Thunnus thynnus). Fisheries Research 177:13–17. https://doi.org/10.1016/j.fishres.2016.01.004\n\n\nSoliman M, Lyubchich V, Gel YR (2019) Complementing the power of deep\nlearning with statistical model fusion: Probabilistic forecasting of\ninfluenza in Dallas County, Texas, USA. Epidemics\n28:100345. https://doi.org/10.1016/j.epidem.2019.05.004\n\n\nStasinopoulos DM, Rigby RA (2007) Generalized additive models for\nlocation scale and shape (GAMLSS) in R.\nJournal of Statistical Software 23:1–46. https://doi.org/10.18637/jss.v023.i07\n\n\nVilar-Fernández JM, González-Manteiga W (2004) Nonparametric comparison\nof curves with dependent errors. Statistics 38:81–99. https://doi.org/10.1080/02331880310001634656\n\n\nVogelsang TJ, Franses PH (2005) Testing for common deterministic trend\nslopes. Journal of Econometrics 126:1–24. https://doi.org/10.1016/j.jeconom.2004.02.004\n\n\nZeger SL, Qaqish B (1988) Markov regression models for time series: A\nquasi-likelihood approach. Biometrics 44:1019–1031. https://doi.org/10.2307/2531732\n\n\nZhang T (2013) Clustering high-dimensional time series based on\nparallelism. Journal of the American Statistical Association\n108:577–588. https://doi.org/10.1080/01621459.2012.760458\n\n\nZuur A, Ieno EN, Walker NJ, et al (2009) Mixed effects models\nand extensions in ecology with R. Springer, New York",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix-prompt-engineering.html",
    "href": "appendix-prompt-engineering.html",
    "title": "Appendix A — Weighted least squares",
    "section": "",
    "text": "We can often hypothesize that the standard deviation of residuals in the model \\[\ny_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\n\\tag{A.1}\\] is proportional to the predictor \\(X\\), so \\[\n\\mathrm{var}(\\varepsilon_i)=k^2x^2_i, \\;\\; k&gt;0.\n\\]\nIn the weighted least squares (WLS) method, we can stabilize the variance by dividing both sides of Equation A.1 by \\(x_i\\): \\[\n\\frac{y_i}{x_i}=\\frac{\\beta_0}{x_i}+\\beta_1+\\frac{\\varepsilon_i}{x_i},\n\\tag{A.2}\\] then \\(\\mathrm{var}\\left(\\frac{\\varepsilon_i}{x_i}\\right)=k^2\\), i.e., it is now stabilized.\n\n\n\n\n\n\nExample: WLS applied ‘manually’\n\n\n\nConsider a simulated example of a linear model \\(y=3-2x\\) with noise, which is a function of \\(x\\).\n\nCodeset.seed(111)\nk = 0.5\nn = 100\nx &lt;- rnorm(n, 0, 5)\ny &lt;- 3 - 2 * x + rnorm(n, 0, k*x^2)\n\n\nThe coefficients estimated using ordinary least squares (OLS):\n\nfit_ols &lt;- lm(y ~ x)\nsummary(fit_ols)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -47.41  -8.01  -2.32   1.58 286.47 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)    6.131      3.402    1.80    0.075 .  \n#&gt; x             -3.571      0.639   -5.59    2e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 34 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.242,  Adjusted R-squared:  0.234 \n#&gt; F-statistic: 31.3 on 1 and 98 DF,  p-value: 2.04e-07\n\n\nBased on Figure A.1, the OLS assumption of homoskedasticity is violated, because the observations deviate farther from the regression line at its ends (i.e., the variability of regression residuals is higher at the low and high values of the predictor).\n\nCodep1 &lt;- ggplot(data.frame(x, y), aes(x = x, y = y)) + \n    geom_abline(intercept = 3, slope = -2, col = \"gray50\", lwd = 1.5) +\n    geom_abline(intercept = fit_ols$coefficients[1], \n                slope = fit_ols$coefficients[2], lty = 2) +\n    geom_point()\np2 &lt;- ggplot(data.frame(x, y), aes(x = x, y = rstandard(fit_ols))) +\n    geom_hline(yintercept = 0, col = \"gray50\") +\n    geom_point() +\n    xlab(\"x\") +\n    ylab(\"Standardized residuals\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure A.1: Simulated data example with heteroskedasticity. The gray line represents the underlying model; the dashed line is obtained from the OLS fit.\n\n\n\n\nTo stabilize the variance ‘manually,’ transform the variables according to Equation A.2 and refit the model:\n\nY.t &lt;- y/x\nX.t &lt;- 1/x\nfit_wls &lt;- lm(Y.t ~ X.t)\nsummary(fit_wls)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Y.t ~ X.t)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -20.021  -0.635   0.251   1.322   5.676 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   -2.154      0.293   -7.36  5.7e-11 ***\n#&gt; X.t            3.008      0.168   17.95  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.9 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.767,  Adjusted R-squared:  0.764 \n#&gt; F-statistic:  322 on 1 and 98 DF,  p-value: &lt;2e-16\n\n\nCheck Equation A.2 to see the correspondence of the coefficients, see the results in Figure A.2.\n\nCodep1 &lt;- ggplot(data.frame(x, y), aes(x = x, y = y)) + \n    geom_abline(intercept = 3, slope = -2, col = \"gray50\", lwd = 1.5) +\n    geom_abline(intercept = fit_wls$coefficients[2], \n                slope = fit_wls$coefficients[1], lty = 2) +\n    geom_point()\np2 &lt;- ggplot(data.frame(x, y), aes(x = x, y = rstandard(fit_wls))) +\n    geom_hline(yintercept = 0, col = \"gray50\") +\n    geom_point() +\n    xlab(\"x\") +\n    ylab(\"Standardized residuals\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure A.2: Simulated data example with heteroskedasticity. The gray line represents the underlying model; the dashed line is obtained from the WLS fit.\n\n\n\n\n\n\nInstead of minimizing the residual sum of squares (using the original or transformed data in Equation A.1 and Equation A.2), \\[\nRSS(\\beta) = \\sum_{i=1}^n(y_i - x_i\\beta)^2,\n\\] we minimize the weighted sum of squares, where \\(w_i\\) are the weights: \\[\nWSS(\\beta; w) = \\sum_{i=1}^nw_i(y_i - x_i\\beta)^2.\n\\] This includes OLS as the special case when all the weights \\(w_i = 1\\) (\\(i=1,\\dots,n\\)). In the example above, \\(w_i=1/x^2_i\\).\nIn matrix form, \\[\n\\hat{\\boldsymbol{\\beta}}=(\\boldsymbol{X}^{\\top}\\boldsymbol{W}\\boldsymbol{X})^{-1}\\boldsymbol{X}^{\\top}\\boldsymbol{W}\\boldsymbol{Y}.\n\\tag{A.3}\\]\nTo apply Equation A.3 in R, specify the argument weights, and remember to take an inverse. Note that the coefficients are now labeled as expected.\n\nfit_wls2 &lt;- lm(y ~ x, weights = 1/x^2)\nsummary(fit_wls2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x, weights = 1/x^2)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -7.122 -0.842  0.018  1.238 20.021 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)    3.008      0.168   17.95  &lt; 2e-16 ***\n#&gt; x             -2.154      0.293   -7.36  5.7e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.9 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.356,  Adjusted R-squared:  0.349 \n#&gt; F-statistic: 54.2 on 1 and 98 DF,  p-value: 5.72e-11\n\n\nChatterjee and Hadi (2006) in Chapter 7 consider two more cases for applying WLS, both related to grouping. We skip those cases for now and revisit our data example from Chapter 32.\n\n\n\n\n\n\nExample: Dishwasher shipments WLS model\n\n\n\nFirst, use OLS to estimate the simple linear regression exploring dishwasher shipments (DISH) and private residential investments (RES) for several years.\n\nD &lt;- read.delim(\"data/dish.txt\") %&gt;% \n    rename(Year = YEAR)\nmodDish_ols &lt;- lm(DISH ~ RES, data = D)\n\nThe plot in Figure A.3 indicates that the variance might be decreasing with higher investments.\n\nCodeggplot(D, aes(x = RES, y = rstandard(modDish_ols))) +\n    geom_hline(yintercept = 0, col = \"gray50\") +\n    geom_point() +\n    xlab(\"Residential investments\") +\n    ylab(\"Standardized residuals\")\n\n\n\n\n\n\nFigure A.3: OLS residuals vs. the predictor.\n\n\n\n\nApply the WLS:\n\nmodDish_wls &lt;- lm(DISH ~ RES, data = D, weights = RES^2)\n\nIn Figure A.4 we see minor changes in the slope (better fit?).\n\nCodep1 &lt;- ggplot(D, aes(x = RES, y = DISH)) + \n    geom_abline(intercept = modDish_wls$coefficients[1], \n                slope = modDish_wls$coefficients[2], lty = 2) +\n    geom_abline(intercept = modDish_ols$coefficients[1], \n                slope = modDish_ols$coefficients[2], \n                col = \"gray50\") +\n    geom_point() +\n    xlab(\"Residential investments\") + \n    ylab(\"Dishwasher shipments\")\np2 &lt;- ggplot(D, aes(x = RES, y = rstandard(modDish_wls))) +\n    geom_hline(yintercept = 0, col = \"gray50\") +\n    geom_point() +\n    xlab(\"Residential investments\") +\n    ylab(\"Standardized residuals\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure A.4: The regression fits (OLS – solid line; WLS – dashed line) and the WLS residuals vs. the predictor.\n\n\n\n\nHowever, the residuals are still autocorrelated, which violates another assumption of the OLS and WLS methods:\n\nlawstat::runs.test(rstandard(modDish_wls))\n\n#&gt; \n#&gt;  Runs Test - Two sided\n#&gt; \n#&gt; data:  rstandard(modDish_wls)\n#&gt; Standardized Runs Statistic = -2, p-value = 0.05\n\n\n\n\nSee Appendix B on the method of generalized least squares (GLS) that allows accounting for autocorrelation in regression modeling.\n\n\n\n\nChatterjee S, Hadi AS (2006) Regression analysis by example, 4th edn. John Wiley & Sons, Hoboken, NJ, USA",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Weighted least squares</span>"
    ]
  },
  {
    "objectID": "appendix-eval-checklists.html",
    "href": "appendix-eval-checklists.html",
    "title": "Appendix B — Generalized least squares",
    "section": "",
    "text": "Here we use time series data (ordered by \\(t\\)), thus, Equation A.1 will be written with the time indices \\(t\\) as \\[\ny_t=\\beta_0+\\beta_1x_t+\\varepsilon_t,\n\\tag{B.1}\\] where the regression errors at times \\(t\\) and \\(t-1\\) are \\[\n\\begin{split}\n\\varepsilon_t&=y_t-\\beta_0-\\beta_1x_t,\\\\\n\\varepsilon_{t-1}&=y_{t-1}-\\beta_0-\\beta_1x_{t-1}.\n\\end{split}\n\\tag{B.2}\\]\nAn AR(1) model for the errors will yield \\[\n\\begin{split}\ny_t-\\beta_0-\\beta_1x_t & = \\rho\\varepsilon_{t-1} + w_t, \\\\\ny_t-\\beta_0-\\beta_1x_t & = \\rho(y_{t-1}-\\beta_0-\\beta_1x_{t-1})+w_t,\n\\end{split}\n\\tag{B.3}\\] where \\(w_t\\) are uncorrelated errors.\nRewrite it as \\[\n\\begin{split}\ny_t-\\rho y_{t-1}&=\\beta_0(1-\\rho)+\\beta_1(x_t-\\rho x_{t-1})+w_t,\\\\\ny_t^* &= \\beta_0^* + \\beta_1 x_t^*+w_t,\n\\end{split}\n\\tag{B.4}\\] where \\(y_t^* = y_t-\\rho y_{t-1}\\); \\(\\beta_0^* = \\beta_0(1-\\rho)\\); \\(x_t^* = x_t-\\rho x_{t-1}\\). Notice the errors \\(w_t\\) in the final Equation B.4 for the transformed variables \\(y_t^*\\) and \\(x_t^*\\) are uncorrelated.\nTo get from Equation B.1 to Equation B.4, we can use an iterative procedure by Cochrane and Orcutt (1949) as in the example below.\n\n\n\n\n\n\nExample: Dishwasher shipments model accounting for autocorrelation\n\n\n\n\nEstimate the model in Equation B.1 using OLS.\n\n\nCodeD &lt;- read.delim(\"data/dish.txt\") %&gt;% \n    rename(Year = YEAR)\nmodDish_ols &lt;- lm(DISH ~ RES, data = D)\n\n\n\nCalculate residuals \\(\\hat{\\varepsilon}_t\\) and estimate \\(\\rho\\) as \\[\n\\hat{\\rho}=\\frac{\\sum_{t=2}^n\\hat{\\varepsilon}_t\\hat{\\varepsilon}_{t-1}}{\\sum_{t=1}^n\\hat{\\varepsilon}^2_t}.\n\\]\n\n\n\nCodee &lt;- modDish_ols$residuals\nrho &lt;- sum(e[-1] * e[-length(e)]) / sum(e^2)\nrho\n\n#&gt; [1] 0.694\n\n\n\nCalculate transformed variables \\(x^*_t\\) and \\(y^*_t\\) and fit model in Equation B.4.\n\n\nCodey.star &lt;- D$DISH[-1] - rho * D$DISH[-length(D$DISH)]\nx.star &lt;- D$RES[-1] - rho * D$RES[-length(D$RES)]\nmodDish_ar1 &lt;- lm(y.star ~ x.star)\nsummary(modDish_ar1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y.star ~ x.star)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -479.7 -117.8   32.9  120.7  536.1 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)    26.76     130.69    0.20     0.84    \n#&gt; x.star         50.99       7.74    6.59    1e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 252 on 23 degrees of freedom\n#&gt; Multiple R-squared:  0.654,  Adjusted R-squared:  0.639 \n#&gt; F-statistic: 43.4 on 1 and 23 DF,  p-value: 1.01e-06\n\n\n\nExamine the residuals of the newly fitted equation (Figure B.1) and repeat the procedure, if needed.\n\n\nCodep1 &lt;- ggplot(D, aes(x = Year, y = modDish_ols$residuals)) + \n    geom_line() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    ggtitle(\"OLS model modDish_ols\") +\n    ylab(\"Residuals\")\np2 &lt;- ggplot(D[-1,], aes(x = Year, y = modDish_ar1$residuals)) + \n    geom_line() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    ggtitle(\"Transformed model modDish_ar1\") +\n    ylab(\"Residuals\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure B.1: Residual plots of the original OLS model and the model transformed to account for autocorrelation in residuals.\n\n\n\n\nBased on the runs test, there is not enough evidence of autocorrelation in the new residuals:\n\nlawstat::runs.test(rstandard(modDish_ar1))\n\n#&gt; \n#&gt;  Runs Test - Two sided\n#&gt; \n#&gt; data:  rstandard(modDish_ar1)\n#&gt; Standardized Runs Statistic = -0.6, p-value = 0.5\n\n\n\n\nWhat we have just applied is the method of generalized least squares (GLS): \\[\n\\hat{\\boldsymbol{\\beta}} = \\left( \\boldsymbol{X}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{Y},\n\\tag{B.5}\\] where \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix. The method of weighted least squares (WLS; Appendix A) is just a special case of the GLS. In the WLS approach, all the off-diagonal entries of \\(\\boldsymbol{\\Sigma}\\) are 0.\nWe can use the function nlme::gls() and specify the correlation structure to avoid iterating the steps from the previous example manually:\n\nmodDish_ar1_v2 &lt;- nlme::gls(DISH ~ RES\n                            ,correlation = nlme::corAR1(form = ~Year)\n                            ,data = D)\nsummary(modDish_ar1_v2)\n\n#&gt; Generalized least squares fit by REML\n#&gt;   Model: DISH ~ RES \n#&gt;   Data: D \n#&gt;   AIC BIC logLik\n#&gt;   342 347   -167\n#&gt; \n#&gt; Correlation Structure: AR(1)\n#&gt;  Formula: ~Year \n#&gt;  Parameter estimate(s):\n#&gt; Phi \n#&gt;   1 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Value Std.Error t-value p-value\n#&gt; (Intercept) -137.5   3714140    0.00       1\n#&gt; RES           45.7         6    7.35       0\n#&gt; \n#&gt;  Correlation: \n#&gt;     (Intr)\n#&gt; RES 0     \n#&gt; \n#&gt; Standardized residuals:\n#&gt;       Min        Q1       Med        Q3       Max \n#&gt; -0.000249 -0.000014  0.000135  0.000232  0.000338 \n#&gt; \n#&gt; Residual standard error: 3714140 \n#&gt; Degrees of freedom: 26 total; 24 residual\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the function nlme::gls() we can also specify weights to accommodate heteroskedastic errors, but the syntax differs from the weights specification in the function stats::lm() (Appendix A). See ?nlme::varFixed.\n\n\n\n\n\n\nCochrane D, Orcutt GH (1949) Application of least squares regression to relationships containing auto-correlated error terms. Journal of the American Statistical Association 44:32–61. https://doi.org/10.2307/2280349",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Generalized least squares</span>"
    ]
  },
  {
    "objectID": "appendix-security-patterns.html",
    "href": "appendix-security-patterns.html",
    "title": "Appendix C — Synchrony of parametric trends",
    "section": "",
    "text": "The problem of detecting joint trend dynamics in time series is essential in a variety of applications, ranging from the analysis of macroeconomic indicators (Vogelsang and Franses 2005; Eun and Lee 2010) to assessing patterns in ice phenology measurements from multiple locations (Latifovic and Pouliot 2007; Duguay et al. 2013) to evaluating yields of financial instruments at various maturity levels (Park et al. 2009) and cell phone download activity at different area codes (Degras et al. 2012).\nThe extensive research on comparing trend patterns follows two main directions:\n\nTesting for joint mean functions.\nAnalysis of joint stochastic trends, which is closely linked to the cointegration notion by Engle and Granger (1987) (?sec-cointegration).\n\nHere we explore the first direction, that is, assess whether several observed time series follow the same hypothesized parametric trend.\nThere exist many tests for comparing mean functions, but most of the developed methods assume independent errors. Substantially less is known about testing for joint deterministic trends in a time series framework.\nOne of the methods developed for time series is by Degras et al. (2012) and Zhang (2013) who extended the integrated square error (ISE) based approach of Vilar-Fernández and González-Manteiga (2004) to a case of multiple time series with weakly dependent (non)stationary errors. For a comprehensive literature review of available methodology for comparing mean functions embedded into independent errors in a time series framework, see Degras et al. (2012) and Park et al. (2014). Most of these methods, however, either focus on aligning only two curves or require us to select multiple hyperparameters, such as the bandwidth, level of smoothness, and window size for a long-run variance function. As mentioned by Park et al. (2014), the choice of such multiple nuisance parameters is challenging for a comparison of curves (even under the independent and identically distributed setup) and often leads to inadequate performance, especially in samples of moderate size.\nAs an alternative, consider an extension of the WAVK test (?sec-WAVK) to a case of multiple time series (Lyubchich and Gel 2016). Let us observe \\(N\\) time series \\[\nY_{it} = \\mu_i(t/T) + \\epsilon_{it},\n\\] where \\(i = 1, \\dots, N\\) (\\(N\\) is the number of time series), \\(t=1, \\dots, T\\) (\\(T\\) is the length of the time series), \\(\\mu_i(u)\\) (\\(0&lt;u\\leqslant 1\\)) are unknown smooth trend functions, and the noise \\(\\epsilon_{it}\\) can be represented as a finite-order AR(\\(p\\)) process or infinite-order AR(\\(\\infty\\)) process with i.i.d. innovations \\({e}_{it}\\).\nWe are interested in testing whether these \\(N\\) observed time series have the same trend of some pre-specified smooth parametric form \\(f(\\theta, u)\\):\n\\(H_0\\): \\(\\mu_i(u)= c_i + f(\\theta, u)\\)\\(H_1\\): there exists \\(i\\), such that \\(\\mu_i(u)\\neq c_i + f(\\theta, u)\\),\nwhere the reference curve \\(f(\\theta, u)\\) with a vector of parameters \\(\\theta\\) belongs to a known family of smooth parametric functions, and \\(1\\leqslant i \\leqslant N\\). For identifiability, assume that \\(\\sum_{i=1}^N c_i=0\\). Notice that the hypotheses include (but are not limited to) the special cases of\\(f(\\theta,u)\\equiv 0\\) (testing for no trend);\\(f(\\theta,u)=\\theta_0+\\theta_1 u\\) (testing for a common linear trend);\\(f(\\theta,u)=\\theta_0+\\theta_1 u+\\theta_2u^2\\) (testing for a common quadratic trend).\nThis hypothesis testing approach allows us to answer the following questions:\n\nDo trends in temperature (or wind speeds, or precipitation) reproduced by a climate model correspond to the historical observations? I.e., is the model generally correct?\nDo different instruments (sensors) capture changes similarly, or deviate, for example, due to aging of some of the instruments?\nDo trends estimated at different locations (Canada and USA, lower and mid-troposphere, etc.) follow some hypothesized global trend?\n\nTest the null hypothesis by following these steps (Lyubchich and Gel 2016):\n\nEstimate the joint hypothetical trend \\(f({\\theta}, u)\\) using the aggregated sample \\(\\left\\{\\overline{Y}_{\\cdot t}\\right\\}_{t=1}^T\\) (i.e., a time series obtained by averaging across all \\(N\\) time series).\nFor each time series, subtract the estimated trend, then apply the autoregressive filter to obtain residuals \\(\\hat{e}_{it}\\), which under the \\(H_0\\) behave asymptotically like the independent and identically distributed \\({e}_{it}\\): \\[\n\\begin{split}\n\\hat{e}_{it}&= \\hat{\\epsilon}_{it}-\\sum_{j=1}^{p_i(T)}{\\hat{\\phi}_{ij}\\hat{\\epsilon}_{i,t-j}} \\\\\n&=\n\\left\\{ Y_{it} - f(\\hat{\\theta},u_{t}) \\right\\} -\n\\left\\{ \\sum_{j=1}^{p_i(T)}{\\hat{\\phi}_{ij}{Y}_{i,t-j}} - \\sum_{j=1}^{p_i(T)}{\\hat{\\phi}_{ij}f(\\hat{\\theta},u_{t-j})} \\right\\}.\n\\end{split}\n\\]\n\nConstruct a sequence of \\(N\\) statistics \\({\\rm WAVK}_{1}(k_{1}), \\dots, {\\rm WAVK}_{N}(k_{N})\\). Then, the synchrony test statistic is \\[\nS_T = \\sum_{i=1}^N k_{i}^{-1/2}{\\rm WAVK}_{i}(k_{i}),\n\\] where \\(k_{i}\\) is the local window size for the WAVK statistic.\nEstimate the variance of \\(\\hat{e}_{it}\\), e.g., using the robust difference-based estimator by Rice (1984): \\[\ns_i^2= \\frac{\\sum_{t=2}^T(\\hat{e}_{it}-\\hat{e}_{i,t-1})^2}{2(T-1)}.\n\\]\n\nSimulate \\(BN\\) times \\(T\\)-dimensional vectors \\(e^*_{iT}\\) from the multivariate normal distribution \\(MVN\\left(0, s_i^2\\boldsymbol{I}\\right)\\), where \\(B\\) is the number of bootstrap replications, \\(\\boldsymbol{I}\\) is a \\(T\\times T\\) identity matrix.\nCompute \\(B\\) bootstrapped statistics on \\(e^*_{iT}\\): \\[S^*_T=\\sum_{i=1}^N k^{-1/2}_{i} {\\rm WAVK}^*_{i}(k_{i}).\\]\n\nThe bootstrap \\(p\\)-value for testing the \\(H_0\\) is the proportion of \\(|S^*_T|\\) that exceed \\(|S_T|\\).\n\nSee the application of both the WAVK and synchrony tests in Lyubchich (2016).\nIf the null hypothesis is rejected, the method does not tell, however, what was the reason, and which particular time series caused the rejection of the \\(H_0\\). One can remove the time series (or several time series at once) with the largest WAVK statistic(s) and apply the test again, although repeated testing increases the probability of Type I error. For an application of this method in trend clustering, see this vignette.\n\n\n\n\n\n\nExample: CMIP5 vs. observations\n\n\n\nReplicate the test for synchrony of trends found in two time series (Lyubchich 2016):\n\na multi-model average of temperatures from the 5th phase of the Coupled Model Intercomparison Project (CMIP5) and\nobserved global temperature anomalies relative to the base period of 1981–2010.\n\n\nCodeD &lt;- read.csv(\"data/CMIP5.csv\") %&gt;% \n    filter(1948 &lt;= Year & Year &lt;= 2013) %&gt;%\n    mutate(Temp_CMIP5 = Temp_CMIP5 - 273.15)\n\n\nSee Figure C.1 showing the time series plots after converting the CMIP data to degrees Celsius.\n\nCodep1 &lt;- D %&gt;% ggplot(aes(x = Year, y = Temp_CMIP5)) +\n    geom_line()\np2 &lt;- D %&gt;% ggplot(aes(x = Year, y = Temp_obs)) +\n    geom_line()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure C.1: Global annual mean temperature (°C) in 1948–2013: CMIP5 multi-model average and observed anomalies relative to the base period of 1981–2010.\n\n\n\n\nTest the synchrony of parametric linear trends in these time series:\n\nset.seed(123)\nfuntimes::sync_test(D[, c(\"Temp_CMIP5\", \"Temp_obs\")] ~ t)\n\n#&gt; \n#&gt;  Nonparametric test for synchronism of parametric trends\n#&gt; \n#&gt; data:  D[, c(\"Temp_CMIP5\", \"Temp_obs\")] \n#&gt; Test statistic = 0.04, p-value = 0.03\n#&gt; alternative hypothesis: common trend is not of the form D[, c(\"Temp_CMIP5\", \"Temp_obs\")] ~ t.\n#&gt; sample estimates:\n#&gt; $common_trend_estimates\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)    -1.57     0.0962   -16.4 1.44e-24\n#&gt; t               3.10     0.1647    18.8 8.39e-28\n#&gt; \n#&gt; $ar.order_used\n#&gt;          Temp_CMIP5 Temp_obs\n#&gt; ar.order          1        1\n#&gt; \n#&gt; $Window_used\n#&gt;        Temp_CMIP5 Temp_obs\n#&gt; Window          3        3\n#&gt; \n#&gt; $all_considered_windows\n#&gt;  Window Statistic p-value Asympt. p-value\n#&gt;       3    0.0406   0.034          0.0246\n#&gt;       4    0.0373   0.042          0.0386\n#&gt;       6    0.0413   0.020          0.0222\n#&gt; \n#&gt; $wavk_obs\n#&gt; [1] 0.0143 0.0263\n\n\nThe \\(p\\)-value below the usual significance level \\(\\alpha = 0.05\\) allows us to reject the null hypothesis, however, as Lyubchich (2016) pointed out, the decision would differ if more confidence is required (e.g., \\(\\alpha = 0.01\\)). Note that the \\(p\\)-value of 0.012 reported by Lyubchich (2016) differs from the one reported above due to the function settings and randomness due to the bootstrapping. We should save the random number generator state with set.seed() for replicability (so the test results are exactly the same every time the test is applied), and use a larger number of bootstrap replications B for consistency (so the test leads to the same conclusions when the function set.seed() is not used).\nNow test the synchrony of parametric quadratic trends in these time series:\n\nset.seed(123)\nfuntimes::sync_test(D[, c(\"Temp_CMIP5\", \"Temp_obs\")] ~ poly(t, 2))\n\n#&gt; \n#&gt;  Nonparametric test for synchronism of parametric trends\n#&gt; \n#&gt; data:  D[, c(\"Temp_CMIP5\", \"Temp_obs\")] \n#&gt; Test statistic = 0.05, p-value = 0.002\n#&gt; alternative hypothesis: common trend is not of the form D[, c(\"Temp_CMIP5\", \"Temp_obs\")] ~ poly(t, 2).\n#&gt; sample estimates:\n#&gt; $common_trend_estimates\n#&gt;              Estimate Std. Error   t value Pr(&gt;|t|)\n#&gt; (Intercept) -1.77e-15     0.0324 -5.45e-14 1.00e+00\n#&gt; poly(t, 2)1  7.27e+00     0.2632  2.76e+01 6.20e-37\n#&gt; poly(t, 2)2  2.28e+00     0.2632  8.65e+00 2.62e-12\n#&gt; \n#&gt; $ar.order_used\n#&gt;          Temp_CMIP5 Temp_obs\n#&gt; ar.order          2        0\n#&gt; \n#&gt; $Window_used\n#&gt;        Temp_CMIP5 Temp_obs\n#&gt; Window          3        3\n#&gt; \n#&gt; $all_considered_windows\n#&gt;  Window Statistic p-value Asympt. p-value\n#&gt;       3    0.0518   0.002        0.000282\n#&gt;       4    0.0350   0.028        0.014236\n#&gt;       6    0.0222   0.070        0.120094\n#&gt; \n#&gt; $wavk_obs\n#&gt; [1] -0.000673  0.052504\n\n\nNote these results differ substantially based on the window used for computing the WAVK statistic. The function funtimes::sync_test() automatically selects the optimal window based on the heuristic approach of comparing distances between bootstrap distributions (Lyubchich 2016; Lyubchich and Gel 2016).\n\n\n\n\n\n\nDegras D, Xu Z, Zhang T, Wu WB (2012) Testing for parallelism among trends in multiple time series. IEEE Transactions on Signal Processing 60:1087–1097. https://doi.org/10.1109/TSP.2011.2177831\n\n\nDuguay CR, Brown L, Kang K-K, Kheyrollah Pour H (2013) State of the climate in 2012: Lake ice. Bulletin of the American Meteorological Society 94:S124–S126. https://doi.org/10.1175/2013BAMSStateoftheClimate.1\n\n\nEngle RF, Granger CWJ (1987) Co-integration and error correction: Representation, estimation, and testing. Econometrica 55:251–276. https://doi.org/10.2307/1913236\n\n\nEun CS, Lee J (2010) Mean-variance convergence around the world. Journal of Banking & Finance 34:856–870. https://doi.org/10.1016/j.jbankfin.2009.09.016\n\n\nLatifovic R, Pouliot D (2007) Analysis of climate change impacts on lake ice phenology in Canada using the historical satellite data record. Remote Sensing of Environment 106:492–507. https://doi.org/10.1016/j.rse.2006.09.015\n\n\nLyubchich V (2016) Detecting time series trends and their synchronization in climate data. Intelligence Innovations Investments 12:132–137. https://www.researchgate.net/publication/318283780_Detecting_time_series_trends_and_their_synchronization_in_climate_data\n\n\nLyubchich V, Gel YR (2016) A local factor nonparametric test for trend synchronism in multiple time series. Journal of Multivariate Analysis 150:91–104. https://doi.org/10.1016/j.jmva.2016.05.004\n\n\nPark C, Hannig J, Kang K-H (2014) Nonparametric comparison of multiple regression curves in scale-space. Journal of Computational and Graphical Statistics 23:657–677. https://doi.org/10.1080/10618600.2013.822816\n\n\nPark C, Vaughan A, Hannig J, Kang K-H (2009) SiZer analysis for the comparison of time series. Journal of Statistical Planning and Inference 139:3974–3988. https://doi.org/10.1016/j.jspi.2009.05.003\n\n\nRice J (1984) Bandwidth choice for nonparametric regression. The Annals of Statistics 12:1215–1230. https://doi.org/10.1214/aos/1176346788\n\n\nVilar-Fernández JM, González-Manteiga W (2004) Nonparametric comparison of curves with dependent errors. Statistics 38:81–99. https://doi.org/10.1080/02331880310001634656\n\n\nVogelsang TJ, Franses PH (2005) Testing for common deterministic trend slopes. Journal of Econometrics 126:1–24. https://doi.org/10.1016/j.jeconom.2004.02.004\n\n\nZhang T (2013) Clustering high-dimensional time series based on parallelism. Journal of the American Statistical Association 108:577–588. https://doi.org/10.1080/01621459.2012.760458",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Synchrony of parametric trends</span>"
    ]
  },
  {
    "objectID": "appendix-datasets-benchmarks.html",
    "href": "appendix-datasets-benchmarks.html",
    "title": "Appendix D — Analysis of precipitation extremes and climate projections",
    "section": "",
    "text": "Text",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Analysis of precipitation extremes and climate projections</span>"
    ]
  },
  {
    "objectID": "appendix-tools-libraries.html",
    "href": "appendix-tools-libraries.html",
    "title": "Appendix E — Practice exercises",
    "section": "",
    "text": "E.1 Intro practice\nAnswer whether each of these statements is true or false.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Practice exercises</span>"
    ]
  },
  {
    "objectID": "appendix-tools-libraries.html#sec-intropractice",
    "href": "appendix-tools-libraries.html#sec-intropractice",
    "title": "Appendix E — Practice exercises",
    "section": "",
    "text": "The highest autocorrelation is observed when each next observation is exactly the same as the previous.\n‘Time series is autocorrelated’ means there is a trend.\nAutocorrelation goes away if we smooth the data, for example, with a moving average.\n‘Random variables \\(X\\) and \\(Y\\) are uncorrelated’ means \\(X\\) and \\(Y\\) are independent.\nTime series is an uninterrupted sequence of observations. Missing observations break the sequence into multiple separate time series.\nThe most appropriate statistical tool to detect a trend is the simple Student’s \\(t\\)-test.\nIf there is no autocorrelation at the first lag, i.e., \\(\\mathrm{cor}(X_t, X_{t-1}) = 0\\), then \\(\\mathrm{cor}(X_t, X_{t-2}) = 0\\).\nPrediction mean absolute error (PMAE) measures the quality of point forecasts, whereas prediction mean squared error (PMSE) measures the quality of interval forecasts.\nIf a time series \\(X_t\\) (\\(t = 1, \\dots, T\\)) is stationary, then all predictions for times \\(T+1\\), \\(T+2\\), \\(\\dots\\) are the same.\nWhite noise is a sequence of weakly correlated random variables.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Practice exercises</span>"
    ]
  },
  {
    "objectID": "appendix-tools-libraries.html#sec-acfpractice",
    "href": "appendix-tools-libraries.html#sec-acfpractice",
    "title": "Appendix E — Practice exercises",
    "section": "\nE.2 ARMA practice",
    "text": "E.2 ARMA practice\nBelow are several examples of time series with their ACF and PACF plots. For each example time series, use the plots to decide whether an ARMA(\\(p, q\\)) model is appropriate, and if so, suggest the orders \\(p\\) and \\(q\\). Use ?tbl-arma for help.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Practice exercises</span>"
    ]
  },
  {
    "objectID": "appendix-tools-libraries.html#sec-trendpractice",
    "href": "appendix-tools-libraries.html#sec-trendpractice",
    "title": "Appendix E — Practice exercises",
    "section": "\nE.3 Trend practice",
    "text": "E.3 Trend practice\nAnswer whether each of these statements is true or false.\n\nIf ACF values at the first ten lags are statistically significant, then the time series is not stationary.\nIf \\(X_t\\) is an ARIMA\\((p,d,q)\\) process, then \\((1-B)^d X_t\\) is an ARMA\\((p,q)\\) process.\nSlowly decaying ACF is a sign of nonstationarity.\nIf the hypothesis \\(H_1\\) of a linear trend was accepted for the series \\(U_t\\), \\(t = 1, \\dots, T\\), it will be also accepted for the subsets \\(U_{t'}\\), where \\(t' = j, \\dots, k\\); \\(j&lt;k\\), and \\(j,k &lt; T\\).\nUnit root tests can be applied to determine the appropriate order of differencing \\(d\\).\nA time series that exhibits a quadratic-looking trend can be made stationary (detrended) using the Box–Cox transformation with the power parameter \\(\\lambda = 2\\).\nARIMA(0,1,0) model is a random walk.\nFor the backshift operator \\(B\\), \\((1-B)^dX_t = (1-B^d)X_t\\).\nA linear time trend can be eliminated by differencing the time series once or twice.\nTrend functions (e.g., \\(X_t = 0.35 + 0.11t + \\epsilon_t\\), where \\(\\epsilon_t\\) are uncorrelated errors) express the changes in the process \\(X_t\\) caused by time.\nTime series should be differenced just enough times to remove a stochastic trend. Differencing too many times leads to problems.\nAutocorrelation in observations affects results of the \\(t\\)-test and Mann–Kendall test.\nThe Mann–Kendall test focuses on a more general class of trends than the \\(t\\)-test does.\nThe null hypothesis of the augmented Dickey–Fuller test is no unit root (stationarity).\nARIMA\\((p,d,q)\\) is a difference-stationary process.\nBootstrapping allows us to replicate the finite-sample distribution of the test statistic.\nTo detrend a time series, the difference operator should be applied with the same lag(s) at which the sample ACF has statistically significant values.\nOne of the correct ways to run regression on the time series \\(Y_t\\) and \\(X_t\\) with trends is to detrend these time series before fitting the regression model.\nIn practice, it is seldom necessary to go beyond second-order differences for detrending a time series.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Practice exercises</span>"
    ]
  }
]