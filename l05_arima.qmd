---
output: html_document
editor_options:
    chunk_output_type: console
---

```{r, echo=FALSE}
library(dplyr)
library(ggplot2)
library(patchwork)
options(digits = 3)
theme_set(theme_light())
```

# Deep Learning Models Overview

The goal of this lecture is to understand the core principles and architectures of deep learning models. We will learn how deep learning extends traditional machine learning through multi-layer neural networks, explore the mathematical foundation behind neural computations, and understand how training and optimization methods such as gradient descent enable models to learn complex patterns from data.


**Objectives**

1. Describe how Deep Learning (DL) fits within the broader field of Machine Learning (ML) and Artificial Intelligence (AI).
1. Explain the structure and mathematical operations of a neural network, including layers, weights, biases, and activations.
1. Distinguish between linear and non-linear models, and understand how non-linearity enables complex decision boundaries.
1. Apply concepts of activation functions such as sigmoid, tanh, and ReLU, and interpret their mathematical properties.
1. Understand the training process of deep neural networks using gradient descent and backpropagation.
1. Discuss common problems such as overfitting, vanishing gradients, and how regularization and normalization techniques mitigate them.
1. Identify the structure and purpose of key architectures including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformers.

**Reading materials**

* CS231n: Convolutional Neural Networks for Visual Recognition – Stanford University
* Goodfellow, Bengio & Courville (2016) – Deep Learning, MIT Press, Chapters 6–8
* Hung-Yi Lee – Deep Learning Tutorial
* Ismini Lourentzou – Introduction to Deep Learning
* Sebastian Ruder (2016) – An Overview of Gradient Descent Optimization Algorithms

## Machine Learning Basics

Machine learning enables computers to learn patterns from data without explicit programming.
The goal of an ML model is to find a mapping function:

Machine learning aims to learn a function $f_\theta$ that maps inputs $x$ to outputs $y$:

$$
f_\theta : X \rightarrow Y,
$$

where $\theta$ represents model parameters (weights and biases).  
The learning objective is to minimize the expected loss:

$$
\theta^* = \arg\min_\theta \, \mathbb{E}_{(x, y) \sim D}[L(y, f_\theta(x))].
$$

## Types of Machine Learning

### Supervised Learning
Learns from labeled pairs $(x_i, y_i)$ to predict $\hat{y_i}$.

- **Regression:** $y_i = f_\theta(x_i) + \varepsilon_i$
- **Classification:** $P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}}$

### Unsupervised Learning
Finds structure in unlabeled data $\{x_i\}$.

- **Clustering:** 
  $$
  \min_{\{C_k\}} \sum_{k=1}^K \sum_{x_i \in C_k} \|x_i - \mu_k\|^2
  $$
- **PCA:** 
  $$
  W^* = \arg\max_W \det(W^\top S W)
  $$

### Reinforcement Learning
Learns a policy $\pi(a|s)$ that maximizes reward:

$$
J(\pi) = \mathbb{E}\left[\sum_{t=0}^{T}\gamma^t r_t\right].
$$

## 3. Linear vs Nonlinear Models

A **linear classifier** defines a boundary:

$$
\mathbf{w}^\top \mathbf{x} + b = 0.
$$

A **nonlinear classifier** maps data to a transformed space $\phi(x)$:

$$
f(x) = \mathbf{w}^\top \phi(x) + b.
$$

Neural networks learn these nonlinear transformations automatically through layers of activations.
![](figures/linear_vs_nonlinear.png){fig-cap="Linear vs Nonlinear Classification.  
Linear classifiers separate data using a single hyperplane, while nonlinear classifiers create curved or complex boundaries through kernel transformations or hidden layers in neural networks." width=80%}

```{python}

import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
X1 = np.random.randn(40, 2) + np.array([-2, 0])
X2 = np.random.randn(40, 2) + np.array([2, 0])

# Plot points
plt.scatter(X1[:, 0], X1[:, 1], color="dodgerblue", label="Class A")
plt.scatter(X2[:, 0], X2[:, 1], color="orange", label="Class B")

# Linear boundary
x_vals = np.linspace(-4, 4, 100)
plt.plot(x_vals, -0.5 * x_vals, 'b--', lw=2, label="Linear Boundary")

# Nonlinear boundary (quadratic)
plt.plot(x_vals, 0.2 * x_vals**2 - 2, 'r-', lw=2, label="Nonlinear Boundary")

plt.title("Linear vs Nonlinear Classification")
plt.xlabel("$x_1$")
plt.ylabel("$x_2$")
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```


## Structure of Neural Networks

Each neuron performs a linear combination followed by a nonlinear transformation:

$$
z = \mathbf{w}^\top \mathbf{x} + b, \quad a = \sigma(z)
$$

For a deep network with $L$ layers:

$$
\begin{aligned}
a^{(1)} &= \sigma(W^{(1)}x + b^{(1)}) \\
a^{(2)} &= \sigma(W^{(2)}a^{(1)} + b^{(2)}) \\
&\vdots \\
\hat{y} &= \sigma(W^{(L)}a^{(L-1)} + b^{(L)})
\end{aligned}
$$

## Activation Functions

| Function | Formula | Range | Notes |
|-----------|----------|--------|-------|
| **Sigmoid** | $\sigma(x) = \frac{1}{1 + e^{-x}}$ | (0, 1) | Saturates, causes vanishing gradients |
| **Tanh** | $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1) | Zero-centered |
| **ReLU** | $\text{ReLU}(x) = \max(0, x)$ | [0, ∞) | Fast and sparse |
| **Leaky ReLU** | $\text{LeakyReLU}(x) = \max(\alpha x, x)$ | (-∞, ∞) | Fixes “dying ReLU” |
| **Linear** | $f(x) = cx$ | (-∞, ∞) | Used in regression |

## Loss Functions

To train a network, we minimize a loss function $L(y, \hat{y})$:

- **Mean Squared Error (MSE):**
  $$
  L = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2
  $$
- **Cross-Entropy Loss:**
  $$
  L = -\sum_i y_i \log(\hat{y}_i)
  $$

## Gradient Descent

Model parameters are updated iteratively:

$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t),
$$

where $\eta$ is the **learning rate**.  

### Mini-batch Gradient Descent

$$
\theta_{t+1} = \theta_t - \frac{\eta}{m} \sum_{i=1}^{m}\nabla_\theta L_i(\theta_t)
$$

Balances speed and stability in updates.

## Momentum and Learning Rate Schedules

Add momentum to accelerate convergence:

$$
v_t = \beta v_{t-1} + (1 - \beta)\nabla_\theta L(\theta_t),
\quad \theta_{t+1} = \theta_t - \eta v_t
$$

**Learning rate schedules:**

- Step decay: $\eta_t = \eta_0 \gamma^{\lfloor t/s \rfloor}$
- Exponential decay: $\eta_t = \eta_0 e^{-\lambda t}$
- Cosine annealing: $\eta_t = \eta_0 \frac{1}{2}(1 + \cos(\pi t/T))$

## Vanishing and Exploding Gradients

Gradients may vanish or explode when propagated through many layers:

$$
\frac{\partial L}{\partial W^{(l)}} \propto \prod_{i>l} \frac{\partial a^{(i)}}{\partial z^{(i)}}
$$

**Solutions:**
- ReLU activations
- Batch normalization
- Gradient clipping
- LSTM gates in RNNs

## Regularization for Generalization

### Weight Decay

$$
L' = L + \lambda \|W\|_2^2
$$

### Dropout

$$
a_i' = a_i \cdot r_i, \quad r_i \sim \text{Bernoulli}(1 - p)
$$

### Early Stopping
Stop when validation loss no longer improves.

### Batch Normalization
$$
\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad y = \gamma \hat{x} + \beta
$$

## Convolutional Neural Networks (CNNs)

Convolution extracts local spatial features:

$$
Y(i, j) = \sum_m \sum_n X(i + m, j + n) K(m, n)
$$

Pooling reduces spatial dimension:

- Max Pooling: $y = \max(x_1, \dots, x_n)$  
- Average Pooling: $y = \frac{1}{n}\sum_i x_i$

## Residual Networks (ResNets)

Residual connections help train very deep networks:

$$
y = F(x) + x
$$

The network learns the residual mapping $F(x) = H(x) - x$, stabilizing gradients across hundreds of layers.


## Recurrent Neural Networks (RNNs)

Sequential models update a hidden state over time:

$$
h_t = f(W_h h_{t-1} + W_x x_t + b)
$$

### Long Short-Term Memory (LSTM)

$$
\begin{aligned}
i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \\
f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \\
o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tanh(W_c x_t + U_c h_{t-1} + b_c) \\
h_t &= o_t \odot \tanh(c_t)
\end{aligned}
$$

## Transformer Networks

Self-attention replaces recurrence, allowing parallel computation.

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

Multiple heads attend to different relationships:

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(head_1, \ldots, head_h) W^O
$$

Transformers power modern large language models such as GPT and BERT.


## Summary

Deep learning combines principles of **linear algebra, calculus, and probability** to learn from massive datasets.

$$
\text{Forward Pass: } y = f_\theta(x), \quad
\text{Backward Pass: } \theta \leftarrow \theta - \eta \nabla_\theta L
$$

From perceptrons to transformers, every model refines this foundation — learning richer representations through mathematics and computation.


We have already discussed the class of ARMA models for representing stationary series. 
A generalization of this class, which incorporates a wide range of nonstationary series, is provided by the *autoregressive integrated moving average* (ARIMA) processes, i.e., processes which, after differencing finitely many times, reduce to ARMA processes.

If $d$ is a nonnegative integer, then $X_t$ is an ARIMA($p, d, q$) process if $Y_t=(1 - B)^d X_t$ is a causal ARMA($p, q$) process. 
The process is stationary if $d = 0$, in which case it reduces to an ARMA($p, q$) process.

For example $X_t$ is an ARIMA(1,1,0) process, then $Y_t$ representing the series of its first-order differences (because $d = 1$) is an ARMA(1,0) process
$$
Y_t = (1 - B)X_t = \phi_1 Y_{t-1} + Z_t,
$$
where $|\phi_1| < 1$ and $Z_t$ is white noise.

An equation for ARIMA($p, d, q$) is
$$
(1 - B)^d (1 - \phi_1B - \dots - \phi_pB^p)X_t = 
(1 + \theta_1B + \dots + \theta_qB^q)Z_t,
$$ {#eq-arima}
where $B$ is the backshift operator, $d$ is the order of differences, $\phi_1, \dots, \phi_p$ are autoregression coefficients, $p$ is the autoregressive order, $\theta_1, \dots, \theta_q$ are moving average coefficients, $q$ is the moving average order, and $Z_t\sim \mathrm{WN}(0,\sigma^2)$. 
The left part of @eq-arima consists of the differences and the AR part; the right part represents the MA part.

::: {.callout-note}
Modifications of @eq-arima exists, such as those having both the AR and MA parts on the right side. 
This affects the signs of estimated coefficients $\phi_1, \dots, \phi_p$ and possibly $\theta_1, \dots, \theta_q$. 
Check the help files of the software to know the exact form of the model it is estimating.
:::

**Why the process is called 'integrated'?**

Recall the geometric interpretation of the integral of a curve $y = f(x)$ defined on continuous $x$. 
The integral of $y$ corresponds to the area under the curve. 
For example, if $y = f(x)$ is a function describing income $y$ based on working time $x$, the integral corresponds to the total income in a certain period.

In our lectures, we deal with time series defined using discrete times $t$ (for example, years), so yearly income is $Y_t$, and the total income over several years can be obtained by summing the individual annual incomes, $\sum Y_t$. 
Hence, here we integrate by summation.

Recall the definition of random walk series, which is a cumulative sum of i.i.d. noise:
$$
X_t = \sum_{i=1}^t Y_i,
$$
where $Y_t \sim$ i.i.d.($0,\sigma^2$).
This $X_t$ is the simplest example of integrated series. 
The notation $X_t \sim$ I(1) means $X_t$ is a first-order integrated series. 
The differences of $X_t$
$$
\begin{align}
(1 - B)X_t &= X_t - BX_t \\
&= X_t - X_{t-1} \\
&= Y_t
\end{align}
$$
give us back the uncorrelated series $Y_t$, hence the process $X_t$ is an ARIMA(0,1,0) process.

::: {.callout-note}
ARIMA($p, d, q$) processes with $d \geqslant 1$ are also called *difference-stationary* processes or processes with a *stochastic trend*.
I.e., difference-stationary means the process is not stationary but can be made stationary by proper differencing.
:::


## Box--Jenkins methodology

The approach developed by @Box:Jenkins:1976 applies the differencing to the original time series repeatedly, until a stationary time series is obtained. 
We will first learn how to identify the number of differences (i.e., the order of differences $d$) by analyzing plots of the time series and ACF at each iteration of the method. 
In the later lecture on detecting trends in time series, we will also introduce formal tests for the integration order.

Below is a general algorithm used to fit ARIMA($p, d, q$) models

1. Start assuming $d = 0$.
1. Plot the time series and ACF.
1. If the plots suggest nonstationarity, iterate differencing and plotting to update $d$:
    * Apply differences of the current time series, write $d = d + 1$
    * Plot the time series and ACF
    * If nonstationarity is still obvious, repeat the differencing and plotting
1. Identify the orders $p$ and $q$ from the plots of ACF and PACF of the latest (differenced) time series.
1. Estimate the model.
1. Apply model diagnostics, particularly for homogeneity, uncorrelatedness, and normality of residuals. 
Address the violations by respecifying the model.
1. Forecast with the resultant model.

Using the Box--Jenkins methodology, the linear predictor approximately follows a normal distribution, i.e.,
$$
\hat{X}_{n+h} \sim N\left( X_{n+h}, \mathrm{var}(\hat{X}_{n+h}) \right).
$$

Therefore, a $(100 - \alpha)$\% prediction interval is
$$
\hat{X}_{n+h} \pm z_{1-\alpha/2} \sqrt{\mathrm{var}(\hat{X}_{n+h})}.
$$

::: {.callout-note icon=false}

## Example: ARIMA for Lake Baikal

Here we find an ARIMA model for the Lake Baikal thaw (breakup) dates from the Global Lake and River Ice Phenology Database [@Benson:etal:2020].

```{r}
# Calculate calendar day from ice break-up date
B <- read.csv("data/baikal.csv", skip = 1) %>%
    mutate(Date_iceoff = as.Date(paste(iceoff_year, iceoff_month, iceoff_day,
                                       sep = "-"))) %>%
    mutate(DoY_iceoff = as.numeric(format(Date_iceoff, "%j")))

# Convert to ts format
iceoff <- ts(B$DoY_iceoff, start = B$iceoff_year[1])
```

@fig-BaikalARIMA shows that there is possibly a decreasing trend (ice melts earlier in the year), although the ACF declines fast.

```{r}
#| label: fig-BaikalARIMA
#| fig-cap: "Plots for the original time series of ice breakup days."
#| fig-height: 7

X <- iceoff
p1 <- forecast::autoplot(X) +
    xlab("Year") +
    ylab("Ice breakup day")
p2 <- forecast::ggAcf(X) +
    ggtitle("")
p3 <- forecast::ggAcf(X, type = "partial") +
    ggtitle("")
p1 / (p2 + p3) +
    plot_annotation(tag_levels = 'A')
```

To remove this potential trend, we apply differencing once (more specifically, consecutive differencing, $X_t - X_{t-1}$) and produce the plots again. 
From @fig-BaikalARIMAd1, there is no tendency in the differenced series, and the ACF declines fast, so we achieved stationarity and no need to difference the data more. 
Overall, we differenced the time series once to achieve stationarity, so the order of differences $d = 1$.

```{r}
#| label: fig-BaikalARIMAd1
#| fig-cap: "Plots for the differenced time series of ice breakup days."
#| fig-height: 7

X <- diff(iceoff)
p1 <- ggplot2::autoplot(X) +
    xlab("Year") +
    ylab("diff(Ice breakup day)")
p2 <- forecast::ggAcf(X) +
    ggtitle("")
p3 <- forecast::ggAcf(X, type = "partial") +
    ggtitle("")
p1 / (p2 + p3) +
    plot_annotation(tag_levels = 'A')
```

Continue working with @fig-BaikalARIMAd1 to identify the orders $p$ and $q$. 
If we treat the behavior of ACF as exhibiting a cut-off, and PACF having an exponential decay, an MA($q$) model might be plausible (i.e., $p = 0$). 
Since the ACF cuts off after lag 1, $q = 1$ in this case. 

Hence, we specified the model for ice breakup dates to be ARIMA(0,1,1), which can be written as 
$$
(1 - B) Y_t = (1 + \theta_1B)Z_t
$$
or
$$
Y_t = Y_{t-1} + \theta_1 Z_{t-1} + Z_t,
$$
where $Y_t$ represents the ice breakup dates in the year $t$, $\theta_1$ is the moving average coefficient, and $Z_t\sim \mathrm{WN}(0,\sigma^2)$.

We can now estimate the model, for example, using `stats::arima()`.

```{r}
#| code-fold: false

mod_baikal <- stats::arima(iceoff, order = c(0, 1, 1))
mod_baikal
```

We can also check the orders selected automatically: 

```{r}
#| code-fold: false

forecast::auto.arima(iceoff)
```

In the next step, we apply diagnostic checks for the residuals, for example, using plots (@fig-BaikalARIMAres). 
Remember that the residuals should resemble white noise.

```{r}
#| label: fig-BaikalARIMAres
#| fig-cap: "Residual diagnostics for the ARIMA(0,1,1) model for ice breakup dates."
#| fig-height: 7

e <- mod_baikal$residuals
p1 <- ggplot2::autoplot(e) + 
    ylab("Residuals")
p2 <- forecast::ggAcf(e) +
    ggtitle("")
p3 <- ggpubr::ggqqplot(e) + 
    xlab("Standard normal quantiles")
p1 / (p2 + p3) +
    plot_annotation(tag_levels = 'A')
```

Given that the diagnostics plots show satisfactory behavior of the residuals (@fig-BaikalARIMAres), continue with forecasting using this model (@fig-BaikalARIMAfcst). 
Note that ARIMA(0,1,1) is mathematically equivalent to simple exponential smoothing, hence the forecast is a horizontal line.

```{r}
#| label: fig-BaikalARIMAfcst
#| fig-cap: "ARIMA(0,1,1) model predictions of ice breakup dates 10 years ahead."

ggplot2::autoplot(forecast::forecast(mod_baikal, h = 10)) + 
    xlab("Year") +
    ylab("Ice breakup day") +
    ggtitle("")
```
:::


## Seasonal ARIMA (SARIMA)

Recall that time series with seasonal variability or another strictly periodic component (e.g., daily cycles) can be deseasonalized by applying differencing that is not of consecutive values but with a lag equal to the period of the cyclical variability. 
We will use such differences to remove *strong* periodicity, as an additional step in the Box--Jenkins algorithm.

Similar to the regular differences, we will apply seasonal differences not to eliminate autocorrelations at the corresponding lags, but to achieve fast decay of ACF at seasonal lags. 
Even after the seasonal differences, there might be significant spikes at the seasonal lags in ACF and PACF, which can be addressed by selecting proper seasonal autoregressive and moving average orders. 
Therefore, we can define orders of integration, AR, and MA for the seasonal part of the time series in the same way we define these orders for the non-seasonal part.

Based on the definitions in @Brockwell:Davis:2002, $X_t$ is a *seasonal autoregressive integrated moving average*, SARIMA($p,d,q$)($P,D,Q$)$_s$ process if the differenced series $Y_t=(1 - B)^d (1 - B^s)^D X_t$ is a causal ARMA process. 
Here $d$ and $D$ are nonnegative integers, and $s$ is the period.

::: {.callout-note}
In practice, $D \leqslant 1$ and $P, Q \leqslant 3$.
:::

An equation for SARIMA($p, d, q$)($P, D, Q$)$_s$ is
$$
\begin{split}
(1 - B)^d (1 - \phi_1B - \dots - \phi_pB^p) (1 - B^s)^D (1 - \Phi_1B^s - \dots - \Phi_PB^{sP}) X_t \\
= (1 + \theta_1B + \dots + \theta_qB^q) (1 + \Theta_1B^s + \dots + \Theta_QB^{sQ})Z_t,
\end{split}
$$ {#eq-sarima}
where $D$ is the order of seasonal differences, $\Phi_1, \dots, \Phi_P$ are the seasonal autoregression coefficients, $P$ is the seasonal autoregressive order, $\Theta_1, \dots, \Theta_q$ are seasonal moving average coefficients, $Q$ is the seasonal moving average order, and the rest of the terms are the same as in @eq-arima.

::: {.callout-note icon=false}

## Example: SARIMA for the number of airline passengers

Here we revisit the time series on airline passengers from which we have removed the trend by taking regular non-seasonal differences once ($d = 1$). 
Notice from @fig-AirDiff C and D how after taking the usual differences the upward trend disappeared, and ACF started to decay much faster. 
At the seasonal lags, however, the ACF decays still linearly (@fig-AirDiff D), which suggested differencing at the seasonal lag to remove strong periodicity. 
After taking seasonal differences once ($D = 1$), the time series looks stationary (@fig-AirDiff E), and the ACF decays fast at both seasonal and non-seasonal lags. 
This is enough differencing.

```{r}
#| label: fig-AirDiff
#| fig-cap: "Time series plot of the airline passenger series with an estimated ACF and the detrended (differenced) series with their ACFs."
#| fig-height: 9

Yt <- log10(AirPassengers)

# Apply first-order (non-seasonal) differences
D1 <- diff(Yt)

# Additionally, apply first-order seasonal differences
D1D12 <- diff(D1, lag = 12)

p1 <- ggplot2::autoplot(Yt) + 
    xlab("Year") + 
    ylab("lg(Air passangers)") + 
    ggtitle("Yt")
p2 <- forecast::ggAcf(Yt) + 
    ggtitle("Yt") +
    xlab("Lag (months)")
p3 <- ggplot2::autoplot(D1) + 
    xlab("Year") + 
    ylab("lg(Air passangers)") + 
    ggtitle("(1-B)Yt")
p4 <- forecast::ggAcf(D1) + 
    ggtitle("(1-B)Yt") +
    xlab("Lag (months)")
p5 <- ggplot2::autoplot(D1D12) + 
    xlab("Year") + 
    ylab("lg(Air passangers)") + 
    ggtitle("(1-B)(1-B12)Yt")
p6 <- forecast::ggAcf(D1D12) + 
    ggtitle("(1-B)(1-B12)Yt") +
    xlab("Lag (months)")
(p1 + p2) / (p3 + p4) / (p5 + p6) +
    plot_annotation(tag_levels = 'A')
```

For the next step, see @fig-AirDiffACF to identify the orders $p$ and $q$. 
For that, look only at the non-seasonal lags, 1--11. 
Both ACF and PACF have significant values at lags 1 and 3, which could correspond to AR(3), MA(3), ARMA(1,1), or ARMA with higher orders. 
From these options ARMA(1,1) has fewer parameters, hence we prefer this model, with $p = 1$ and $q = 1$, as the most parsimonious option. 
(However, given that the correlations at lag 2 are not statistically significant, information criteria may penalize adding extra arguments and might prefer a more compact specification, AR(1) or MA(1).)

Next, use @fig-AirDiffACF again to identify orders $P$ and $Q$. 
Now look only at the seasonal lags 12, 24, 36, etc. 
Both the ACF and PACF have significant values only on the first of those lags (lag 12), which could correspond to AR(1), MA(1), or ARMA(1,1) for the seasonal component. 
From these options, AR(1) and MA(1) are the most parsimonious so we should select one of them or use some numeric criterion to select the best model (e.g., information criterion like AIC or forecasting accuracy on a testing set).

```{r}
#| label: fig-AirDiffACF
#| fig-cap: "ACF and PACF of the differenced airline passenger time series $(1-B)(1-B^{12})Y_t$."

p6 <- forecast::ggAcf(D1D12, lag.max = 36) + 
    ggtitle("(1-B)(1-B12)Yt") +
    xlab("Lag (months)")
p7 <- forecast::ggAcf(D1D12, lag.max = 36, type = "partial") + 
    ggtitle("(1-B)(1-B12)Yt") +
    xlab("Lag (months)")
p6 + p7 +
    plot_annotation(tag_levels = 'A')
```

Overall, our analysis suggested that SARIMA(1,1,1)(1,1,0) or SARIMA(1,1,1)(0,1,1) is a plausible model for this time series.
For example, estimate SARIMA(1,1,1)(0,1,1):

```{r}
#| code-fold: false

mod_air <- stats::arima(Yt, order = c(1, 1, 1),
                        seasonal = list(order = c(0, 1, 1),
                                        period = 12))
mod_air
```

SARIMA(1,1,1)(0,1,1)$_{12}$ model can be written down as
$$
\begin{split}
(1 - B) (1 - \phi_1B) (1 - B^{12}) X_t \\
= (1 + \theta_1B) (1 + \Theta_1B^{12})Z_t.
\end{split}
$$
We can apply the backshift operations are rewrite the model without the backshift operator as follows:
$$
\begin{split}
(1 - B - \phi_1B + \phi_1B^2) (1 - B^{12}) X_t \\
= (1 + \theta_1B + \Theta_1B^{12} + \theta_1\Theta_1B^{13})Z_t,
\end{split}
$$
then
$$
\begin{split}
(1 - B - \phi_1B + \phi_1B^2 - B^{12} + B^{13} + \phi_1B^{13} - \phi_1B^{14}) X_t \\
= (1 + \theta_1B + \Theta_1B^{12} + \theta_1\Theta_1B^{13})Z_t,
\end{split}
$$
then
$$
\begin{split}
X_t - X_{t-1} - \phi_1X_{t-1} + \phi_1X_{t-2} - X_{t-12} + X_{t-13} + \phi_1X_{t-13} - \phi_1X_{t-14} \\
= Z_t + \theta_1Z_{t-1} + \Theta_1Z_{t-12} + \theta_1\Theta_1Z_{t-13},
\end{split}
$$
then
$$
\begin{split}
X_t = X_{t-1} + \phi_1X_{t-1} - \phi_1X_{t-2} + X_{t-12} - X_{t-13} - \phi_1X_{t-13} + \phi_1X_{t-14} \\
+ \theta_1Z_{t-1} + \Theta_1Z_{t-12} + \theta_1\Theta_1Z_{t-13} + Z_t,
\end{split}
$$
where $X_t = \lg(AirPassengers)$ and $Z_t\sim \mathrm{WN}(0,\sigma^2)$.

Below are results based on the automatic selection of the orders:

```{r}
#| code-fold: false

forecast::auto.arima(Yt)
```

The orders selected automatically based on AIC suggest that indeed the non-significance of correlations at lag 2 and relatively low correlations at lag 3 made it not worthy to estimate additional parameters in the non-seasonal part, for which MA(1) specification was selected, not the suggested ARMA(1,1), AR(3), or MA(3).

In the next step, we apply diagnostic checks for the residuals, for example, using plots (@fig-AirARIMAres).

```{r}
#| label: fig-AirARIMAres
#| fig-cap: "Residual diagnostics for the SARIMA model for the airline passenger data."
#| fig-height: 7

e <- mod_air$residuals
p1 <- ggplot2::autoplot(e) + 
    ylab("Residuals")
p2 <- forecast::ggAcf(e) +
    ggtitle("")
p3 <- ggpubr::ggqqplot(e) + 
    xlab("Standard normal quantiles")
p1 / (p2 + p3) +
    plot_annotation(tag_levels = 'A')
```

Given that the diagnostics plots show satisfactory behavior of the residuals (@fig-AirARIMAres), continue with forecasting using this model (@fig-AirARIMAfcst).

```{r}
#| label: fig-AirARIMAfcst
#| fig-cap: "SARIMA model predictions of airline passenger data 2 years ahead."

ggplot2::autoplot(forecast::forecast(mod_air, h = 24)) + 
    xlab("Year") +
    ylab("lg(Air passangers)") +
    ggtitle("")
```

Compare ACF in @fig-AirARIMAres with ACF of regression residuals in @fig-airpassangersRegTrendSeas and @fig-airpassangersRegTrendFourier. 
The residuals of those regression models are autocorrelated, while the models have more parameters than the specified SARIMA models. 
Hence, the SARIMA model is much better for this time series.
:::


## Conclusion

In this lecture, we discovered ARIMA as an extension of ARMA modeling to nonstationary data and SARIMA as an extension of ARIMA models to time series with periodicity (seasonality).

We learned Box--Jenkins iterative procedure for identifying such models, including the orders of differences, $d$ and $D$, and $p$, $q$, $P$, and $Q$.

Please remember to specify the criterion for selecting a model beforehand and consider such options as cross-validation and using a testing set.


## Appendix

**Equivalences**

Mathematically, some models are equivalent one to another. Below are some examples.

* Forecasts of ARIMA(0,1,1) are equivalent to simple exponential smoothing.

An ARIMA(0,1,1) model can be written as 
$$
Y_t = Y_{t-1} + \theta_1 Z_{t-1} + Z_t,
$$
from which the one-step-ahead forecast is
$$
\hat{Y}_t = Y_{t-1} + \theta_1 (Y_{t-1} - \hat{Y}_{t-1}).
$$
If we define $\theta_1 = \alpha - 1$, then the above equation transforms to
$$
\begin{split}
\hat{Y}_t &= Y_{t-1} + (\alpha - 1) (Y_{t-1} - \hat{Y}_{t-1})\\
&= Y_{t-1} + (\alpha - 1) Y_{t-1} - (\alpha - 1) \hat{Y}_{t-1}\\
&= \alpha Y_{t-1} + (1 - \alpha) \hat{Y}_{t-1}
\end{split}
$$

* Forecasts of ARIMA(0,2,2) are equivalent to Holt's method.

* Forecasts of SARIMA(0,1,$s+1$)(0,1,0)$_s$ are equivalent to Holt--Winters additive method.
